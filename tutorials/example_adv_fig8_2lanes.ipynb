{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Adversarial Figure 8 with 2 Lanes of Traffic\n",
    "\n",
    "This example walks you through implementing modifications in the figure 8 template. In particular, we show how to modify the number of lanes, change the car behaviour accordingly and add an adversarial agent that learns how to slow down the flow of traffic using RL. We explain the settings used to modify each parameter so you can configure them based on your own needs.\n",
    "\n",
    "\n",
    "<img src=\"img/fig8_2lane_crowding.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "<center>**Figure 1.** Traffic on a figure 8 environment with two lanes and an adversarial agent</center>\n",
    "\n",
    "The remainder of this tutorial is organized as follows:\n",
    " \n",
    "* We start with an overview of the Figure 8 example from the core files \n",
    "* Section 1 gives an overview of the scripts involved in the simulation\n",
    "* Section 2 explains the changes on the environment file\n",
    "* Section 3 explains the changes on the network file\n",
    "* Section 4 shows how to run the training script\n",
    "\n",
    "\n",
    "## The Figure 8 example\n",
    "* The goal is to maximize the system-wide velocity for fourteen vehicles, which necessitates spacing the vehicles so that they don’t run into conflicts at the merging points. \n",
    "\n",
    "* The network is fully observed: all vehicles speeds and positions are visible to the controller. \n",
    "\n",
    "* This is an example of a multi-agent adversarial environment. \n",
    "* The example consists of one autonomous vehicle controlled by 2 agents:\n",
    "\n",
    "1. an RL-trained adversary agent whose objective is to disrupt the flow of traffic. \n",
    "2. an optimizing RL agent ('AV vehicle') whose objective is to optimize the flow of traffic (i.e. maximizing the speed of traffic)\n",
    "\n",
    "* Note that the expected total reward is zero as the adversary's reward is the negative of the AV reward\n",
    "\n",
    "* The example on this tutorial extends the simple figure8 example described here:\n",
    "\n",
    "    https://flow.readthedocs.io/en/latest/examples.html#figure-eight\n",
    "    \n",
    "    https://flow.readthedocs.io/en/latest/examples.html\n",
    "\n",
    "    https://flow-project.github.io/papers/vinitsky18a.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Steps Required\n",
    "\n",
    "The idea is to reuse the existing code and modify it according to our needs. \n",
    "Below are the steps required and the files we need to modify:\n",
    "\n",
    "**1. Change the configuration file to avoid script stopping at errors.**\n",
    "\n",
    "    ~flow/examples/exp_configs/rl/multiagent/multiagent_figure_eight.py\n",
    "\n",
    "**2. Change the configuration file to increase the the weight of the adversarial agent in the AV car**\n",
    "\n",
    "    ~flow/examples/exp_configs/rl/multiagent/multiagent_figure_eight.py\n",
    "    \n",
    "**3. Change the controllers dynamics to allow human and AV cars to pass each other.**\n",
    "\n",
    "    ~flow/examples/exp_configs/rl/multiagent/multiagent_figure_eight.py\n",
    "    \n",
    "\n",
    "**4. Change the network file to allow 2 lanes.**\n",
    "\n",
    "    ~flow/flow/networks/figure_eight.py\n",
    "\n",
    "\n",
    "**5. Run the usual runner used to train.**\n",
    "\n",
    "    ~flow/examples/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Changes in the Configuration File\n",
    "\n",
    "File path:\n",
    "**~flow/examples/exp_configs/rl/multiagent/multiagent_figure_eight.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add More Human Vehicles:\n",
    "We will change the number of human vehicles a piaccere, this is to show clearly the effects of a lane change. As an example, change the number of Human cars as follows:\n",
    "\n",
    "**N_HUMANS   = 25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Changes in SUMO-Related Parameters:\n",
    "We will add this line, to avoid the script stopping at errors:\n",
    "\n",
    "**restart_instance = True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add Lane Change Controllers to Human and AV Vehicles\n",
    "When we add a second line to our figure eight, we would like our cars to benefit from the additional line to pass each other to optimize the flow of traffic. We will import the controller and then add the controlling parameters to both the human and autonomous vehicles\n",
    "\n",
    "* Add the following import:\n",
    "\n",
    "    **from flow.controllers import SimLaneChangeController**\n",
    "\n",
    "* Add the following parameters to the humand and the av agents\n",
    "\n",
    "    **lane_change_controller=(SimLaneChangeController, {}),**\n",
    "    **lane_change_params=SumoLaneChangeParams(lane_change_mode=\"strategic\",),**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note: Additional Information on Line Changing Modes**\n",
    "The line changing parameters are controlled by SUMO. \n",
    "In the line above we have used the 'lane_change_mode=\"strategic\"'\n",
    "\n",
    "You can read more about the available options from SUMO documentation:\n",
    "\n",
    "The laneChangeModel discriminates four reasons to change lanes:\n",
    "\n",
    "    * strategic (change lanes to continue the route)\n",
    "    * cooperative (change in order to allow others to change)\n",
    "    * speed gain (the other lane allows for faster driving)\n",
    "    * obligation to drive on the right\n",
    "\n",
    "\n",
    "https://sumo.dlr.de/docs/TraCI/Change_Vehicle_State.html#lane_change_mode_0xb6\n",
    "\n",
    "https://sumo.dlr.de/docs/Definition_of_Vehicles,_Vehicle_Types,_and_Routes.html#lane-changing_models\n",
    "\n",
    "https://sumo.dlr.de/docs/Simulation/SublaneModel.html#lane-changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Increasing the weight of the adversarial agent.\n",
    "Change the perturbation weight to see the adversarial agent in full control\n",
    "\n",
    "**'perturb_weight': 0.9**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Changes in the Configuration File: Putting It All Together...\n",
    "Your environment file should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~flow/examples/exp_configs/rl/multiagent/multiagent_figure_eight.py\n",
    "\n",
    "    from copy import deepcopy\n",
    "    from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy\n",
    "    from flow.controllers import ContinuousRouter\n",
    "    from flow.controllers import IDMController #the human vehicles follows a physics algo\n",
    "    from flow.controllers import RLController  #the av drives according to RL algo\n",
    "    from flow.controllers import SimLaneChangeController #Controller used to enforce sumo lane-change dynamics on a vehicle.\n",
    "    from flow.core.params import EnvParams\n",
    "    from flow.core.params import InitialConfig\n",
    "    from flow.core.params import NetParams\n",
    "    from flow.core.params import SumoParams\n",
    "    from flow.core.params import SumoCarFollowingParams,SumoLaneChangeParams #lane change params\n",
    "    from flow.core.params import VehicleParams\n",
    "    from flow.networks.figure_eight import ADDITIONAL_NET_PARAMS #specifies the env structure (for example, the number of lanes)\n",
    "    from flow.envs.multiagent import MultiAgentAccelEnv\n",
    "    from flow.networks import FigureEightNetwork\n",
    "    from flow.utils.registry import make_create_env\n",
    "    from ray.tune.registry import register_env\n",
    "\n",
    "    # time horizon of a single rollout\n",
    "    HORIZON    = 1500\n",
    "    # number of rollouts per training iteration\n",
    "    N_ROLLOUTS = 4\n",
    "    # number of parallel workers\n",
    "    N_CPUS     = 44\n",
    "    # number of human-driven vehicles\n",
    "    N_HUMANS   = 25 #increased number of human cars to make it more dramatic\n",
    "    # number of automated vehicles\n",
    "    N_AVS      = 1\n",
    "\n",
    "     # ContinuousRouter controller -> to perpetually maintain the vehicle within the network.\n",
    "    # lane_change_controller=(SimLaneChangeController, {}) -> used to enforce sumo lane-change dynamics on a vehicle.\n",
    "    # lane_change_params=SumoLaneChangeParams(lane_change_mode=\"strategic\",) - > cars can change lane\n",
    "\n",
    "    vehicles = VehicleParams()\n",
    "    vehicles.add(\n",
    "        veh_id='human',\n",
    "        lane_change_controller=(SimLaneChangeController, {}),\n",
    "        lane_change_params=SumoLaneChangeParams(lane_change_mode=\"strategic\",),\n",
    "        acceleration_controller=(IDMController, {\n",
    "            'noise': 0.2\n",
    "        }),\n",
    "        routing_controller=(ContinuousRouter, {}),\n",
    "        car_following_params=SumoCarFollowingParams(\n",
    "            speed_mode='obey_safe_speed',\n",
    "        ),\n",
    "        num_vehicles=N_HUMANS)\n",
    "\n",
    "    #RL agent\n",
    "    vehicles.add(\n",
    "        veh_id='rl',\n",
    "        lane_change_controller=(SimLaneChangeController, {}),\n",
    "        lane_change_params=SumoLaneChangeParams(lane_change_mode=\"strategic\",),\n",
    "        acceleration_controller=(RLController, {}),\n",
    "        routing_controller=(ContinuousRouter, {}),\n",
    "        car_following_params=SumoCarFollowingParams(\n",
    "            speed_mode='obey_safe_speed',\n",
    "        ),\n",
    "        num_vehicles=N_AVS)\n",
    "\n",
    "    flow_params = dict(\n",
    "        # name of the experiment\n",
    "        exp_tag='multiagent_figure_eight',\n",
    "\n",
    "        # name of the flow environment the experiment is running on\n",
    "        env_name=MultiAgentAccelEnv,\n",
    "\n",
    "        # name of the network class the experiment is running on\n",
    "        network=FigureEightNetwork,\n",
    "\n",
    "        # simulator that is used by the experiment\n",
    "        simulator='traci',\n",
    "\n",
    "        # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "        sim=SumoParams(\n",
    "            sim_step =0.1,\n",
    "            render   =False,\n",
    "            restart_instance = True,\n",
    "         ),\n",
    "\n",
    "        # environment related parameters (see flow.core.params.EnvParams)\n",
    "        env=EnvParams(\n",
    "            horizon=HORIZON,\n",
    "            additional_params={\n",
    "                'target_velocity': 20,\n",
    "                'max_accel': 3,\n",
    "                'max_decel': 3,\n",
    "                'perturb_weight': 0.9, #0.03, #weight of the adversarial agent\n",
    "                'sort_vehicles': False\n",
    "            },\n",
    "        ),\n",
    "\n",
    "        # network-related parameters (see flow.core.params.NetParams and the\n",
    "        # network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "        net=NetParams(\n",
    "            additional_params=deepcopy(ADDITIONAL_NET_PARAMS),\n",
    "        ),\n",
    "\n",
    "        # vehicles to be placed in the network at the start of a rollout (see\n",
    "        # flow.core.params.VehicleParams)\n",
    "        veh=vehicles,\n",
    "\n",
    "        # parameters specifying the positioning of vehicles upon initialization/\n",
    "        # reset (see flow.core.params.InitialConfig)\n",
    "        initial=InitialConfig(),\n",
    "    )\n",
    "\n",
    "\n",
    "    create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "    # Register as rllib env\n",
    "    register_env(env_name, create_env)\n",
    "\n",
    "    test_env = create_env()\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "\n",
    "\n",
    "    def gen_policy():\n",
    "        \"\"\"Generate a policy in RLlib.\"\"\"\n",
    "        return PPOTFPolicy, obs_space, act_space, {}\n",
    "\n",
    "\n",
    "    # Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "    POLICY_GRAPHS = {'av': gen_policy(), 'adversary': gen_policy()}\n",
    "\n",
    "\n",
    "    def policy_mapping_fn(agent_id):\n",
    "        \"\"\"Map a policy in RLlib.\"\"\"\n",
    "        return agent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changes in the Network File\n",
    "We will change the network file to allow 2 lanes.\n",
    "\n",
    "File path: **~flow/flow/networks/figure_eight.py**\n",
    "\n",
    "Change the \"lanes\" entry in the ADDITIONAL_NET_PARAMS dictionary as shown below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_NET_PARAMS = {\n",
    "    # radius of the circular components\n",
    "    \"radius_ring\": 30,\n",
    "    # number of lanes\n",
    "    \"lanes\": 2,  #CHANGE HERE to add more lanes to the environment\n",
    "    # speed limit for all edges\n",
    "    \"speed_limit\": 30,\n",
    "    # resolution of the curved portions\n",
    "    \"resolution\": 40\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Runner\n",
    "\n",
    "After implementing the above changes, you can run the trainimng script as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~flow/examples/train multiagent_figure_eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“Life is not easy for any of us. But what of that? We must have perseverance and above all have confidence in ourselves. We must believe that we are gifted for something and that this thing must be attained.” Madame Curie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_for_flow",
   "language": "python",
   "name": "kernel_for_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
