{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Running RLlib Experiments\n",
    "\n",
    "This tutorial walks you through the process of running traffic simulations in Flow with trainable RLlib-powered agents. Autonomous agents will learn to maximize a certain reward over the rollouts, using the [**RLlib**](https://ray.readthedocs.io/en/latest/rllib.html) library ([citation](https://arxiv.org/abs/1712.09381)) ([installation instructions](https://flow.readthedocs.io/en/latest/flow_setup.html#optional-install-ray-rllib)). Simulations of this form will depict the propensity of RL agents to influence the traffic of a human fleet in order to make the whole fleet more efficient (for some given metrics). \n",
    "\n",
    "In this tutorial, we simulate an initially perturbed single lane ring road, where we introduce a single autonomous vehicle. We witness that, after some training, that the autonomous vehicle learns to dissipate the formation and propagation of \"phantom jams\" which form when only human driver dynamics are involved.\n",
    "\n",
    "## 1. Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *network*, and an *environment*. Networks describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc... in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act as the primary interface between the reinforcement learning algorithm and the network. Moreover, custom environments may be used to modify the dynamical features of an network. Finally, in the RL case, it is in the *environment* that the state/action spaces and the reward function are defined. \n",
    "\n",
    "## 2. Setting up a Network\n",
    "Flow contains a plethora of pre-designed networks used to replicate highways, intersections, and merges in both closed and open settings. All these networks are located in flow/networks. For this tutorial, which involves a single lane ring road, we will use the network `RingNetwork`.\n",
    "\n",
    "### 2.1 Setting up Network Parameters\n",
    "\n",
    "The network mentioned at the start of this section, as well as all other networks in Flow, are parameterized by the following arguments: \n",
    "* name\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "\n",
    "These parameters are explained in detail in `tutorial01_sumo.ipynb`. Moreover, all parameters excluding vehicles (covered in section 2.2) do not change from the previous tutorial. Accordingly, we specify them nearly as we have before, and leave further explanations of the parameters to `tutorial01_sumo.ipynb`.\n",
    "\n",
    "We begin by choosing the network the experiment will be trained on. We use one of Flow's builtin networks, located in `flow.networks`. A list of all available networks can be found by running the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flow.networks as networks\n",
    "\n",
    "# print(networks.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we choose to use the ring road network. The network class is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.networks import RingNetwork\n",
    "\n",
    "# ring road network class\n",
    "network_name = RingNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key difference between SUMO and RLlib experiments is that, in RLlib experiments, the network classes do not need to be defined; instead users should simply name the network class they wish to use. Later on, an environment setup module will import the correct network class based on the provided names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter classes to the network class\n",
    "from flow.core.params import NetParams, InitialConfig\n",
    "\n",
    "# name of the network\n",
    "name = \"training_example16\"\n",
    "\n",
    "# network-specific parameters\n",
    "from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)\n",
    "\n",
    "# initial configuration to vehicles\n",
    "initial_config = InitialConfig(spacing=\"uniform\", perturbation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Trainable Autonomous Vehicles\n",
    "The `Vehicles` class stores state information on all vehicles in the network. This class is used to identify the dynamical features of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various `get` methods within this class.\n",
    "\n",
    "The dynamics of vehicles in the `Vehicles` class can either be depicted by sumo or by the dynamical methods located in flow/controllers. For human-driven vehicles, we use the IDM model for acceleration behavior, with exogenous gaussian acceleration noise with std 0.2 m/s2 to induce perturbations that produce stop-and-go behavior. In addition, we use the `ContinousRouter` routing controller so that the vehicles may maintain their routes closed networks.\n",
    "\n",
    "As we have done in `tutorial01_sumo.ipynb`, human-driven vehicles are defined in the `VehicleParams` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles class\n",
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# vehicles dynamics models\n",
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "#vehicles.add(\"human\",\n",
    "#             acceleration_controller=(IDMController, {}),\n",
    "#             routing_controller=(ContinuousRouter, {}),\n",
    "#             num_vehicles=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above addition to the `Vehicles` class only accounts for 21 of the 22 vehicles that are placed in the network. We now add an additional trainable autuonomous vehicle whose actions are dictated by an RL agent. This is done by specifying an `RLController` as the acceleraton controller to the vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this controller serves primarirly as a placeholder that marks the vehicle as a component of the RL agent, meaning that lane changing and routing actions can also be specified by the RL agent to this vehicle.\n",
    "\n",
    "We finally add the vehicle as follows, while again using the `ContinuousRouter` to perpetually maintain the vehicle within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow.energy_models.toyota_energy import TacomaEnergy\n",
    "# vehicles.add(veh_id=\"rl\",\n",
    "#              acceleration_controller=(RLController, {}),\n",
    "#              routing_controller=(ContinuousRouter, {}),\n",
    "#              initial_speed =20,\n",
    "#              energy_model = TacomaEnergy,\n",
    "#              num_vehicles=1)\n",
    "\n",
    "\n",
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             initial_speed =15,\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several environments in Flow exist to train RL agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. The use of an environment allows us to view the cumulative reward simulation rollouts receive, along with to specify the state/action spaces.\n",
    "\n",
    "Sumo envrionments in Flow are parametrized by three components:\n",
    "* `SumoParams`\n",
    "* `EnvParams`\n",
    "* `Network`\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length of any simulation step and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and deactivate the GUI. \n",
    "\n",
    "**Note** For training purposes, it is highly recommanded to deactivate the GUI in order to avoid global slow down. In such case, one just needs to specify the following: `render=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specifies environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the network. For the environment `WaveAttenuationPOEnv`, these parameters are used to dictate bounds on the accelerations of the autonomous vehicles, as well as the range of ring lengths (and accordingly network densities) the agent is trained on.\n",
    "\n",
    "Finally, it is important to specify here the *horizon* of the experiment, which is the duration of one episode (during which the RL-agent acquire data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "# Define horizon as a variable to ensure consistent use across notebook\n",
    "HORIZON=2000\n",
    "\n",
    "env_params = EnvParams(\n",
    "    # length of one rollout\n",
    "    horizon=HORIZON,\n",
    "\n",
    "    additional_params={\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 1,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 1,\n",
    "        # bounds on the ranges of ring road lengths the autonomous vehicle \n",
    "        # is trained on\n",
    "        \"ring_length\": [220, 270],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initializing a Gym Environment\n",
    "\n",
    "Now, we have to specify our Gym Environment and the algorithm that our RL agents will use. Similar to the network, we choose to use on of Flow's builtin environments, a list of which is provided by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Env', 'AccelEnv', 'LaneChangeAccelEnv', 'LaneChangeAccelPOEnv', 'TrafficLightGridTestEnv', 'MergePOEnv', 'BottleneckEnv', 'BottleneckAccelEnv', 'WaveAttenuationEnv', 'WaveAttenuationPOEnv', 'EnergyOptEnv', 'EnergyOptPOEnv', 'TrafficLightGridEnv', 'TrafficLightGridPOEnv', 'TrafficLightGridBenchmarkEnv', 'BottleneckDesiredVelocityEnv', 'TestEnv', 'BayBridgeEnv', 'SingleStraightRoad', 'BottleNeckAccelEnv', 'DesiredVelocityEnv', 'PO_TrafficLightGridEnv', 'GreenWaveTestEnv']\n"
     ]
    }
   ],
   "source": [
    "import flow.envs as flowenvs\n",
    "\n",
    "print(flowenvs.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the environment \"WaveAttenuationPOEnv\", which is used to train autonomous vehicles to attenuate the formation and propagation of waves in a partially observable variable density ring road. To create the Gym Environment, the only necessary parameters are the environment name plus the previously defined variables. These are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.envs import EnergyOptPOEnv\n",
    "\n",
    "env_name = EnergyOptPOEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from flow.envs import WaveAttenuationPOEnv\n",
    "\n",
    "# env_name = WaveAttenuationPOEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Setting up Flow Parameters\n",
    "\n",
    "RLlib experiments both generate a `params.json` file for each experiment run. For RLlib experiments, the parameters defining the Flow network and environment must be stored as well. As such, in this section we define the dictionary `flow_params`, which contains the variables required by the utility function `make_create_env`. `make_create_env` is a higher-order function which returns a function `create_env` that initializes a Gym environment corresponding to the Flow network specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=network_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # simulation-related parameters\n",
    "    sim=sim_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Running RL experiments in Ray\n",
    "\n",
    "### 4.1 Import \n",
    "\n",
    "First, we must import modules required to run experiments in Ray. The `json` package is required to store the Flow experiment parameters in the `params.json` file, as is `FlowParamsEncoder`. Ray-related imports are required: the PPO algorithm agent, `ray.tune`'s experiment runner, and environment helper methods `register_env` and `make_create_env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "# from ray.rllib.agents.agent import get_agent_class\n",
    "#from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Ray\n",
    "Here, we initialize Ray and experiment-based constant variables specifying parallelism in the experiment as well as experiment batch size in terms of number of rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 18:03:34,383\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-07-28_18-03-34_382327_12178/logs.\n",
      "2020-07-28 18:03:34,529\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:23649 to respond...\n",
      "2020-07-28 18:03:34,699\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:31066 to respond...\n",
      "2020-07-28 18:03:34,713\tINFO services.py:809 -- Starting Redis shard with 3.3 GB max memory.\n",
      "2020-07-28 18:03:34,831\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-07-28_18-03-34_382327_12178/logs.\n",
      "2020-07-28 18:03:34,839\tWARNING services.py:1330 -- WARNING: The default object store size of 4.96 GB will use more than 50% of the available memory on this node (6.75 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2020-07-28 18:03:34,844\tINFO services.py:1475 -- Starting the Plasma object store with 4.96 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.100.38',\n",
       " 'redis_address': '192.168.100.38:23649',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-28_18-03-34_382327_12178/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-28_18-03-34_382327_12178/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-07-28_18-03-34_382327_12178'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 6\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "#ray.shutdown()\n",
    "ray.init(num_cpus=N_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configuration and Setup\n",
    "Here, we copy and modify the default configuration for the [PPO algorithm](https://arxiv.org/abs/1707.06347). The agent has the number of parallel workers specified, a batch size corresponding to `N_ROLLOUTS` rollouts (each of which has length `HORIZON` steps), a discount rate $\\gamma$ of 0.999, two hidden layers of size 16, uses Generalized Advantage Estimation, $\\lambda$ of 0.97, and other parameters as set below.\n",
    "\n",
    "Once `config` contains the desired parameters, a JSON string corresponding to the `flow_params` specified in section 3 is generated. The `FlowParamsEncoder` maps objects to string representations so that the experiment can be reproduced later. That string representation is stored within the `env_config` section of the `config` dictionary. Later, `config` is written out to the file `params.json`. \n",
    "\n",
    "Next, we call `make_create_env` and pass in the `flow_params` to return a function we can use to register our Flow environment with Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Running Experiments\n",
    "\n",
    "Here, we use the `run_experiments` function from `ray.tune`. The function takes a dictionary with one key, a name corresponding to the experiment, and one value, itself a dictionary containing parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 18:03:35,618\tINFO trial_runner.py:176 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/16.5 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 18:03:35,840\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-07-28 18:03:35,854\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-07-28 18:03:35,856\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2020-07-28 18:03:35,941\tWARNING util.py:145 -- The `start_trial` operation took 0.12888240814208984 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:42,934\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44,154\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.156763: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.193624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.194590: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1e18000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.194658: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.198569: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.198620: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44.198660: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:44,829\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:46,695\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f1e43ae8bd0>}\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:46,695\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f1e43ae8910>}\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:46,695\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1e43bf6c50>}\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:46,817\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:53,823\tINFO trainable.py:105 -- _setup took 11.591 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:03:53,823\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55,246\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.251179: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.293705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.294262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0cb8000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.294326: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.300678: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.300784: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m 2020-07-28 18:03:55.300862: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55,466\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.473768: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.522220: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.522807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2ad4000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.522885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.535228: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.535325: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m 2020-07-28 18:03:55.535401: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55,551\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.556223: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.597348: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.598045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7180000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.598127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55,563\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.568078: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.605899: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.605952: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m 2020-07-28 18:03:55.606003: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.607261: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.607867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7dd0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.607972: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.613742: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.613799: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m 2020-07-28 18:03:55.613843: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55,604\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.611963: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.650497: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.651136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6ea4000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.651221: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.660040: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.660146: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:55.660235: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:56,476\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:03:59,075\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,472\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333)}}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,473\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,475\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333)\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,476\tINFO sampler.py:407 -- Filtered obs: np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333)\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,481\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,482\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:00,619\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m { 'default_policy': ( np.ndarray((1, 1), dtype=float32, min=-0.017, max=-0.017, mean=-0.017),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.401, max=0.401, mean=0.401),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.005, max=0.0, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.005, max=-0.005, mean=-0.005)})}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:02,762\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.017, max=0.401, mean=0.302),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'actions': np.ndarray((200, 1), dtype=float32, min=-2.495, max=2.309, mean=0.029),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'advantages': np.ndarray((200,), dtype=float32, min=0.001, max=0.327, mean=0.238),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.005, max=0.0, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'eps_id': np.ndarray((200,), dtype=int64, min=1471962581.0, max=1471962581.0, mean=1471962581.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'new_obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.072, mean=0.343),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.072, mean=0.343),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'prev_actions': np.ndarray((200, 1), dtype=float32, min=-2.495, max=2.309, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=0.018, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'rewards': np.ndarray((200,), dtype=float32, min=0.001, max=0.018, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'value_targets': np.ndarray((200,), dtype=float32, min=-0.004, max=0.322, mean=0.233),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m                         'vf_preds': np.ndarray((200,), dtype=float32, min=-0.005, max=-0.005, mean=-0.005)},\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m 2020-07-28 18:04:02,782\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.017, max=0.401, mean=0.302),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'actions': np.ndarray((200, 1), dtype=float32, min=-2.495, max=2.309, mean=0.029),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.001, max=0.327, mean=0.238),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.005, max=0.0, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=1471962581.0, max=1471962581.0, mean=1471962581.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'new_obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.072, mean=0.343),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.072, mean=0.343),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'prev_actions': np.ndarray((200, 1), dtype=float32, min=-2.495, max=2.309, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=0.018, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=0.001, max=0.018, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-0.004, max=0.322, mean=0.233),\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.005, max=-0.005, mean=-0.005)},\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/kernel:0' shape=(3, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/kernel:0' shape=(16, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,216\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/kernel:0' shape=(3, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/kernel:0' shape=(16, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,217\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,219\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m { 'inputs': [ np.ndarray((2000, 1), dtype=float32, min=-3.66, max=3.124, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000,), dtype=float32, min=0.0, max=0.019, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000, 3), dtype=float32, min=0.0, max=1.16, mean=0.328),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000, 1), dtype=float32, min=-3.66, max=3.124, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-3.328, max=1.614, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000, 2), dtype=float32, min=-0.005, max=0.001, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-0.004, max=0.337, mean=0.226),\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-0.006, max=-0.005, mean=-0.005)],\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m 2020-07-28 18:04:05,219\tINFO multi_gpu_impl.py:191 -- Divided 2000 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-06\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 1149.347\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4152172803878784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.309937215410173e-05\n",
      "        policy_loss: -0.0005203213659115136\n",
      "        total_loss: 0.05580121651291847\n",
      "        vf_explained_var: -0.0004712343215942383\n",
      "        vf_loss: 0.05631893128156662\n",
      "    load_time_ms: 111.918\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "    sample_time_ms: 10188.242\n",
      "    update_time_ms: 1117.106\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.95263157894736\n",
      "    ram_util_percent: 68.01578947368422\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 12.654260635375977\n",
      "  time_this_iter_s: 12.654260635375977\n",
      "  time_total_s: 12.654260635375977\n",
      "  timestamp: 1595948646\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 1\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 12 s, 1 iter, 2000 ts, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m /home/solom/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m /home/solom/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=12225)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-15\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 627.916\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4139621257781982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.49419848236721e-06\n",
      "        policy_loss: -0.00019337082630954683\n",
      "        total_loss: 0.05802299827337265\n",
      "        vf_explained_var: 7.653236389160156e-05\n",
      "        vf_loss: 0.05821552127599716\n",
      "    load_time_ms: 57.871\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    sample_time_ms: 9368.177\n",
      "    update_time_ms: 561.791\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.07499999999999\n",
      "    ram_util_percent: 68.32499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 21.346137285232544\n",
      "  time_this_iter_s: 8.691876649856567\n",
      "  time_total_s: 21.346137285232544\n",
      "  timestamp: 1595948655\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 2\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 21 s, 2 iter, 4000 ts, nan rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-24\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 452.674\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4136086702346802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.259794877725653e-06\n",
      "        policy_loss: -0.00019533729937393218\n",
      "        total_loss: 0.05139659345149994\n",
      "        vf_explained_var: 0.002275526523590088\n",
      "        vf_loss: 0.05159171298146248\n",
      "    load_time_ms: 39.551\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "    sample_time_ms: 9224.045\n",
      "    update_time_ms: 379.746\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.57692307692308\n",
      "    ram_util_percent: 68.32307692307691\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 30.425666570663452\n",
      "  time_this_iter_s: 9.079529285430908\n",
      "  time_total_s: 30.425666570663452\n",
      "  timestamp: 1595948664\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 3\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 30 s, 3 iter, 6000 ts, nan rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-34\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 374.664\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4134113788604736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.387020908325212e-06\n",
      "        policy_loss: -0.00022562217782251537\n",
      "        total_loss: 0.054756976664066315\n",
      "        vf_explained_var: 0.006632506847381592\n",
      "        vf_loss: 0.0549825057387352\n",
      "    load_time_ms: 31.067\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    sample_time_ms: 9327.443\n",
      "    update_time_ms: 287.755\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.2857142857143\n",
      "    ram_util_percent: 68.37142857142855\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 40.26158142089844\n",
      "  time_this_iter_s: 9.835914850234985\n",
      "  time_total_s: 40.26158142089844\n",
      "  timestamp: 1595948674\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 4\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 40 s, 4 iter, 8000 ts, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 18.02937122375481\n",
      "  episode_reward_mean: 16.690167357372573\n",
      "  episode_reward_min: 15.130968498161673\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 5\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 330.119\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4133905172348022\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.735887159768026e-06\n",
      "        policy_loss: -0.0002447795995976776\n",
      "        total_loss: 0.05529431998729706\n",
      "        vf_explained_var: 0.010105490684509277\n",
      "        vf_loss: 0.05553904548287392\n",
      "    load_time_ms: 26.157\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    sample_time_ms: 9926.402\n",
      "    update_time_ms: 236.969\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.77777777777777\n",
      "    ram_util_percent: 68.32222222222222\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.21036332896326\n",
      "    mean_inference_ms: 3.567830804942072\n",
      "    mean_processing_ms: 1.592013265179849\n",
      "  time_since_restore: 52.83478832244873\n",
      "  time_this_iter_s: 12.573206901550293\n",
      "  time_total_s: 52.83478832244873\n",
      "  timestamp: 1595948686\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 5\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 52 s, 5 iter, 10000 ts, 16.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-04-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 18.02937122375481\n",
      "  episode_reward_mean: 16.690167357372573\n",
      "  episode_reward_min: 15.130968498161673\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 5\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 293.874\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0062500000931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4133620262145996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.714403752994258e-06\n",
      "        policy_loss: -0.00042519092676229775\n",
      "        total_loss: 0.05416777357459068\n",
      "        vf_explained_var: 0.002315223217010498\n",
      "        vf_loss: 0.05459294840693474\n",
      "    load_time_ms: 22.569\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    sample_time_ms: 9929.394\n",
      "    update_time_ms: 201.796\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.64\n",
      "    ram_util_percent: 68.26666666666665\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.21036332896326\n",
      "    mean_inference_ms: 3.567830804942072\n",
      "    mean_processing_ms: 1.592013265179849\n",
      "  time_since_restore: 62.95406484603882\n",
      "  time_this_iter_s: 10.119276523590088\n",
      "  time_total_s: 62.95406484603882\n",
      "  timestamp: 1595948697\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 6\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 62 s, 6 iter, 12000 ts, 16.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 18.02937122375481\n",
      "  episode_reward_mean: 16.690167357372573\n",
      "  episode_reward_min: 15.130968498161673\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 5\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 271.63\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0031250000465661287\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.41373610496521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.937116268090904e-06\n",
      "        policy_loss: -0.00030709075508639216\n",
      "        total_loss: 0.05609804391860962\n",
      "        vf_explained_var: 0.016836225986480713\n",
      "        vf_loss: 0.05640510097146034\n",
      "    load_time_ms: 20.244\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "    sample_time_ms: 9997.216\n",
      "    update_time_ms: 176.814\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.24000000000001\n",
      "    ram_util_percent: 68.37999999999998\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.21036332896326\n",
      "    mean_inference_ms: 3.567830804942072\n",
      "    mean_processing_ms: 1.592013265179849\n",
      "  time_since_restore: 73.56918478012085\n",
      "  time_this_iter_s: 10.615119934082031\n",
      "  time_total_s: 73.56918478012085\n",
      "  timestamp: 1595948707\n",
      "  timesteps_since_restore: 14000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 7\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 73 s, 7 iter, 14000 ts, 16.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 18.02937122375481\n",
      "  episode_reward_mean: 16.690167357372573\n",
      "  episode_reward_min: 15.130968498161673\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 5\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 255.156\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0015625000232830644\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4144983291625977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.663812316925032e-06\n",
      "        policy_loss: -0.0001386079820804298\n",
      "        total_loss: 0.05566589534282684\n",
      "        vf_explained_var: 0.03175729513168335\n",
      "        vf_loss: 0.05580448359251022\n",
      "    load_time_ms: 19.927\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    sample_time_ms: 9882.702\n",
      "    update_time_ms: 158.258\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.26923076923079\n",
      "    ram_util_percent: 68.37692307692306\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.21036332896326\n",
      "    mean_inference_ms: 3.567830804942072\n",
      "    mean_processing_ms: 1.592013265179849\n",
      "  time_since_restore: 82.88895320892334\n",
      "  time_this_iter_s: 9.31976842880249\n",
      "  time_total_s: 82.88895320892334\n",
      "  timestamp: 1595948717\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 8\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 82 s, 8 iter, 16000 ts, 16.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 18.02937122375481\n",
      "  episode_reward_mean: 16.690167357372573\n",
      "  episode_reward_min: 15.130968498161673\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 5\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 241.37\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0007812500116415322\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4147170782089233\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.139091520803049e-06\n",
      "        policy_loss: -8.398055797442794e-05\n",
      "        total_loss: 0.05160564184188843\n",
      "        vf_explained_var: 0.04261672496795654\n",
      "        vf_loss: 0.051689621061086655\n",
      "    load_time_ms: 18.419\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "    sample_time_ms: 9852.999\n",
      "    update_time_ms: 142.282\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.41428571428571\n",
      "    ram_util_percent: 68.30714285714284\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.21036332896326\n",
      "    mean_inference_ms: 3.567830804942072\n",
      "    mean_processing_ms: 1.592013265179849\n",
      "  time_since_restore: 92.69713711738586\n",
      "  time_this_iter_s: 9.808183908462524\n",
      "  time_total_s: 92.69713711738586\n",
      "  timestamp: 1595948726\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 9\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 92 s, 9 iter, 18000 ts, 16.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.463923574052636\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 10\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 227.526\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0003906250058207661\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4149997234344482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.308219563929015e-06\n",
      "        policy_loss: -0.00014995098172221333\n",
      "        total_loss: 0.042244210839271545\n",
      "        vf_explained_var: 0.05498850345611572\n",
      "        vf_loss: 0.04239417240023613\n",
      "    load_time_ms: 16.93\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    sample_time_ms: 9792.505\n",
      "    update_time_ms: 130.642\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.71428571428571\n",
      "    ram_util_percent: 68.43571428571427\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.505661302024258\n",
      "    mean_inference_ms: 3.6383297118918803\n",
      "    mean_processing_ms: 1.5864791649752457\n",
      "  time_since_restore: 102.11150431632996\n",
      "  time_this_iter_s: 9.414367198944092\n",
      "  time_total_s: 102.11150431632996\n",
      "  timestamp: 1595948736\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 10\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 102 s, 10 iter, 20000 ts, 16.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.463923574052636\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 10\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 119.737\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.00019531250291038305\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4154735803604126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.71562565912609e-06\n",
      "        policy_loss: -0.00021220350754447281\n",
      "        total_loss: 0.055099837481975555\n",
      "        vf_explained_var: 0.005636751651763916\n",
      "        vf_loss: 0.05531204864382744\n",
      "    load_time_ms: 6.274\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "    sample_time_ms: 9469.594\n",
      "    update_time_ms: 21.086\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.2\n",
      "    ram_util_percent: 68.38\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.505661302024258\n",
      "    mean_inference_ms: 3.6383297118918803\n",
      "    mean_processing_ms: 1.5864791649752457\n",
      "  time_since_restore: 109.20900678634644\n",
      "  time_this_iter_s: 7.0975024700164795\n",
      "  time_total_s: 109.20900678634644\n",
      "  timestamp: 1595948743\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 11\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 109 s, 11 iter, 22000 ts, 16.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.463923574052636\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 10\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 112.634\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.882812572759576e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4163682460784912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.997797522170004e-06\n",
      "        policy_loss: -0.0004543046816252172\n",
      "        total_loss: 0.04887746647000313\n",
      "        vf_explained_var: 0.0329095721244812\n",
      "        vf_loss: 0.049331773072481155\n",
      "    load_time_ms: 6.329\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "    sample_time_ms: 8770.23\n",
      "    update_time_ms: 22.274\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.575\n",
      "    ram_util_percent: 68.4125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.505661302024258\n",
      "    mean_inference_ms: 3.6383297118918803\n",
      "    mean_processing_ms: 1.5864791649752457\n",
      "  time_since_restore: 119.92334961891174\n",
      "  time_this_iter_s: 5.795742988586426\n",
      "  time_total_s: 119.92334961891174\n",
      "  timestamp: 1595948754\n",
      "  timesteps_since_restore: 26000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 13\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 119 s, 13 iter, 26000 ts, 16.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.463923574052636\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 10\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 103.775\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.441406286379788e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4167704582214355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.760597443644656e-06\n",
      "        policy_loss: -0.00018454551172908396\n",
      "        total_loss: 0.0466763935983181\n",
      "        vf_explained_var: 0.039009034633636475\n",
      "        vf_loss: 0.046860940754413605\n",
      "    load_time_ms: 5.918\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    sample_time_ms: 8303.33\n",
      "    update_time_ms: 21.701\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.11428571428573\n",
      "    ram_util_percent: 68.38571428571427\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 17.505661302024258\n",
      "    mean_inference_ms: 3.6383297118918803\n",
      "    mean_processing_ms: 1.5864791649752457\n",
      "  time_since_restore: 124.98515391349792\n",
      "  time_this_iter_s: 5.061804294586182\n",
      "  time_total_s: 124.98515391349792\n",
      "  timestamp: 1595948759\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 14\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 124 s, 14 iter, 28000 ts, 16.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.24259827543273\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 15\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 98.392\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.220703143189894e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.415807843208313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.499141363747185e-06\n",
      "        policy_loss: -0.00012675285688601434\n",
      "        total_loss: 0.04253249987959862\n",
      "        vf_explained_var: 0.061194777488708496\n",
      "        vf_loss: 0.04265925660729408\n",
      "    load_time_ms: 5.6\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    sample_time_ms: 7823.257\n",
      "    update_time_ms: 20.794\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.81818181818183\n",
      "    ram_util_percent: 68.49090909090908\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.854618040926553\n",
      "    mean_inference_ms: 3.520607084691234\n",
      "    mean_processing_ms: 1.54618186649112\n",
      "  time_since_restore: 132.67073774337769\n",
      "  time_this_iter_s: 7.685583829879761\n",
      "  time_total_s: 132.67073774337769\n",
      "  timestamp: 1595948767\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 15\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 132 s, 15 iter, 30000 ts, 16.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.242598275432734\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 15\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 91.101\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 6.10351571594947e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4143743515014648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.838606506993528e-06\n",
      "        policy_loss: -0.00022222661937121302\n",
      "        total_loss: 0.055079385638237\n",
      "        vf_explained_var: 0.011537790298461914\n",
      "        vf_loss: 0.05530160292983055\n",
      "    load_time_ms: 5.299\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    sample_time_ms: 7498.533\n",
      "    update_time_ms: 20.602\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64\n",
      "    ram_util_percent: 68.47999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.854618040926553\n",
      "    mean_inference_ms: 3.5206070846912345\n",
      "    mean_processing_ms: 1.5461818664911202\n",
      "  time_since_restore: 139.44739198684692\n",
      "  time_this_iter_s: 6.776654243469238\n",
      "  time_total_s: 139.44739198684692\n",
      "  timestamp: 1595948774\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 16\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 139 s, 16 iter, 32000 ts, 16.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.242598275432734\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 15\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 84.952\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.051757857974735e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4152947664260864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.903141638758825e-06\n",
      "        policy_loss: -0.00016424083150923252\n",
      "        total_loss: 0.0528448112308979\n",
      "        vf_explained_var: 0.026705801486968994\n",
      "        vf_loss: 0.05300905182957649\n",
      "    load_time_ms: 4.852\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "    sample_time_ms: 7248.636\n",
      "    update_time_ms: 19.298\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.18333333333332\n",
      "    ram_util_percent: 68.52499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.854618040926553\n",
      "    mean_inference_ms: 3.5206070846912345\n",
      "    mean_processing_ms: 1.5461818664911202\n",
      "  time_since_restore: 147.47407484054565\n",
      "  time_this_iter_s: 8.02668285369873\n",
      "  time_total_s: 147.47407484054565\n",
      "  timestamp: 1595948782\n",
      "  timesteps_since_restore: 34000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 17\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 147 s, 17 iter, 34000 ts, 16.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 16.242598275432734\n",
      "  episode_reward_min: 13.246412673206699\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 15\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 68.032\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.629394644936838e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4142768383026123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0238379218208138e-05\n",
      "        policy_loss: -0.0001515736512374133\n",
      "        total_loss: 0.03373634070158005\n",
      "        vf_explained_var: 0.19136035442352295\n",
      "        vf_loss: 0.033887919038534164\n",
      "    load_time_ms: 2.841\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "    sample_time_ms: 6219.132\n",
      "    update_time_ms: 19.165\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.73333333333333\n",
      "    ram_util_percent: 68.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.854618040926553\n",
      "    mean_inference_ms: 3.5206070846912345\n",
      "    mean_processing_ms: 1.5461818664911202\n",
      "  time_since_restore: 156.05967330932617\n",
      "  time_this_iter_s: 3.827686071395874\n",
      "  time_total_s: 156.05967330932617\n",
      "  timestamp: 1595948790\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 19\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 156 s, 19 iter, 38000 ts, 16.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.890906345260376\n",
      "  episode_reward_min: 12.736265199552175\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 20\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 65.716\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.814697322468419e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4129369258880615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.029998020385392e-05\n",
      "        policy_loss: -0.0001692514488240704\n",
      "        total_loss: 0.030657364055514336\n",
      "        vf_explained_var: 0.21526741981506348\n",
      "        vf_loss: 0.030826611444354057\n",
      "    load_time_ms: 2.966\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    sample_time_ms: 5863.675\n",
      "    update_time_ms: 17.764\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.6875\n",
      "    ram_util_percent: 68.2875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.220516862580904\n",
      "    mean_inference_ms: 3.4049460339547992\n",
      "    mean_processing_ms: 1.507173733320212\n",
      "  time_since_restore: 161.87731051445007\n",
      "  time_this_iter_s: 5.817637205123901\n",
      "  time_total_s: 161.87731051445007\n",
      "  timestamp: 1595948796\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 20\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 161 s, 20 iter, 40000 ts, 15.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.89090634526038\n",
      "  episode_reward_min: 12.736265199552175\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 20\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 59.04\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.536743306171047e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4125268459320068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.9421358855615836e-06\n",
      "        policy_loss: -0.00011248445662204176\n",
      "        total_loss: 0.043933071196079254\n",
      "        vf_explained_var: 0.0669105052947998\n",
      "        vf_loss: 0.04404554143548012\n",
      "    load_time_ms: 2.387\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    sample_time_ms: 5498.375\n",
      "    update_time_ms: 16.929\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.3\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.220516862580908\n",
      "    mean_inference_ms: 3.4049460339547997\n",
      "    mean_processing_ms: 1.507173733320212\n",
      "  time_since_restore: 170.13957500457764\n",
      "  time_this_iter_s: 3.7210350036621094\n",
      "  time_total_s: 170.13957500457764\n",
      "  timestamp: 1595948804\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 22\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 170 s, 22 iter, 44000 ts, 15.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.89090634526038\n",
      "  episode_reward_min: 12.736265199552175\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 20\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 63.33\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.3841858265427618e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4140722751617432\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5662184270913713e-06\n",
      "        policy_loss: -0.00010490989370737225\n",
      "        total_loss: 0.02114793099462986\n",
      "        vf_explained_var: 0.17618924379348755\n",
      "        vf_loss: 0.0212528295814991\n",
      "    load_time_ms: 2.529\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    sample_time_ms: 5497.33\n",
      "    update_time_ms: 18.629\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.59\n",
      "    ram_util_percent: 68.30999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 16.220516862580908\n",
      "    mean_inference_ms: 3.4049460339547997\n",
      "    mean_processing_ms: 1.507173733320212\n",
      "  time_since_restore: 181.04919409751892\n",
      "  time_this_iter_s: 6.521399259567261\n",
      "  time_total_s: 181.04919409751892\n",
      "  timestamp: 1595948815\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 24\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 181 s, 24 iter, 48000 ts, 15.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.282105929802892\n",
      "  episode_reward_min: 12.19972212733805\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 25\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 67.254\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1920929132713809e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.414276123046875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9821823116217274e-06\n",
      "        policy_loss: -9.190035052597523e-05\n",
      "        total_loss: 0.013836986385285854\n",
      "        vf_explained_var: 0.08869779109954834\n",
      "        vf_loss: 0.013928882777690887\n",
      "    load_time_ms: 2.733\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    sample_time_ms: 5855.773\n",
      "    update_time_ms: 17.144\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.9125\n",
      "    ram_util_percent: 68.3875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.704076033220506\n",
      "    mean_inference_ms: 3.3086103988931534\n",
      "    mean_processing_ms: 1.4811846066525802\n",
      "  time_since_restore: 192.35201978683472\n",
      "  time_this_iter_s: 11.302825689315796\n",
      "  time_total_s: 192.35201978683472\n",
      "  timestamp: 1595948827\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 25\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 192 s, 25 iter, 50000 ts, 15.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.282105929802892\n",
      "  episode_reward_min: 12.19972212733805\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 25\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 76.878\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.9604645663569045e-09\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4118396043777466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.449861419037916e-06\n",
      "        policy_loss: -0.00017861365631688386\n",
      "        total_loss: 0.05522125959396362\n",
      "        vf_explained_var: 0.03642106056213379\n",
      "        vf_loss: 0.055399879813194275\n",
      "    load_time_ms: 3.354\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    sample_time_ms: 6167.939\n",
      "    update_time_ms: 17.268\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.26428571428572\n",
      "    ram_util_percent: 68.46428571428571\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.704076033220506\n",
      "    mean_inference_ms: 3.3086103988931534\n",
      "    mean_processing_ms: 1.4811846066525802\n",
      "  time_since_restore: 202.38760089874268\n",
      "  time_this_iter_s: 10.035581111907959\n",
      "  time_total_s: 202.38760089874268\n",
      "  timestamp: 1595948837\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 26\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 202 s, 26 iter, 52000 ts, 15.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.282105929802892\n",
      "  episode_reward_min: 12.19972212733805\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 25\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 72.581\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.9802322831784522e-09\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4118452072143555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.115645192039665e-06\n",
      "        policy_loss: -0.00014322686183732003\n",
      "        total_loss: 0.044048868119716644\n",
      "        vf_explained_var: 0.12679773569107056\n",
      "        vf_loss: 0.04419209808111191\n",
      "    load_time_ms: 3.313\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "    sample_time_ms: 6096.498\n",
      "    update_time_ms: 17.96\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.10909090909091\n",
      "    ram_util_percent: 68.5909090909091\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.704076033220506\n",
      "    mean_inference_ms: 3.3086103988931534\n",
      "    mean_processing_ms: 1.4811846066525802\n",
      "  time_since_restore: 209.6464810371399\n",
      "  time_this_iter_s: 7.258880138397217\n",
      "  time_total_s: 209.6464810371399\n",
      "  timestamp: 1595948844\n",
      "  timesteps_since_restore: 54000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 27\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 209 s, 27 iter, 54000 ts, 15.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 15.282105929802892\n",
      "  episode_reward_min: 12.19972212733805\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 25\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 68.317\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.450580707946131e-10\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.411526083946228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6998222892871127e-06\n",
      "        policy_loss: -6.56394986435771e-05\n",
      "        total_loss: 0.02640628069639206\n",
      "        vf_explained_var: 0.23673856258392334\n",
      "        vf_loss: 0.026471910998225212\n",
      "    load_time_ms: 3.223\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "    sample_time_ms: 6029.404\n",
      "    update_time_ms: 15.012\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.01666666666667\n",
      "    ram_util_percent: 68.43333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.704076033220506\n",
      "    mean_inference_ms: 3.3086103988931534\n",
      "    mean_processing_ms: 1.4811846066525802\n",
      "  time_since_restore: 217.4686095714569\n",
      "  time_this_iter_s: 4.0702292919158936\n",
      "  time_total_s: 217.4686095714569\n",
      "  timestamp: 1595948852\n",
      "  timesteps_since_restore: 58000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 29\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 217 s, 29 iter, 58000 ts, 15.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.981943367587132\n",
      "  episode_reward_min: 10.495285209635023\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 30\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 66.871\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.8626451769865326e-10\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4080705642700195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0049253432953265e-05\n",
      "        policy_loss: -0.00020975303777959198\n",
      "        total_loss: 0.053418148308992386\n",
      "        vf_explained_var: 0.04801344871520996\n",
      "        vf_loss: 0.05362790822982788\n",
      "    load_time_ms: 2.923\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "    sample_time_ms: 5823.637\n",
      "    update_time_ms: 13.874\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.64\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.287407024358629\n",
      "    mean_inference_ms: 3.2301664346467005\n",
      "    mean_processing_ms: 1.4570813340130029\n",
      "  time_since_restore: 225.7248501777649\n",
      "  time_this_iter_s: 3.3147904872894287\n",
      "  time_total_s: 225.7248501777649\n",
      "  timestamp: 1595948860\n",
      "  timesteps_since_restore: 62000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 31\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 225 s, 31 iter, 62000 ts, 15 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.981943367587132\n",
      "  episode_reward_min: 10.495285209635023\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 30\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 66.518\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.6566129424663316e-11\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4045116901397705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.784759842848871e-05\n",
      "        policy_loss: -0.00015057182463351637\n",
      "        total_loss: 0.03326907008886337\n",
      "        vf_explained_var: 0.20881366729736328\n",
      "        vf_loss: 0.033419638872146606\n",
      "    load_time_ms: 2.897\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "    sample_time_ms: 5673.762\n",
      "    update_time_ms: 12.442\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.22500000000001\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 15.287407024358629\n",
      "    mean_inference_ms: 3.2301664346467005\n",
      "    mean_processing_ms: 1.4570813340130029\n",
      "  time_since_restore: 232.29955434799194\n",
      "  time_this_iter_s: 3.2207577228546143\n",
      "  time_total_s: 232.29955434799194\n",
      "  timestamp: 1595948867\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 33\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 232 s, 33 iter, 66000 ts, 15 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-07-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.750102985853283\n",
      "  episode_reward_min: 10.495285209635023\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 35\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1641532356165829e-11\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4067292213439941\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.387962005443114e-07\n",
      "        policy_loss: 5.756377959187375e-06\n",
      "        total_loss: 0.017288822680711746\n",
      "        vf_explained_var: 0.386802613735199\n",
      "        vf_loss: 0.01728307269513607\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    sample_time_ms: 4614.615\n",
      "    update_time_ms: 11.497\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.449999999999996\n",
      "    ram_util_percent: 68.23333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.840734590332424\n",
      "    mean_inference_ms: 3.1431926689616634\n",
      "    mean_processing_ms: 1.4312016245440686\n",
      "  time_since_restore: 239.32126832008362\n",
      "  time_this_iter_s: 4.133552551269531\n",
      "  time_total_s: 239.32126832008362\n",
      "  timestamp: 1595948874\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 35\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 239 s, 35 iter, 70000 ts, 14.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.750102985853283\n",
      "  episode_reward_min: 10.495285209635023\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 35\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.257\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.9103830890414573e-12\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4013173580169678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.693752346909605e-05\n",
      "        policy_loss: -0.00022882485063746572\n",
      "        total_loss: 0.029469333589076996\n",
      "        vf_explained_var: 0.24673408269882202\n",
      "        vf_loss: 0.029698150232434273\n",
      "    load_time_ms: 1.738\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "    sample_time_ms: 3639.308\n",
      "    update_time_ms: 9.332\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.6\n",
      "    ram_util_percent: 68.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.840734590332424\n",
      "    mean_inference_ms: 3.143192668961664\n",
      "    mean_processing_ms: 1.431201624544069\n",
      "  time_since_restore: 246.70634198188782\n",
      "  time_this_iter_s: 3.55759334564209\n",
      "  time_total_s: 246.70634198188782\n",
      "  timestamp: 1595948881\n",
      "  timesteps_since_restore: 74000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 37\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 246 s, 37 iter, 74000 ts, 14.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.750102985853283\n",
      "  episode_reward_min: 10.495285209635023\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 35\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.005\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.275957722603643e-13\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.403348684310913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4846325484541012e-06\n",
      "        policy_loss: 4.354214615887031e-05\n",
      "        total_loss: 0.01374968234449625\n",
      "        vf_explained_var: 0.4865037202835083\n",
      "        vf_loss: 0.013706150464713573\n",
      "    load_time_ms: 1.877\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "    sample_time_ms: 3626.634\n",
      "    update_time_ms: 10.019\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.02000000000001\n",
      "    ram_util_percent: 68.14\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.840734590332424\n",
      "    mean_inference_ms: 3.143192668961664\n",
      "    mean_processing_ms: 1.431201624544069\n",
      "  time_since_restore: 254.47353315353394\n",
      "  time_this_iter_s: 3.4788637161254883\n",
      "  time_total_s: 254.47353315353394\n",
      "  timestamp: 1595948889\n",
      "  timesteps_since_restore: 78000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 39\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 254 s, 39 iter, 78000 ts, 14.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.274141145004597\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 40\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.401\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.6379788613018216e-13\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4052728414535522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9320845012771315e-07\n",
      "        policy_loss: -1.3126373232807964e-05\n",
      "        total_loss: 0.008320874534547329\n",
      "        vf_explained_var: 0.41880422830581665\n",
      "        vf_loss: 0.00833401083946228\n",
      "    load_time_ms: 1.911\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    sample_time_ms: 3709.984\n",
      "    update_time_ms: 9.909\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.525\n",
      "    ram_util_percent: 68.4375\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.431863080110087\n",
      "    mean_inference_ms: 3.0629294274596726\n",
      "    mean_processing_ms: 1.4078120972483483\n",
      "  time_since_restore: 260.2591459751129\n",
      "  time_this_iter_s: 5.7856128215789795\n",
      "  time_total_s: 260.2591459751129\n",
      "  timestamp: 1595948895\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 40\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 260 s, 40 iter, 80000 ts, 14.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.274141145004597\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 40\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.901\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.094947153254554e-14\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3962937593460083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1088580322393682e-05\n",
      "        policy_loss: -0.0002675008727237582\n",
      "        total_loss: 0.031613465398550034\n",
      "        vf_explained_var: 0.23150873184204102\n",
      "        vf_loss: 0.03188096731901169\n",
      "    load_time_ms: 1.983\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    sample_time_ms: 3962.06\n",
      "    update_time_ms: 11.248\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.95\n",
      "    ram_util_percent: 68.3625\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.431863080110087\n",
      "    mean_inference_ms: 3.0629294274596726\n",
      "    mean_processing_ms: 1.4078120972483483\n",
      "  time_since_restore: 269.49421405792236\n",
      "  time_this_iter_s: 5.673248291015625\n",
      "  time_total_s: 269.49421405792236\n",
      "  timestamp: 1595948904\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 42\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 269 s, 42 iter, 84000 ts, 14.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 14.274141145004597\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 40\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 56.209\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.547473576627277e-14\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.398389458656311\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.574707069579745e-06\n",
      "        policy_loss: -0.00013949394633527845\n",
      "        total_loss: 0.022325614467263222\n",
      "        vf_explained_var: 0.3641745448112488\n",
      "        vf_loss: 0.022465115413069725\n",
      "    load_time_ms: 2.313\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "    sample_time_ms: 4403.676\n",
      "    update_time_ms: 10.789\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.76363636363635\n",
      "    ram_util_percent: 68.52727272727273\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.431863080110087\n",
      "    mean_inference_ms: 3.0629294274596726\n",
      "    mean_processing_ms: 1.4078120972483483\n",
      "  time_since_restore: 277.21314764022827\n",
      "  time_this_iter_s: 7.718933582305908\n",
      "  time_total_s: 277.21314764022827\n",
      "  timestamp: 1595948912\n",
      "  timesteps_since_restore: 86000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 43\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 277 s, 43 iter, 86000 ts, 14.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.922003668134943\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 45\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.523\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1368683941568192e-14\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4037179946899414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9679664546856657e-06\n",
      "        policy_loss: -9.930992382578552e-05\n",
      "        total_loss: 0.007653388194739819\n",
      "        vf_explained_var: 0.5424231290817261\n",
      "        vf_loss: 0.007752705365419388\n",
      "    load_time_ms: 2.285\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    sample_time_ms: 4589.818\n",
      "    update_time_ms: 11.523\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.71666666666667\n",
      "    ram_util_percent: 68.36666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.08638703595047\n",
      "    mean_inference_ms: 2.9948801629594413\n",
      "    mean_processing_ms: 1.387580685651526\n",
      "  time_since_restore: 286.09274983406067\n",
      "  time_this_iter_s: 4.63958477973938\n",
      "  time_total_s: 286.09274983406067\n",
      "  timestamp: 1595948921\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 45\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 286 s, 45 iter, 90000 ts, 13.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.922003668134936\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.206\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.684341970784096e-15\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3891853094100952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6477635401533917e-05\n",
      "        policy_loss: -0.0003484373155515641\n",
      "        total_loss: 0.051303230226039886\n",
      "        vf_explained_var: 0.13506770133972168\n",
      "        vf_loss: 0.05165165662765503\n",
      "    load_time_ms: 2.255\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    sample_time_ms: 4888.617\n",
      "    update_time_ms: 11.256\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.56\n",
      "    ram_util_percent: 68.35\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.08638703595047\n",
      "    mean_inference_ms: 2.9948801629594413\n",
      "    mean_processing_ms: 1.3875806856515263\n",
      "  time_since_restore: 292.90079617500305\n",
      "  time_this_iter_s: 6.808046340942383\n",
      "  time_total_s: 292.90079617500305\n",
      "  timestamp: 1595948928\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 46\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 292 s, 46 iter, 92000 ts, 13.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.922003668134936\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 61.45\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.842170985392048e-15\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3898913860321045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.450596341281198e-05\n",
      "        policy_loss: -0.0004583177505992353\n",
      "        total_loss: 0.03171078488230705\n",
      "        vf_explained_var: 0.27997666597366333\n",
      "        vf_loss: 0.03216910734772682\n",
      "    load_time_ms: 2.507\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "    sample_time_ms: 5224.981\n",
      "    update_time_ms: 11.898\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.81\n",
      "    ram_util_percent: 68.29999999999998\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.08638703595047\n",
      "    mean_inference_ms: 2.9948801629594413\n",
      "    mean_processing_ms: 1.3875806856515263\n",
      "  time_since_restore: 299.9083499908447\n",
      "  time_this_iter_s: 7.007553815841675\n",
      "  time_total_s: 299.9083499908447\n",
      "  timestamp: 1595948935\n",
      "  timesteps_since_restore: 94000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 47\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 299 s, 47 iter, 94000 ts, 13.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.922003668134936\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 65.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.421085492696024e-15\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3924005031585693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0670146113843657e-05\n",
      "        policy_loss: -0.00023101281840354204\n",
      "        total_loss: 0.01754084788262844\n",
      "        vf_explained_var: 0.4005606174468994\n",
      "        vf_loss: 0.017771873623132706\n",
      "    load_time_ms: 2.624\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    sample_time_ms: 5495.863\n",
      "    update_time_ms: 13.148\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.03\n",
      "    ram_util_percent: 68.29999999999998\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.08638703595047\n",
      "    mean_inference_ms: 2.9948801629594413\n",
      "    mean_processing_ms: 1.3875806856515263\n",
      "  time_since_restore: 306.9632959365845\n",
      "  time_this_iter_s: 7.054945945739746\n",
      "  time_total_s: 306.9632959365845\n",
      "  timestamp: 1595948942\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 48\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 306 s, 48 iter, 96000 ts, 13.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.922003668134936\n",
      "  episode_reward_min: 9.136506222477985\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 66.99\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.10542746348012e-16\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3975270986557007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.920644303434528e-06\n",
      "        policy_loss: 1.2645721199078253e-06\n",
      "        total_loss: 0.0075132702477276325\n",
      "        vf_explained_var: 0.6234502196311951\n",
      "        vf_loss: 0.0075120022520422935\n",
      "    load_time_ms: 2.741\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "    sample_time_ms: 5860.186\n",
      "    update_time_ms: 13.734\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.42999999999999\n",
      "    ram_util_percent: 68.38\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 14.08638703595047\n",
      "    mean_inference_ms: 2.9948801629594413\n",
      "    mean_processing_ms: 1.3875806856515263\n",
      "  time_since_restore: 314.119588136673\n",
      "  time_this_iter_s: 7.156292200088501\n",
      "  time_total_s: 314.119588136673\n",
      "  timestamp: 1595948949\n",
      "  timesteps_since_restore: 98000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 49\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 314 s, 49 iter, 98000 ts, 13.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.528851129274859\n",
      "  episode_reward_min: 8.412621555413866\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 50\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 67.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.55271373174006e-16\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4040744304656982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.6251884927860374e-08\n",
      "        policy_loss: 6.031989869370591e-06\n",
      "        total_loss: 0.0020828761626034975\n",
      "        vf_explained_var: 0.6025709509849548\n",
      "        vf_loss: 0.0020768397953361273\n",
      "    load_time_ms: 2.821\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    sample_time_ms: 6224.545\n",
      "    update_time_ms: 14.547\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.55\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.834996929114045\n",
      "    mean_inference_ms: 2.9456070519567454\n",
      "    mean_processing_ms: 1.3731663196972326\n",
      "  time_since_restore: 323.5619468688965\n",
      "  time_this_iter_s: 9.44235873222351\n",
      "  time_total_s: 323.5619468688965\n",
      "  timestamp: 1595948959\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 50\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 323 s, 50 iter, 100000 ts, 13.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.528851129274859\n",
      "  episode_reward_min: 8.412621555413866\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 50\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 70.369\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.77635686587003e-16\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3819645643234253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.00632130651502e-06\n",
      "        policy_loss: -0.00032430983264930546\n",
      "        total_loss: 0.048399489372968674\n",
      "        vf_explained_var: 0.20613831281661987\n",
      "        vf_loss: 0.048723794519901276\n",
      "    load_time_ms: 3.613\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "    sample_time_ms: 6706.846\n",
      "    update_time_ms: 15.604\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.875\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.834996929114045\n",
      "    mean_inference_ms: 2.9456070519567454\n",
      "    mean_processing_ms: 1.3731663196972328\n",
      "  time_since_restore: 332.0037248134613\n",
      "  time_this_iter_s: 8.44177794456482\n",
      "  time_total_s: 332.0037248134613\n",
      "  timestamp: 1595948967\n",
      "  timesteps_since_restore: 102000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 51\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 332 s, 51 iter, 102000 ts, 13.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.528851129274859\n",
      "  episode_reward_min: 8.412621555413866\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 50\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 75.272\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 8.88178432935015e-17\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3878893852233887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0614752682158723e-05\n",
      "        policy_loss: -0.00013209057215135545\n",
      "        total_loss: 0.022596849128603935\n",
      "        vf_explained_var: 0.4157572388648987\n",
      "        vf_loss: 0.022728944197297096\n",
      "    load_time_ms: 3.981\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    sample_time_ms: 6917.813\n",
      "    update_time_ms: 16.166\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.04545454545455\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.834996929114045\n",
      "    mean_inference_ms: 2.9456070519567454\n",
      "    mean_processing_ms: 1.3731663196972328\n",
      "  time_since_restore: 339.8553104400635\n",
      "  time_this_iter_s: 7.851585626602173\n",
      "  time_total_s: 339.8553104400635\n",
      "  timestamp: 1595948975\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 52\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 339 s, 52 iter, 104000 ts, 13.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.528851129274859\n",
      "  episode_reward_min: 8.412621555413866\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 50\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 72.222\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.440892164675075e-17\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3942179679870605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8479823893358116e-06\n",
      "        policy_loss: -2.0394325019879034e-06\n",
      "        total_loss: 0.010812886990606785\n",
      "        vf_explained_var: 0.5819568037986755\n",
      "        vf_loss: 0.010814913548529148\n",
      "    load_time_ms: 3.922\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "    sample_time_ms: 6863.578\n",
      "    update_time_ms: 16.648\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.36\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.834996929114045\n",
      "    mean_inference_ms: 2.9456070519567454\n",
      "    mean_processing_ms: 1.3731663196972328\n",
      "  time_since_restore: 347.0034866333008\n",
      "  time_this_iter_s: 7.148176193237305\n",
      "  time_total_s: 347.0034866333008\n",
      "  timestamp: 1595948982\n",
      "  timesteps_since_restore: 106000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 53\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 347 s, 53 iter, 106000 ts, 13.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.528851129274859\n",
      "  episode_reward_min: 8.412621555413866\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 50\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 79.151\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.2204460823375376e-17\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4020088911056519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.6633014133258257e-07\n",
      "        policy_loss: -2.734470399445854e-05\n",
      "        total_loss: 0.0024648017715662718\n",
      "        vf_explained_var: 0.6920145750045776\n",
      "        vf_loss: 0.0024921444710344076\n",
      "    load_time_ms: 4.205\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    sample_time_ms: 7152.249\n",
      "    update_time_ms: 16.06\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.63636363636364\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.834996929114045\n",
      "    mean_inference_ms: 2.9456070519567454\n",
      "    mean_processing_ms: 1.3731663196972328\n",
      "  time_since_restore: 354.21511149406433\n",
      "  time_this_iter_s: 7.21162486076355\n",
      "  time_total_s: 354.21511149406433\n",
      "  timestamp: 1595948989\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 54\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 354 s, 54 iter, 108000 ts, 13.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.052394828103962\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 55\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 83.383\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1102230411687688e-17\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.406650424003601\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.530416729015997e-07\n",
      "        policy_loss: -6.239080539671704e-05\n",
      "        total_loss: 0.0007438030443154275\n",
      "        vf_explained_var: 0.614009439945221\n",
      "        vf_loss: 0.0008061775006353855\n",
      "    load_time_ms: 4.282\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    sample_time_ms: 7584.124\n",
      "    update_time_ms: 17.614\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.04615384615386\n",
      "    ram_util_percent: 68.47692307692307\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.65386551124906\n",
      "    mean_inference_ms: 2.910793932324126\n",
      "    mean_processing_ms: 1.3628923796001793\n",
      "  time_since_restore: 363.24331974983215\n",
      "  time_this_iter_s: 9.028208255767822\n",
      "  time_total_s: 363.24331974983215\n",
      "  timestamp: 1595948998\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 55\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 363 s, 55 iter, 110000 ts, 13.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.05239482810396\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 55\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 82.848\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.551115205843844e-18\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3827170133590698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.201646207657177e-06\n",
      "        policy_loss: -0.0001922979427035898\n",
      "        total_loss: 0.04973677545785904\n",
      "        vf_explained_var: 0.1663638949394226\n",
      "        vf_loss: 0.049929071217775345\n",
      "    load_time_ms: 4.285\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    sample_time_ms: 7531.467\n",
      "    update_time_ms: 18.44\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.96666666666665\n",
      "    ram_util_percent: 68.55555555555556\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.65386551124906\n",
      "    mean_inference_ms: 2.9107939323241263\n",
      "    mean_processing_ms: 1.3628923796001793\n",
      "  time_since_restore: 369.527330160141\n",
      "  time_this_iter_s: 6.284010410308838\n",
      "  time_total_s: 369.527330160141\n",
      "  timestamp: 1595949005\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 56\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 369 s, 56 iter, 112000 ts, 13.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.05239482810396\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 55\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 82.091\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.775557602921922e-18\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3889448642730713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.765186753909802e-06\n",
      "        policy_loss: -6.602764187846333e-05\n",
      "        total_loss: 0.026341848075389862\n",
      "        vf_explained_var: 0.33163779973983765\n",
      "        vf_loss: 0.026407862082123756\n",
      "    load_time_ms: 4.381\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "    sample_time_ms: 7570.13\n",
      "    update_time_ms: 18.677\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.17999999999999\n",
      "    ram_util_percent: 68.55999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.65386551124906\n",
      "    mean_inference_ms: 2.9107939323241263\n",
      "    mean_processing_ms: 1.3628923796001793\n",
      "  time_since_restore: 376.91576647758484\n",
      "  time_this_iter_s: 7.388436317443848\n",
      "  time_total_s: 376.91576647758484\n",
      "  timestamp: 1595949012\n",
      "  timesteps_since_restore: 114000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 57\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 376 s, 57 iter, 114000 ts, 13.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.05239482810396\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 55\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 82.872\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.387778801460961e-18\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.396080493927002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0050237051473232e-06\n",
      "        policy_loss: -1.6474723452120088e-06\n",
      "        total_loss: 0.010716842487454414\n",
      "        vf_explained_var: 0.5776244401931763\n",
      "        vf_loss: 0.010718482546508312\n",
      "    load_time_ms: 4.508\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    sample_time_ms: 7542.312\n",
      "    update_time_ms: 18.132\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.38\n",
      "    ram_util_percent: 68.51\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.65386551124906\n",
      "    mean_inference_ms: 2.9107939323241263\n",
      "    mean_processing_ms: 1.3628923796001793\n",
      "  time_since_restore: 383.7143156528473\n",
      "  time_this_iter_s: 6.798549175262451\n",
      "  time_total_s: 383.7143156528473\n",
      "  timestamp: 1595949019\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 58\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 383 s, 58 iter, 116000 ts, 13.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 13.05239482810396\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 55\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 78.9\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 6.938894007304805e-19\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.403085470199585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.775324278696644e-07\n",
      "        policy_loss: -6.426811069104588e-06\n",
      "        total_loss: 0.0034143361262977123\n",
      "        vf_explained_var: 0.7336118221282959\n",
      "        vf_loss: 0.0034207659773528576\n",
      "    load_time_ms: 4.3\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "    sample_time_ms: 7477.338\n",
      "    update_time_ms: 19.869\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.26666666666667\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.65386551124906\n",
      "    mean_inference_ms: 2.9107939323241263\n",
      "    mean_processing_ms: 1.3628923796001793\n",
      "  time_since_restore: 390.1903772354126\n",
      "  time_this_iter_s: 6.476061582565308\n",
      "  time_total_s: 390.1903772354126\n",
      "  timestamp: 1595949026\n",
      "  timesteps_since_restore: 118000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 59\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 390 s, 59 iter, 118000 ts, 13.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.7156869254532\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 60\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 80.948\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.4694470036524025e-19\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4042760133743286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.7309527556317335e-07\n",
      "        policy_loss: -6.485461926786229e-05\n",
      "        total_loss: 0.0028349265921860933\n",
      "        vf_explained_var: 0.623701274394989\n",
      "        vf_loss: 0.0028997850604355335\n",
      "    load_time_ms: 4.334\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    sample_time_ms: 7397.667\n",
      "    update_time_ms: 20.416\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.32307692307691\n",
      "    ram_util_percent: 68.60000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.511448416741318\n",
      "    mean_inference_ms: 2.8834326979446794\n",
      "    mean_processing_ms: 1.355195143894858\n",
      "  time_since_restore: 398.8674895763397\n",
      "  time_this_iter_s: 8.677112340927124\n",
      "  time_total_s: 398.8674895763397\n",
      "  timestamp: 1595949034\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 60\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 398 s, 60 iter, 120000 ts, 12.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.715686925453197\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 60\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 79.056\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.7347235018262012e-19\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3844518661499023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.190934592112171e-07\n",
      "        policy_loss: -3.305816790089011e-05\n",
      "        total_loss: 0.05126207321882248\n",
      "        vf_explained_var: 0.1971948742866516\n",
      "        vf_loss: 0.05129513517022133\n",
      "    load_time_ms: 3.734\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "    sample_time_ms: 7388.119\n",
      "    update_time_ms: 20.068\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.1\n",
      "    ram_util_percent: 68.54166666666666\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.511448416741318\n",
      "    mean_inference_ms: 2.8834326979446794\n",
      "    mean_processing_ms: 1.3551951438948582\n",
      "  time_since_restore: 407.1815810203552\n",
      "  time_this_iter_s: 8.314091444015503\n",
      "  time_total_s: 407.1815810203552\n",
      "  timestamp: 1595949043\n",
      "  timesteps_since_restore: 122000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 61\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 407 s, 61 iter, 122000 ts, 12.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.715686925453197\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 60\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 79.651\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 8.673617509131006e-20\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3911830186843872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.895823051105253e-06\n",
      "        policy_loss: -0.00015516852727159858\n",
      "        total_loss: 0.023066937923431396\n",
      "        vf_explained_var: 0.3952517509460449\n",
      "        vf_loss: 0.02322210744023323\n",
      "    load_time_ms: 3.673\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    sample_time_ms: 7475.809\n",
      "    update_time_ms: 19.569\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.26666666666668\n",
      "    ram_util_percent: 68.425\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.511448416741318\n",
      "    mean_inference_ms: 2.8834326979446794\n",
      "    mean_processing_ms: 1.3551951438948582\n",
      "  time_since_restore: 415.91621255874634\n",
      "  time_this_iter_s: 8.734631538391113\n",
      "  time_total_s: 415.91621255874634\n",
      "  timestamp: 1595949051\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 62\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 415 s, 62 iter, 124000 ts, 12.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.715686925453197\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 60\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 79.059\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.336808754565503e-20\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3985285758972168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7587542515684618e-06\n",
      "        policy_loss: 1.4713048585690558e-05\n",
      "        total_loss: 0.009067738428711891\n",
      "        vf_explained_var: 0.651542067527771\n",
      "        vf_loss: 0.009053037501871586\n",
      "    load_time_ms: 3.478\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "    sample_time_ms: 7608.066\n",
      "    update_time_ms: 19.85\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.04166666666667\n",
      "    ram_util_percent: 68.39999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.511448416741318\n",
      "    mean_inference_ms: 2.8834326979446794\n",
      "    mean_processing_ms: 1.3551951438948582\n",
      "  time_since_restore: 424.3916895389557\n",
      "  time_this_iter_s: 8.47547698020935\n",
      "  time_total_s: 424.3916895389557\n",
      "  timestamp: 1595949060\n",
      "  timesteps_since_restore: 126000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 63\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 424 s, 63 iter, 126000 ts, 12.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.715686925453197\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 60\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 76.611\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.1684043772827515e-20\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.40491783618927\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.832966285699513e-06\n",
      "        policy_loss: -0.00023228836653288454\n",
      "        total_loss: 0.0031766186002641916\n",
      "        vf_explained_var: 0.7479376196861267\n",
      "        vf_loss: 0.0034088962711393833\n",
      "    load_time_ms: 3.407\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    sample_time_ms: 7634.638\n",
      "    update_time_ms: 20.631\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.00909090909092\n",
      "    ram_util_percent: 68.43636363636364\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.511448416741318\n",
      "    mean_inference_ms: 2.8834326979446794\n",
      "    mean_processing_ms: 1.3551951438948582\n",
      "  time_since_restore: 431.8473331928253\n",
      "  time_this_iter_s: 7.455643653869629\n",
      "  time_total_s: 431.8473331928253\n",
      "  timestamp: 1595949068\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 64\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 431 s, 64 iter, 128000 ts, 12.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.398475509974304\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 65\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 72.42\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0842021886413758e-20\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4081506729125977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5462338751603966e-06\n",
      "        policy_loss: 2.247333441118826e-06\n",
      "        total_loss: 0.0018229393754154444\n",
      "        vf_explained_var: 0.6803030967712402\n",
      "        vf_loss: 0.0018206818494945765\n",
      "    load_time_ms: 3.334\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    sample_time_ms: 7566.955\n",
      "    update_time_ms: 20.305\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.31666666666668\n",
      "    ram_util_percent: 68.45\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.411373548166607\n",
      "    mean_inference_ms: 2.864522420801535\n",
      "    mean_processing_ms: 1.3495820290657725\n",
      "  time_since_restore: 440.14773869514465\n",
      "  time_this_iter_s: 8.300405502319336\n",
      "  time_total_s: 440.14773869514465\n",
      "  timestamp: 1595949076\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 65\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 440 s, 65 iter, 130000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.398475509974306\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 65\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 75.365\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.421010943206879e-21\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.389652132987976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8717518034682143e-06\n",
      "        policy_loss: -8.176183473551646e-05\n",
      "        total_loss: 0.049321554601192474\n",
      "        vf_explained_var: 0.19919824600219727\n",
      "        vf_loss: 0.04940331354737282\n",
      "    load_time_ms: 3.517\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    sample_time_ms: 7571.548\n",
      "    update_time_ms: 19.607\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.86666666666666\n",
      "    ram_util_percent: 68.51111111111112\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.411373548166607\n",
      "    mean_inference_ms: 2.8645224208015345\n",
      "    mean_processing_ms: 1.3495820290657725\n",
      "  time_since_restore: 446.51077675819397\n",
      "  time_this_iter_s: 6.363038063049316\n",
      "  time_total_s: 446.51077675819397\n",
      "  timestamp: 1595949082\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 66\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 446 s, 66 iter, 132000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.398475509974306\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 65\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 71.22\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.7105054716034394e-21\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.394382357597351\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.54802146932343e-06\n",
      "        policy_loss: -0.00029419874772429466\n",
      "        total_loss: 0.023401085287332535\n",
      "        vf_explained_var: 0.36425840854644775\n",
      "        vf_loss: 0.02369528077542782\n",
      "    load_time_ms: 3.21\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "    sample_time_ms: 7376.343\n",
      "    update_time_ms: 19.016\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.77499999999999\n",
      "    ram_util_percent: 68.67500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.411373548166607\n",
      "    mean_inference_ms: 2.8645224208015345\n",
      "    mean_processing_ms: 1.3495820290657725\n",
      "  time_since_restore: 451.8834478855133\n",
      "  time_this_iter_s: 5.372671127319336\n",
      "  time_total_s: 451.8834478855133\n",
      "  timestamp: 1595949088\n",
      "  timesteps_since_restore: 134000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 67\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 451 s, 67 iter, 134000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.398475509974306\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 65\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 64.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 6.776263679008599e-22\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4004591703414917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.195852059434401e-06\n",
      "        policy_loss: 2.3375034288619645e-05\n",
      "        total_loss: 0.005413511302322149\n",
      "        vf_explained_var: 0.7099239826202393\n",
      "        vf_loss: 0.005390123464167118\n",
      "    load_time_ms: 2.911\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "    sample_time_ms: 6841.955\n",
      "    update_time_ms: 15.75\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.13333333333333\n",
      "    ram_util_percent: 68.64999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.411373548166607\n",
      "    mean_inference_ms: 2.8645224208015345\n",
      "    mean_processing_ms: 1.3495820290657725\n",
      "  time_since_restore: 459.67062544822693\n",
      "  time_this_iter_s: 3.7933690547943115\n",
      "  time_total_s: 459.67062544822693\n",
      "  timestamp: 1595949095\n",
      "  timesteps_since_restore: 138000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 69\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 459 s, 69 iter, 138000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.17429377841094\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 70\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 63.467\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.3881318395042993e-22\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4054206609725952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.648268259188626e-07\n",
      "        policy_loss: -3.3846856240415946e-05\n",
      "        total_loss: 0.002230878919363022\n",
      "        vf_explained_var: 0.6912529468536377\n",
      "        vf_loss: 0.002264701994135976\n",
      "    load_time_ms: 2.862\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    sample_time_ms: 6525.258\n",
      "    update_time_ms: 14.815\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.05\n",
      "    ram_util_percent: 68.47500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.309166345191649\n",
      "    mean_inference_ms: 2.8451130607858564\n",
      "    mean_processing_ms: 1.3438306761012375\n",
      "  time_since_restore: 465.167875289917\n",
      "  time_this_iter_s: 5.4972498416900635\n",
      "  time_total_s: 465.167875289917\n",
      "  timestamp: 1595949101\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 70\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 465 s, 70 iter, 140000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.174293778410942\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 70\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.435\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 8.470329598760748e-23\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3888747692108154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5995205103536136e-05\n",
      "        policy_loss: -0.00011670398816931993\n",
      "        total_loss: 0.018279245123267174\n",
      "        vf_explained_var: 0.6172789335250854\n",
      "        vf_loss: 0.01839596964418888\n",
      "    load_time_ms: 2.47\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    sample_time_ms: 5574.214\n",
      "    update_time_ms: 13.081\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.86000000000001\n",
      "    ram_util_percent: 68.34\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.309166345191649\n",
      "    mean_inference_ms: 2.8451130607858564\n",
      "    mean_processing_ms: 1.3438306761012377\n",
      "  time_since_restore: 472.5777316093445\n",
      "  time_this_iter_s: 3.879591703414917\n",
      "  time_total_s: 472.5777316093445\n",
      "  timestamp: 1595949108\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 72\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 472 s, 72 iter, 144000 ts, 12.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 12.174293778410942\n",
      "  episode_reward_min: 6.989441015264951\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 70\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.267\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.117582399690187e-23\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4047181606292725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4636983653181233e-06\n",
      "        policy_loss: -7.564735278720036e-05\n",
      "        total_loss: 0.00211337860673666\n",
      "        vf_explained_var: 0.8239113092422485\n",
      "        vf_loss: 0.002189035527408123\n",
      "    load_time_ms: 2.293\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    sample_time_ms: 4891.725\n",
      "    update_time_ms: 12.585\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.76666666666667\n",
      "    ram_util_percent: 68.41666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.309166345191649\n",
      "    mean_inference_ms: 2.8451130607858564\n",
      "    mean_processing_ms: 1.3438306761012377\n",
      "  time_since_restore: 481.60067558288574\n",
      "  time_this_iter_s: 4.5663793087005615\n",
      "  time_total_s: 481.60067558288574\n",
      "  timestamp: 1595949118\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 74\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 481 s, 74 iter, 148000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.863898734989442\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 75\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 54.296\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0587911998450935e-23\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4091087579727173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0986924507960794e-06\n",
      "        policy_loss: -9.595679875928909e-05\n",
      "        total_loss: 0.0005431222962215543\n",
      "        vf_explained_var: 0.3893197178840637\n",
      "        vf_loss: 0.0006390801863744855\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    sample_time_ms: 4643.826\n",
      "    update_time_ms: 11.7\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.96666666666667\n",
      "    ram_util_percent: 68.42222222222222\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.202510236220713\n",
      "    mean_inference_ms: 2.824544167816603\n",
      "    mean_processing_ms: 1.337877847207792\n",
      "  time_since_restore: 487.46127700805664\n",
      "  time_this_iter_s: 5.860601425170898\n",
      "  time_total_s: 487.46127700805664\n",
      "  timestamp: 1595949123\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 75\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 487 s, 75 iter, 150000 ts, 11.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.863898734989442\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 75\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.397\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.646977999612734e-24\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3899027109146118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.422996534638514e-07\n",
      "        policy_loss: -4.449844254850177e-06\n",
      "        total_loss: 0.018956894055008888\n",
      "        vf_explained_var: 0.5915462970733643\n",
      "        vf_loss: 0.018961351364850998\n",
      "    load_time_ms: 2.191\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "    sample_time_ms: 4193.07\n",
      "    update_time_ms: 11.804\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.16\n",
      "    ram_util_percent: 68.38\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.202510236220714\n",
      "    mean_inference_ms: 2.8245441678166023\n",
      "    mean_processing_ms: 1.3378778472077915\n",
      "  time_since_restore: 494.66465401649475\n",
      "  time_this_iter_s: 3.7720890045166016\n",
      "  time_total_s: 494.66465401649475\n",
      "  timestamp: 1595949131\n",
      "  timesteps_since_restore: 154000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 77\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 494 s, 77 iter, 154000 ts, 11.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.863898734989442\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 75\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 53.649\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.323488999806367e-24\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3984683752059937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.01096224070352e-07\n",
      "        policy_loss: -2.474784923833795e-05\n",
      "        total_loss: 0.006642377004027367\n",
      "        vf_explained_var: 0.7487046718597412\n",
      "        vf_loss: 0.006667118053883314\n",
      "    load_time_ms: 2.254\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    sample_time_ms: 4294.367\n",
      "    update_time_ms: 11.537\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.02857142857144\n",
      "    ram_util_percent: 68.48571428571428\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.202510236220714\n",
      "    mean_inference_ms: 2.8245441678166023\n",
      "    mean_processing_ms: 1.3378778472077915\n",
      "  time_since_restore: 499.6830940246582\n",
      "  time_this_iter_s: 5.018440008163452\n",
      "  time_total_s: 499.6830940246582\n",
      "  timestamp: 1595949136\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 78\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 499 s, 78 iter, 156000 ts, 11.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.618247355043511\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 80\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.67\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.3087224995159173e-25\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4071054458618164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5856027175686904e-06\n",
      "        policy_loss: -5.941772542428225e-05\n",
      "        total_loss: 0.001636005355976522\n",
      "        vf_explained_var: 0.6149746179580688\n",
      "        vf_loss: 0.0016954195452854037\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    sample_time_ms: 4267.941\n",
      "    update_time_ms: 11.926\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.67142857142857\n",
      "    ram_util_percent: 68.47142857142856\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.092963760733763\n",
      "    mean_inference_ms: 2.8031668049346488\n",
      "    mean_processing_ms: 1.33165260786598\n",
      "  time_since_restore: 508.68925762176514\n",
      "  time_this_iter_s: 4.544560670852661\n",
      "  time_total_s: 508.68925762176514\n",
      "  timestamp: 1595949145\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 80\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 508 s, 80 iter, 160000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.618247355043511\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 80\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.964\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 8.271806248789793e-26\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3900452852249146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5653789887437597e-05\n",
      "        policy_loss: -0.00027270722785033286\n",
      "        total_loss: 0.01714157499372959\n",
      "        vf_explained_var: 0.5927523374557495\n",
      "        vf_loss: 0.017414283007383347\n",
      "    load_time_ms: 1.946\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    sample_time_ms: 4033.862\n",
      "    update_time_ms: 10.959\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.2\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 13.092963760733763\n",
      "    mean_inference_ms: 2.8031668049346483\n",
      "    mean_processing_ms: 1.3316526078659803\n",
      "  time_since_restore: 513.6695058345795\n",
      "  time_this_iter_s: 2.131560802459717\n",
      "  time_total_s: 513.6695058345795\n",
      "  timestamp: 1595949150\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 82\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 515 s, 83 iter, 166000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.383127975690227\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 85\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.898\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0339757810987241e-26\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.408336877822876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3673305829797755e-06\n",
      "        policy_loss: -0.0001630401675356552\n",
      "        total_loss: 0.0008973446092568338\n",
      "        vf_explained_var: 0.5979368090629578\n",
      "        vf_loss: 0.0010603975970298052\n",
      "    load_time_ms: 1.49\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    sample_time_ms: 3349.211\n",
      "    update_time_ms: 8.404\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.160000000000004\n",
      "    ram_util_percent: 68.34\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.971845183922063\n",
      "    mean_inference_ms: 2.7792719888106587\n",
      "    mean_processing_ms: 1.3246686150651272\n",
      "  time_since_restore: 521.5319271087646\n",
      "  time_this_iter_s: 3.488740921020508\n",
      "  time_total_s: 521.5319271087646\n",
      "  timestamp: 1595949158\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 85\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 521 s, 85 iter, 170000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.383127975690227\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 85\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.253\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.2924697263734052e-27\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3938183784484863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.750081178732216e-06\n",
      "        policy_loss: -0.00014548540639225394\n",
      "        total_loss: 0.008429383859038353\n",
      "        vf_explained_var: 0.8133379220962524\n",
      "        vf_loss: 0.008574867621064186\n",
      "    load_time_ms: 1.334\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    sample_time_ms: 2856.767\n",
      "    update_time_ms: 6.853\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.975\n",
      "    ram_util_percent: 68.375\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.971845183922067\n",
      "    mean_inference_ms: 2.779271988810658\n",
      "    mean_processing_ms: 1.3246686150651275\n",
      "  time_since_restore: 528.7360391616821\n",
      "  time_this_iter_s: 2.2889537811279297\n",
      "  time_total_s: 528.7360391616821\n",
      "  timestamp: 1595949165\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 88\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 528 s, 88 iter, 176000 ts, 11.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.206443520669147\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 90\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.143\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.231174315933513e-28\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4049785137176514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.04126273123984e-07\n",
      "        policy_loss: -6.685495463898405e-05\n",
      "        total_loss: 0.0015092125395312905\n",
      "        vf_explained_var: 0.4305439591407776\n",
      "        vf_loss: 0.0015760717215016484\n",
      "    load_time_ms: 1.167\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    sample_time_ms: 2692.042\n",
      "    update_time_ms: 5.508\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.416666666666664\n",
      "    ram_util_percent: 68.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.845439875092703\n",
      "    mean_inference_ms: 2.754224962498864\n",
      "    mean_processing_ms: 1.3174246509845922\n",
      "  time_since_restore: 536.0658056735992\n",
      "  time_this_iter_s: 4.28152060508728\n",
      "  time_total_s: 536.0658056735992\n",
      "  timestamp: 1595949172\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 90\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 536 s, 90 iter, 180000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.206443520669147\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.358\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.038967894916891e-29\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3908240795135498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.080368173868919e-07\n",
      "        policy_loss: 2.753925400611479e-05\n",
      "        total_loss: 0.005214205011725426\n",
      "        vf_explained_var: 0.7441539168357849\n",
      "        vf_loss: 0.005186653696000576\n",
      "    load_time_ms: 1.153\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "    sample_time_ms: 2646.498\n",
      "    update_time_ms: 5.953\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.845439875092703\n",
      "    mean_inference_ms: 2.7542249624988644\n",
      "    mean_processing_ms: 1.317424650984592\n",
      "  time_since_restore: 542.7781474590302\n",
      "  time_this_iter_s: 2.104930877685547\n",
      "  time_total_s: 542.7781474590302\n",
      "  timestamp: 1595949179\n",
      "  timesteps_since_restore: 186000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 93\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 542 s, 93 iter, 186000 ts, 11.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.022657727124031\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 95\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.91\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0097419737292228e-29\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4049787521362305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1022389116988052e-06\n",
      "        policy_loss: -8.336067548952997e-05\n",
      "        total_loss: 0.0013257894897833467\n",
      "        vf_explained_var: 0.6082749962806702\n",
      "        vf_loss: 0.001409150310792029\n",
      "    load_time_ms: 1.202\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    sample_time_ms: 2617.551\n",
      "    update_time_ms: 5.453\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.35000000000001\n",
      "    ram_util_percent: 68.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.714128659322638\n",
      "    mean_inference_ms: 2.728001411762593\n",
      "    mean_processing_ms: 1.309817537488047\n",
      "  time_since_restore: 548.1461136341095\n",
      "  time_this_iter_s: 3.229889392852783\n",
      "  time_total_s: 548.1461136341095\n",
      "  timestamp: 1595949184\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 95\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 548 s, 95 iter, 190000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 11.022657727124031\n",
      "  episode_reward_min: 5.079514182394884\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 95\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.946\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.2621774671615285e-30\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.392669916152954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.3852797776698935e-08\n",
      "        policy_loss: -3.877925701090135e-05\n",
      "        total_loss: 0.003707245457917452\n",
      "        vf_explained_var: 0.8466885685920715\n",
      "        vf_loss: 0.0037460222374647856\n",
      "    load_time_ms: 1.191\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    sample_time_ms: 2563.737\n",
      "    update_time_ms: 4.874\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.56666666666666\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.714128659322636\n",
      "    mean_inference_ms: 2.7280014117625933\n",
      "    mean_processing_ms: 1.309817537488047\n",
      "  time_since_restore: 554.793181180954\n",
      "  time_this_iter_s: 2.2460250854492188\n",
      "  time_total_s: 554.793181180954\n",
      "  timestamp: 1595949191\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 98\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 554 s, 98 iter, 196000 ts, 11 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 10.820207745508007\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 100\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.543\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.1554436679038213e-31\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4051835536956787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.514779680495849e-06\n",
      "        policy_loss: -0.0001658458641031757\n",
      "        total_loss: 0.0013819669838994741\n",
      "        vf_explained_var: 0.4520975351333618\n",
      "        vf_loss: 0.001547799096442759\n",
      "    load_time_ms: 1.118\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    sample_time_ms: 2378.667\n",
      "    update_time_ms: 4.792\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.739999999999995\n",
      "    ram_util_percent: 68.36\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.580366215003489\n",
      "    mean_inference_ms: 2.7011332738251292\n",
      "    mean_processing_ms: 1.3020165490973474\n",
      "  time_since_restore: 560.2129001617432\n",
      "  time_this_iter_s: 3.2720394134521484\n",
      "  time_total_s: 560.2129001617432\n",
      "  timestamp: 1595949196\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 100\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 560 s, 100 iter, 200000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 10.820207745508009\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 100\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.742\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.9443045848797766e-32\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3920220136642456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9362271359568695e-06\n",
      "        policy_loss: -0.00010998296784237027\n",
      "        total_loss: 0.0038140842225402594\n",
      "        vf_explained_var: 0.8425946235656738\n",
      "        vf_loss: 0.003924072254449129\n",
      "    load_time_ms: 1.109\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "    sample_time_ms: 2342.421\n",
      "    update_time_ms: 3.858\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.26666666666667\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.580366215003494\n",
      "    mean_inference_ms: 2.7011332738251292\n",
      "    mean_processing_ms: 1.3020165490973474\n",
      "  time_since_restore: 566.555073261261\n",
      "  time_this_iter_s: 2.101580858230591\n",
      "  time_total_s: 566.555073261261\n",
      "  timestamp: 1595949203\n",
      "  timesteps_since_restore: 206000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 103\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 566 s, 103 iter, 206000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 10.345700410217313\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 105\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.09\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.860761462199441e-33\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4051579236984253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0526051685010316e-06\n",
      "        policy_loss: -0.00016870307445060462\n",
      "        total_loss: 0.0013973055174574256\n",
      "        vf_explained_var: 0.35908693075180054\n",
      "        vf_loss: 0.0015660172794014215\n",
      "    load_time_ms: 1.067\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    sample_time_ms: 2378.015\n",
      "    update_time_ms: 6.385\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.52\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.207652599392786\n",
      "    mean_inference_ms: 2.629338420310741\n",
      "    mean_processing_ms: 1.2792511507460391\n",
      "  time_since_restore: 572.307924747467\n",
      "  time_this_iter_s: 3.5389490127563477\n",
      "  time_total_s: 572.307924747467\n",
      "  timestamp: 1595949209\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 105\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 572 s, 105 iter, 210000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 19.010186783077017\n",
      "  episode_reward_mean: 10.345700410217312\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 105\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.169\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.2325951827749302e-33\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3873440027236938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.0216915070677715e-09\n",
      "        policy_loss: -3.3950806255234056e-07\n",
      "        total_loss: 0.005488107912242413\n",
      "        vf_explained_var: 0.7649140357971191\n",
      "        vf_loss: 0.005488428752869368\n",
      "    load_time_ms: 1.035\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    sample_time_ms: 2356.245\n",
      "    update_time_ms: 6.48\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.3\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 12.207652599392782\n",
      "    mean_inference_ms: 2.629338420310741\n",
      "    mean_processing_ms: 1.2792511507460391\n",
      "  time_since_restore: 578.7403440475464\n",
      "  time_this_iter_s: 2.1628339290618896\n",
      "  time_total_s: 578.7403440475464\n",
      "  timestamp: 1595949215\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 108\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 578 s, 108 iter, 216000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 17.904615709036594\n",
      "  episode_reward_mean: 9.93120291846733\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 110\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.947\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.0814879569373254e-34\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.405070185661316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0810792900883825e-06\n",
      "        policy_loss: -8.135604730341583e-05\n",
      "        total_loss: 0.0017388906562700868\n",
      "        vf_explained_var: 0.47999054193496704\n",
      "        vf_loss: 0.001820234814658761\n",
      "    load_time_ms: 1.161\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    sample_time_ms: 2359.35\n",
      "    update_time_ms: 6.468\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.94000000000001\n",
      "    ram_util_percent: 68.36\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.792384266374825\n",
      "    mean_inference_ms: 2.5477635496764552\n",
      "    mean_processing_ms: 1.2562788864278145\n",
      "  time_since_restore: 584.2023711204529\n",
      "  time_this_iter_s: 3.318986654281616\n",
      "  time_total_s: 584.2023711204529\n",
      "  timestamp: 1595949221\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 110\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 584 s, 110 iter, 220000 ts, 9.93 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-47\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 17.904615709036594\n",
      "  episode_reward_mean: 9.931202918467331\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 110\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.856\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.851859946171657e-35\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.386236310005188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.175658892156207e-06\n",
      "        policy_loss: -8.6168285633903e-05\n",
      "        total_loss: 0.006274751853197813\n",
      "        vf_explained_var: 0.8487256169319153\n",
      "        vf_loss: 0.006360922008752823\n",
      "    load_time_ms: 1.171\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "    sample_time_ms: 2364.176\n",
      "    update_time_ms: 6.536\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.93333333333334\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.792384266374826\n",
      "    mean_inference_ms: 2.5477635496764552\n",
      "    mean_processing_ms: 1.2562788864278145\n",
      "  time_since_restore: 590.5910234451294\n",
      "  time_this_iter_s: 2.1279561519622803\n",
      "  time_total_s: 590.5910234451294\n",
      "  timestamp: 1595949227\n",
      "  timesteps_since_restore: 226000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 113\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 590 s, 113 iter, 226000 ts, 9.93 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 17.05814126995957\n",
      "  episode_reward_mean: 9.54502630538565\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 115\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.475\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.629649865429142e-36\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4041790962219238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.536667355139798e-06\n",
      "        policy_loss: -8.10823476058431e-05\n",
      "        total_loss: 0.0023262768518179655\n",
      "        vf_explained_var: 0.48520904779434204\n",
      "        vf_loss: 0.002407349646091461\n",
      "    load_time_ms: 1.238\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    sample_time_ms: 2338.143\n",
      "    update_time_ms: 4.004\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.139999999999986\n",
      "    ram_util_percent: 68.34\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.477617765591411\n",
      "    mean_inference_ms: 2.4848564792911447\n",
      "    mean_processing_ms: 1.238378372169206\n",
      "  time_since_restore: 596.1070423126221\n",
      "  time_this_iter_s: 3.3247995376586914\n",
      "  time_total_s: 596.1070423126221\n",
      "  timestamp: 1595949232\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 115\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 596 s, 115 iter, 230000 ts, 9.55 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 17.05814126995957\n",
      "  episode_reward_mean: 9.545026305385647\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 115\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.926\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.4074124663572855e-36\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3662570714950562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.167738446383737e-05\n",
      "        policy_loss: -0.00014734936121385545\n",
      "        total_loss: 0.02468075044453144\n",
      "        vf_explained_var: 0.5083833932876587\n",
      "        vf_loss: 0.024828102439641953\n",
      "    load_time_ms: 1.272\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "    sample_time_ms: 2475.554\n",
      "    update_time_ms: 4.639\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.36666666666666\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.477617765591411\n",
      "    mean_inference_ms: 2.4848564792911443\n",
      "    mean_processing_ms: 1.2383783721692065\n",
      "  time_since_restore: 601.7627885341644\n",
      "  time_this_iter_s: 2.228764533996582\n",
      "  time_total_s: 601.7627885341644\n",
      "  timestamp: 1595949238\n",
      "  timesteps_since_restore: 234000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 117\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 601 s, 117 iter, 234000 ts, 9.55 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 9.301205389786766\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 120\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.581\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.009265582946607e-37\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3946462869644165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.611347203033802e-07\n",
      "        policy_loss: -2.312994001840707e-05\n",
      "        total_loss: 0.005134210456162691\n",
      "        vf_explained_var: 0.6289423108100891\n",
      "        vf_loss: 0.005157334264367819\n",
      "    load_time_ms: 1.215\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    sample_time_ms: 2552.429\n",
      "    update_time_ms: 4.899\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.419999999999995\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.21534095629405\n",
      "    mean_inference_ms: 2.431409442844044\n",
      "    mean_processing_ms: 1.223718813902168\n",
      "  time_since_restore: 610.1924519538879\n",
      "  time_this_iter_s: 3.4697344303131104\n",
      "  time_total_s: 610.1924519538879\n",
      "  timestamp: 1595949247\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 120\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 610 s, 120 iter, 240000 ts, 9.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 9.301205389786766\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 120\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.68\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.523163957366517e-38\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3663371801376343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4458715668297373e-05\n",
      "        policy_loss: -0.00014304256183095276\n",
      "        total_loss: 0.018246732652187347\n",
      "        vf_explained_var: 0.6178538799285889\n",
      "        vf_loss: 0.01838977076113224\n",
      "    load_time_ms: 1.231\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    sample_time_ms: 2623.888\n",
      "    update_time_ms: 5.755\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.83333333333333\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 11.215340956294046\n",
      "    mean_inference_ms: 2.431409442844044\n",
      "    mean_processing_ms: 1.2237188139021677\n",
      "  time_since_restore: 615.1778612136841\n",
      "  time_this_iter_s: 2.2664740085601807\n",
      "  time_total_s: 615.1778612136841\n",
      "  timestamp: 1595949252\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 122\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 617 s, 123 iter, 246000 ts, 9.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 9.089798114820722\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 125\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.172\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3972247838974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8916130102297757e-06\n",
      "        policy_loss: -6.471872620750219e-05\n",
      "        total_loss: 0.00431701447814703\n",
      "        vf_explained_var: 0.5929477214813232\n",
      "        vf_loss: 0.004381739534437656\n",
      "    load_time_ms: 1.18\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    sample_time_ms: 2622.093\n",
      "    update_time_ms: 5.856\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.22500000000001\n",
      "    ram_util_percent: 68.375\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.977500822482764\n",
      "    mean_inference_ms: 2.3827038314289215\n",
      "    mean_processing_ms: 1.209131323567823\n",
      "  time_since_restore: 622.7583160400391\n",
      "  time_this_iter_s: 3.2575743198394775\n",
      "  time_total_s: 622.7583160400391\n",
      "  timestamp: 1595949259\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 125\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 622 s, 125 iter, 250000 ts, 9.09 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 9.089798114820718\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 125\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.794\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.382623553276062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.055926077242475e-06\n",
      "        policy_loss: -9.353637324238662e-06\n",
      "        total_loss: 0.005625234451144934\n",
      "        vf_explained_var: 0.8479347229003906\n",
      "        vf_loss: 0.005634600296616554\n",
      "    load_time_ms: 1.191\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    sample_time_ms: 2439.629\n",
      "    update_time_ms: 5.323\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.875\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.977500822482764\n",
      "    mean_inference_ms: 2.3827038314289215\n",
      "    mean_processing_ms: 1.209131323567823\n",
      "  time_since_restore: 629.3101389408112\n",
      "  time_this_iter_s: 2.3087527751922607\n",
      "  time_total_s: 629.3101389408112\n",
      "  timestamp: 1595949266\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 128\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 629 s, 128 iter, 256000 ts, 9.09 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 8.82768690387366\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 130\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.756\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4008994102478027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0319223494880134e-06\n",
      "        policy_loss: -0.00011776352039305493\n",
      "        total_loss: 0.004588684067130089\n",
      "        vf_explained_var: 0.6850935220718384\n",
      "        vf_loss: 0.004706437233835459\n",
      "    load_time_ms: 1.12\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    sample_time_ms: 2416.992\n",
      "    update_time_ms: 5.073\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.775000000000006\n",
      "    ram_util_percent: 68.325\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.752107886097894\n",
      "    mean_inference_ms: 2.3363262138357697\n",
      "    mean_processing_ms: 1.196022781898112\n",
      "  time_since_restore: 634.7368414402008\n",
      "  time_this_iter_s: 3.2621049880981445\n",
      "  time_total_s: 634.7368414402008\n",
      "  timestamp: 1595949271\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 130\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 634 s, 130 iter, 260000 ts, 8.83 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 16.944436589809737\n",
      "  episode_reward_mean: 8.827686903873662\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 130\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.084\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3851758241653442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.52704033604823e-07\n",
      "        policy_loss: 5.023956418881426e-06\n",
      "        total_loss: 0.004683948587626219\n",
      "        vf_explained_var: 0.8487673401832581\n",
      "        vf_loss: 0.004678918980062008\n",
      "    load_time_ms: 1.24\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "    sample_time_ms: 2342.18\n",
      "    update_time_ms: 4.19\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.23333333333333\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.752107886097894\n",
      "    mean_inference_ms: 2.336326213835769\n",
      "    mean_processing_ms: 1.196022781898112\n",
      "  time_since_restore: 641.1333312988281\n",
      "  time_this_iter_s: 2.1064116954803467\n",
      "  time_total_s: 641.1333312988281\n",
      "  timestamp: 1595949278\n",
      "  timesteps_since_restore: 266000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 133\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 641 s, 133 iter, 266000 ts, 8.83 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.737304646084302\n",
      "  episode_reward_mean: 8.575822014809102\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 135\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3993515968322754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.626712204000796e-07\n",
      "        policy_loss: -4.4434545998228714e-05\n",
      "        total_loss: 0.005967352073639631\n",
      "        vf_explained_var: 0.641855001449585\n",
      "        vf_loss: 0.006011782214045525\n",
      "    load_time_ms: 1.252\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    sample_time_ms: 2370.329\n",
      "    update_time_ms: 4.3\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.779999999999994\n",
      "    ram_util_percent: 68.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.570405901576397\n",
      "    mean_inference_ms: 2.299010053795426\n",
      "    mean_processing_ms: 1.185433940428424\n",
      "  time_since_restore: 646.8440692424774\n",
      "  time_this_iter_s: 3.530212163925171\n",
      "  time_total_s: 646.8440692424774\n",
      "  timestamp: 1595949283\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 135\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 646 s, 135 iter, 270000 ts, 8.58 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.737304646084302\n",
      "  episode_reward_mean: 8.575822014809102\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.833\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3779094219207764\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1701285984599963e-06\n",
      "        policy_loss: 5.272006819723174e-05\n",
      "        total_loss: 0.007359491661190987\n",
      "        vf_explained_var: 0.8050089478492737\n",
      "        vf_loss: 0.0073067666962742805\n",
      "    load_time_ms: 1.202\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    sample_time_ms: 2358.591\n",
      "    update_time_ms: 4.238\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5\n",
      "    ram_util_percent: 68.26666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.570405901576393\n",
      "    mean_inference_ms: 2.299010053795426\n",
      "    mean_processing_ms: 1.185433940428424\n",
      "  time_since_restore: 653.274879693985\n",
      "  time_this_iter_s: 2.1281580924987793\n",
      "  time_total_s: 653.274879693985\n",
      "  timestamp: 1595949290\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 138\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 653 s, 138 iter, 276000 ts, 8.58 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-14-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.484351732238368\n",
      "  episode_reward_mean: 8.46123897368288\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 140\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.065\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4004470109939575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7575323454366298e-06\n",
      "        policy_loss: -9.102821059059352e-05\n",
      "        total_loss: 0.0059382058680057526\n",
      "        vf_explained_var: 0.5081009864807129\n",
      "        vf_loss: 0.006029238924384117\n",
      "    load_time_ms: 1.212\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    sample_time_ms: 2362.467\n",
      "    update_time_ms: 4.44\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.62\n",
      "    ram_util_percent: 68.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.410230183925826\n",
      "    mean_inference_ms: 2.26602894218622\n",
      "    mean_processing_ms: 1.1759580706524473\n",
      "  time_since_restore: 658.7454950809479\n",
      "  time_this_iter_s: 3.2833664417266846\n",
      "  time_total_s: 658.7454950809479\n",
      "  timestamp: 1595949295\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 140\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 658 s, 140 iter, 280000 ts, 8.46 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.484351732238368\n",
      "  episode_reward_mean: 8.46123897368288\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 140\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.647\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3830327987670898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.022683702307404e-06\n",
      "        policy_loss: -0.00013335370749700814\n",
      "        total_loss: 0.0049496302381157875\n",
      "        vf_explained_var: 0.8513479828834534\n",
      "        vf_loss: 0.005082982126623392\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "    sample_time_ms: 2379.571\n",
      "    update_time_ms: 4.375\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.46666666666665\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.410230183925828\n",
      "    mean_inference_ms: 2.26602894218622\n",
      "    mean_processing_ms: 1.1759580706524473\n",
      "  time_since_restore: 665.3073093891144\n",
      "  time_this_iter_s: 2.1260592937469482\n",
      "  time_total_s: 665.3073093891144\n",
      "  timestamp: 1595949302\n",
      "  timesteps_since_restore: 286000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 143\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 665 s, 143 iter, 286000 ts, 8.46 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.8982388779047\n",
      "  episode_reward_mean: 8.3147367582927\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 145\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.645\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.403017282485962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5287567950726952e-06\n",
      "        policy_loss: -0.00012957096623722464\n",
      "        total_loss: 0.005657872185111046\n",
      "        vf_explained_var: 0.4765799641609192\n",
      "        vf_loss: 0.0057874685153365135\n",
      "    load_time_ms: 1.1\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    sample_time_ms: 2365.067\n",
      "    update_time_ms: 4.195\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.580000000000005\n",
      "    ram_util_percent: 68.38000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.255062169299652\n",
      "    mean_inference_ms: 2.234069525361489\n",
      "    mean_processing_ms: 1.1669587911813095\n",
      "  time_since_restore: 670.8616418838501\n",
      "  time_this_iter_s: 3.35329532623291\n",
      "  time_total_s: 670.8616418838501\n",
      "  timestamp: 1595949307\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 145\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 670 s, 145 iter, 290000 ts, 8.31 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.8982388779047\n",
      "  episode_reward_mean: 8.3147367582927\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 145\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.038\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3780065774917603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.79681580398028e-07\n",
      "        policy_loss: 1.3565063454734627e-05\n",
      "        total_loss: 0.009015071205794811\n",
      "        vf_explained_var: 0.7960066795349121\n",
      "        vf_loss: 0.009001504629850388\n",
      "    load_time_ms: 1.119\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    sample_time_ms: 2426.964\n",
      "    update_time_ms: 4.036\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.255062169299649\n",
      "    mean_inference_ms: 2.234069525361489\n",
      "    mean_processing_ms: 1.1669587911813095\n",
      "  time_since_restore: 677.9038662910461\n",
      "  time_this_iter_s: 2.1437971591949463\n",
      "  time_total_s: 677.9038662910461\n",
      "  timestamp: 1595949315\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 148\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 677 s, 148 iter, 296000 ts, 8.31 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.301667742452855\n",
      "  episode_reward_mean: 8.297618221001988\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 150\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.955\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3942936658859253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.410015546658542e-07\n",
      "        policy_loss: -3.783631473197602e-05\n",
      "        total_loss: 0.01006294321268797\n",
      "        vf_explained_var: 0.30149227380752563\n",
      "        vf_loss: 0.010100784711539745\n",
      "    load_time_ms: 1.158\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    sample_time_ms: 2504.598\n",
      "    update_time_ms: 4.015\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.04\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.081271693424537\n",
      "    mean_inference_ms: 2.1982904447151297\n",
      "    mean_processing_ms: 1.1567174414866728\n",
      "  time_since_restore: 684.21213555336\n",
      "  time_this_iter_s: 3.6696598529815674\n",
      "  time_total_s: 684.21213555336\n",
      "  timestamp: 1595949321\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 150\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 684 s, 150 iter, 300000 ts, 8.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.301667742452855\n",
      "  episode_reward_mean: 8.297618221001986\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 150\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.54\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3695693016052246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6524493275937857e-06\n",
      "        policy_loss: -3.804779044003226e-05\n",
      "        total_loss: 0.015056273899972439\n",
      "        vf_explained_var: 0.6281827688217163\n",
      "        vf_loss: 0.015094333328306675\n",
      "    load_time_ms: 1.193\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    sample_time_ms: 2577.342\n",
      "    update_time_ms: 5.155\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.925\n",
      "    ram_util_percent: 68.525\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 10.081271693424535\n",
      "    mean_inference_ms: 2.1982904447151297\n",
      "    mean_processing_ms: 1.1567174414866728\n",
      "  time_since_restore: 689.3812913894653\n",
      "  time_this_iter_s: 2.7718069553375244\n",
      "  time_total_s: 689.3812913894653\n",
      "  timestamp: 1595949326\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 152\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 689 s, 152 iter, 304000 ts, 8.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.301667742452855\n",
      "  episode_reward_mean: 8.33683825446169\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 155\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.786\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3926540613174438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2583434454427334e-06\n",
      "        policy_loss: -3.09600836772006e-05\n",
      "        total_loss: 0.011745077557861805\n",
      "        vf_explained_var: 0.3018859028816223\n",
      "        vf_loss: 0.011776048690080643\n",
      "    load_time_ms: 1.148\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    sample_time_ms: 2589.147\n",
      "    update_time_ms: 5.135\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.29999999999999\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.888106528101606\n",
      "    mean_inference_ms: 2.1582974780023245\n",
      "    mean_processing_ms: 1.145279377784243\n",
      "  time_since_restore: 697.1807279586792\n",
      "  time_this_iter_s: 3.259166717529297\n",
      "  time_total_s: 697.1807279586792\n",
      "  timestamp: 1595949334\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 155\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 697 s, 155 iter, 310000 ts, 8.34 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.301667742452855\n",
      "  episode_reward_mean: 8.33683825446169\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 155\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.538\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3730897903442383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.723322202451527e-05\n",
      "        policy_loss: -0.00010806012141983956\n",
      "        total_loss: 0.008571378886699677\n",
      "        vf_explained_var: 0.8618830442428589\n",
      "        vf_loss: 0.008679444901645184\n",
      "    load_time_ms: 1.175\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    sample_time_ms: 2525.534\n",
      "    update_time_ms: 5.147\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.03333333333333\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.88810652810161\n",
      "    mean_inference_ms: 2.1582974780023245\n",
      "    mean_processing_ms: 1.145279377784243\n",
      "  time_since_restore: 703.5842773914337\n",
      "  time_this_iter_s: 2.1342124938964844\n",
      "  time_total_s: 703.5842773914337\n",
      "  timestamp: 1595949340\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 158\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 703 s, 158 iter, 316000 ts, 8.34 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.370181817646186\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 160\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.799\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3895741701126099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0051727485915762e-06\n",
      "        policy_loss: -8.368301496375352e-05\n",
      "        total_loss: 0.01209751795977354\n",
      "        vf_explained_var: 0.5485963821411133\n",
      "        vf_loss: 0.012181201949715614\n",
      "    load_time_ms: 1.122\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    sample_time_ms: 2452.476\n",
      "    update_time_ms: 4.976\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.160000000000004\n",
      "    ram_util_percent: 68.36\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.683691819631555\n",
      "    mean_inference_ms: 2.1160277323111347\n",
      "    mean_processing_ms: 1.1329478903206072\n",
      "  time_since_restore: 709.0998947620392\n",
      "  time_this_iter_s: 3.2589285373687744\n",
      "  time_total_s: 709.0998947620392\n",
      "  timestamp: 1595949346\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 160\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 709 s, 160 iter, 320000 ts, 8.37 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.370181817646188\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 160\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.305\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3755136728286743\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.684826641707332e-07\n",
      "        policy_loss: -3.2267569622490555e-05\n",
      "        total_loss: 0.0060151503421366215\n",
      "        vf_explained_var: 0.7575563192367554\n",
      "        vf_loss: 0.006047413218766451\n",
      "    load_time_ms: 1.146\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "    sample_time_ms: 2385.526\n",
      "    update_time_ms: 3.804\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.29999999999998\n",
      "    ram_util_percent: 68.32499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.683691819631553\n",
      "    mean_inference_ms: 2.116027732311135\n",
      "    mean_processing_ms: 1.1329478903206072\n",
      "  time_since_restore: 715.9934425354004\n",
      "  time_this_iter_s: 2.327345132827759\n",
      "  time_total_s: 715.9934425354004\n",
      "  timestamp: 1595949353\n",
      "  timesteps_since_restore: 326000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 163\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 715 s, 163 iter, 326000 ts, 8.37 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.404641768542573\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 165\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.339\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3876338005065918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.714127276703948e-06\n",
      "        policy_loss: -6.881427543703467e-05\n",
      "        total_loss: 0.015470110811293125\n",
      "        vf_explained_var: 0.31825506687164307\n",
      "        vf_loss: 0.015538924373686314\n",
      "    load_time_ms: 1.412\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    sample_time_ms: 2438.127\n",
      "    update_time_ms: 4.158\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.73999999999999\n",
      "    ram_util_percent: 68.44000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.46069676060889\n",
      "    mean_inference_ms: 2.069930482709546\n",
      "    mean_processing_ms: 1.1197265344208207\n",
      "  time_since_restore: 721.9907033443451\n",
      "  time_this_iter_s: 3.384932518005371\n",
      "  time_total_s: 721.9907033443451\n",
      "  timestamp: 1595949359\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 165\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 721 s, 165 iter, 330000 ts, 8.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.404641768542573\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 165\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3821299076080322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.376407337578712e-06\n",
      "        policy_loss: -0.00015903281746432185\n",
      "        total_loss: 0.003132469253614545\n",
      "        vf_explained_var: 0.8628488183021545\n",
      "        vf_loss: 0.0032915063202381134\n",
      "    load_time_ms: 1.393\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    sample_time_ms: 2468.718\n",
      "    update_time_ms: 4.337\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.43333333333332\n",
      "    ram_util_percent: 68.33333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.460696760608892\n",
      "    mean_inference_ms: 2.0699304827095455\n",
      "    mean_processing_ms: 1.1197265344208207\n",
      "  time_since_restore: 728.7038941383362\n",
      "  time_this_iter_s: 2.1951417922973633\n",
      "  time_total_s: 728.7038941383362\n",
      "  timestamp: 1595949366\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 168\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 728 s, 168 iter, 336000 ts, 8.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.353093515979909\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 170\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.948\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.397125005722046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.5344660318514798e-06\n",
      "        policy_loss: -7.39698443794623e-05\n",
      "        total_loss: 0.012904001399874687\n",
      "        vf_explained_var: 0.33129745721817017\n",
      "        vf_loss: 0.012977967970073223\n",
      "    load_time_ms: 1.517\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    sample_time_ms: 2525.348\n",
      "    update_time_ms: 5.067\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.45\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.244180571998045\n",
      "    mean_inference_ms: 2.0250384493126345\n",
      "    mean_processing_ms: 1.106892803035075\n",
      "  time_since_restore: 734.8285562992096\n",
      "  time_this_iter_s: 3.7044765949249268\n",
      "  time_total_s: 734.8285562992096\n",
      "  timestamp: 1595949372\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 170\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 734 s, 170 iter, 340000 ts, 8.35 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.353093515979907\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 170\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.396\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.364872694015503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5729239041538676e-06\n",
      "        policy_loss: 8.569717465434223e-06\n",
      "        total_loss: 0.009666320867836475\n",
      "        vf_explained_var: 0.7542319297790527\n",
      "        vf_loss: 0.009657747112214565\n",
      "    load_time_ms: 1.462\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "    sample_time_ms: 2490.615\n",
      "    update_time_ms: 5.145\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.93333333333334\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.244180571998045\n",
      "    mean_inference_ms: 2.0250384493126345\n",
      "    mean_processing_ms: 1.106892803035075\n",
      "  time_since_restore: 741.3688147068024\n",
      "  time_this_iter_s: 2.1599504947662354\n",
      "  time_total_s: 741.3688147068024\n",
      "  timestamp: 1595949378\n",
      "  timesteps_since_restore: 346000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 173\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 741 s, 173 iter, 346000 ts, 8.35 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.495304945941514\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 175\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.566\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3820358514785767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.232704668538645e-06\n",
      "        policy_loss: -0.0001354618143523112\n",
      "        total_loss: 0.020672395825386047\n",
      "        vf_explained_var: 0.22341257333755493\n",
      "        vf_loss: 0.020807849243283272\n",
      "    load_time_ms: 1.232\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    sample_time_ms: 2463.838\n",
      "    update_time_ms: 4.792\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.1\n",
      "    ram_util_percent: 68.78\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.036231701130085\n",
      "    mean_inference_ms: 1.9819150176363989\n",
      "    mean_processing_ms: 1.0944872896240951\n",
      "  time_since_restore: 747.029792547226\n",
      "  time_this_iter_s: 3.514482259750366\n",
      "  time_total_s: 747.029792547226\n",
      "  timestamp: 1595949384\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 175\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 747 s, 175 iter, 350000 ts, 8.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.495304945941516\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 175\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.092\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3572161197662354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.645534762763418e-06\n",
      "        policy_loss: -9.969138773158193e-05\n",
      "        total_loss: 0.015523792244493961\n",
      "        vf_explained_var: 0.6730011701583862\n",
      "        vf_loss: 0.015623478218913078\n",
      "    load_time_ms: 1.245\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "    sample_time_ms: 2604.393\n",
      "    update_time_ms: 4.797\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.08000000000001\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.036231701130085\n",
      "    mean_inference_ms: 1.9819150176363984\n",
      "    mean_processing_ms: 1.0944872896240954\n",
      "  time_since_restore: 752.9609491825104\n",
      "  time_this_iter_s: 3.016226291656494\n",
      "  time_total_s: 752.9609491825104\n",
      "  timestamp: 1595949390\n",
      "  timesteps_since_restore: 354000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 177\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 752 s, 177 iter, 354000 ts, 8.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.495304945941516\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 175\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.53\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3906606435775757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.351139068603516e-06\n",
      "        policy_loss: -0.00041080964729189873\n",
      "        total_loss: 0.001323311822488904\n",
      "        vf_explained_var: 0.9175403714179993\n",
      "        vf_loss: 0.001734117860905826\n",
      "    load_time_ms: 1.502\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "    sample_time_ms: 2707.775\n",
      "    update_time_ms: 4.929\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.7\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 9.036231701130085\n",
      "    mean_inference_ms: 1.9819150176363984\n",
      "    mean_processing_ms: 1.0944872896240954\n",
      "  time_since_restore: 758.6549100875854\n",
      "  time_this_iter_s: 3.0935864448547363\n",
      "  time_total_s: 758.6549100875854\n",
      "  timestamp: 1595949396\n",
      "  timesteps_since_restore: 358000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 179\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 758 s, 179 iter, 358000 ts, 8.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.550830784299986\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 180\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.418\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.341797113418579\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.346140134563029e-07\n",
      "        policy_loss: -4.3395997636253014e-05\n",
      "        total_loss: 0.03933565318584442\n",
      "        vf_explained_var: 0.3263757824897766\n",
      "        vf_loss: 0.03937903419137001\n",
      "    load_time_ms: 1.488\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "    sample_time_ms: 2790.212\n",
      "    update_time_ms: 4.916\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.3\n",
      "    ram_util_percent: 69.23333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.838333793292222\n",
      "    mean_inference_ms: 1.940944356584182\n",
      "    mean_processing_ms: 1.0827030743035662\n",
      "  time_since_restore: 765.4025053977966\n",
      "  time_this_iter_s: 2.629453659057617\n",
      "  time_total_s: 765.4025053977966\n",
      "  timestamp: 1595949402\n",
      "  timesteps_since_restore: 362000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 181\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 765 s, 181 iter, 362000 ts, 8.55 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.550830784299986\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 180\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3773247003555298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.7448107832460664e-06\n",
      "        policy_loss: -0.00010487270628800616\n",
      "        total_loss: 0.0047154296189546585\n",
      "        vf_explained_var: 0.9205631613731384\n",
      "        vf_loss: 0.004820310510694981\n",
      "    load_time_ms: 1.559\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    sample_time_ms: 2875.853\n",
      "    update_time_ms: 5.228\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.175\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.838333793292222\n",
      "    mean_inference_ms: 1.940944356584182\n",
      "    mean_processing_ms: 1.0827030743035662\n",
      "  time_since_restore: 772.7546679973602\n",
      "  time_this_iter_s: 2.5424232482910156\n",
      "  time_total_s: 772.7546679973602\n",
      "  timestamp: 1595949410\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 184\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 772 s, 184 iter, 368000 ts, 8.55 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-16-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.686049198062273\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 185\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.151\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.337051272392273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0848343663383275e-06\n",
      "        policy_loss: -4.5387267164187506e-05\n",
      "        total_loss: 0.03598617762327194\n",
      "        vf_explained_var: 0.4451311230659485\n",
      "        vf_loss: 0.036031574010849\n",
      "    load_time_ms: 1.582\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    sample_time_ms: 2872.991\n",
      "    update_time_ms: 5.925\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.724999999999994\n",
      "    ram_util_percent: 69.17500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.657517524506801\n",
      "    mean_inference_ms: 1.9035165230378106\n",
      "    mean_processing_ms: 1.0719728722348767\n",
      "  time_since_restore: 779.2033007144928\n",
      "  time_this_iter_s: 2.5611793994903564\n",
      "  time_total_s: 779.2033007144928\n",
      "  timestamp: 1595949416\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 186\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 779 s, 186 iter, 372000 ts, 8.69 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.686049198062273\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 185\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.286\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3809325695037842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.2930374800343998e-06\n",
      "        policy_loss: -0.00024164009664673358\n",
      "        total_loss: 0.0028820906300097704\n",
      "        vf_explained_var: 0.9242920875549316\n",
      "        vf_loss: 0.0031237308867275715\n",
      "    load_time_ms: 1.249\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "    sample_time_ms: 2680.725\n",
      "    update_time_ms: 5.715\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.625\n",
      "    ram_util_percent: 69.25\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.657517524506801\n",
      "    mean_inference_ms: 1.9035165230378106\n",
      "    mean_processing_ms: 1.0719728722348767\n",
      "  time_since_restore: 785.9101221561432\n",
      "  time_this_iter_s: 2.379594087600708\n",
      "  time_total_s: 785.9101221561432\n",
      "  timestamp: 1595949423\n",
      "  timesteps_since_restore: 378000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 189\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 785 s, 189 iter, 378000 ts, 8.69 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.742151223572804\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 190\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.031\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3321760892868042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1332107027992606e-05\n",
      "        policy_loss: -0.000417611125158146\n",
      "        total_loss: 0.03853656351566315\n",
      "        vf_explained_var: 0.350008487701416\n",
      "        vf_loss: 0.03895416855812073\n",
      "    load_time_ms: 1.243\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "    sample_time_ms: 2644.109\n",
      "    update_time_ms: 5.122\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.45\n",
      "    ram_util_percent: 69.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.48969601234042\n",
      "    mean_inference_ms: 1.8687561758448523\n",
      "    mean_processing_ms: 1.0619688003564713\n",
      "  time_since_restore: 792.2830781936646\n",
      "  time_this_iter_s: 2.430422306060791\n",
      "  time_total_s: 792.2830781936646\n",
      "  timestamp: 1595949429\n",
      "  timesteps_since_restore: 382000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 191\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 792 s, 191 iter, 382000 ts, 8.74 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.742151223572804\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 190\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.888\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.356424331665039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.912704298476456e-06\n",
      "        policy_loss: 6.949806265765801e-05\n",
      "        total_loss: 0.010578026995062828\n",
      "        vf_explained_var: 0.7773714065551758\n",
      "        vf_loss: 0.010508516803383827\n",
      "    load_time_ms: 1.143\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "    sample_time_ms: 2667.008\n",
      "    update_time_ms: 5.291\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.49999999999999\n",
      "    ram_util_percent: 69.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.48969601234042\n",
      "    mean_inference_ms: 1.8687561758448523\n",
      "    mean_processing_ms: 1.0619688003564713\n",
      "  time_since_restore: 797.3158855438232\n",
      "  time_this_iter_s: 2.3244335651397705\n",
      "  time_total_s: 797.3158855438232\n",
      "  timestamp: 1595949434\n",
      "  timesteps_since_restore: 386000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 193\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 797 s, 193 iter, 386000 ts, 8.74 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.88335435897704\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 195\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.381540060043335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1984706134171574e-06\n",
      "        policy_loss: -2.250122997793369e-05\n",
      "        total_loss: 0.027733732014894485\n",
      "        vf_explained_var: 0.42197543382644653\n",
      "        vf_loss: 0.027756236493587494\n",
      "    load_time_ms: 1.226\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    sample_time_ms: 2580.74\n",
      "    update_time_ms: 5.204\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.580000000000005\n",
      "    ram_util_percent: 68.66000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.335435862180226\n",
      "    mean_inference_ms: 1.8368441580191799\n",
      "    mean_processing_ms: 1.0528075685877012\n",
      "  time_since_restore: 802.8599791526794\n",
      "  time_this_iter_s: 3.3747878074645996\n",
      "  time_total_s: 802.8599791526794\n",
      "  timestamp: 1595949440\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 195\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 802 s, 195 iter, 390000 ts, 8.88 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.883354358977039\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 195\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3503713607788086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.919899427273776e-06\n",
      "        policy_loss: -2.923250212916173e-05\n",
      "        total_loss: 0.013699603267014027\n",
      "        vf_explained_var: 0.7593136429786682\n",
      "        vf_loss: 0.01372882816940546\n",
      "    load_time_ms: 1.335\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "    sample_time_ms: 2606.088\n",
      "    update_time_ms: 4.853\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.07499999999999\n",
      "    ram_util_percent: 68.625\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.335435862180224\n",
      "    mean_inference_ms: 1.8368441580191794\n",
      "    mean_processing_ms: 1.052807568587701\n",
      "  time_since_restore: 807.9113318920135\n",
      "  time_this_iter_s: 2.636735200881958\n",
      "  time_total_s: 807.9113318920135\n",
      "  timestamp: 1595949445\n",
      "  timesteps_since_restore: 394000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 197\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 807 s, 197 iter, 394000 ts, 8.88 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.883354358977039\n",
      "  episode_reward_min: 4.9059291346737846\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 195\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.388\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3843467235565186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.840919253481843e-06\n",
      "        policy_loss: -0.00018675136379897594\n",
      "        total_loss: 0.0024803639389574528\n",
      "        vf_explained_var: 0.9606685042381287\n",
      "        vf_loss: 0.0026671248488128185\n",
      "    load_time_ms: 1.353\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "    sample_time_ms: 2766.04\n",
      "    update_time_ms: 5.395\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.7\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.335435862180224\n",
      "    mean_inference_ms: 1.8368441580191794\n",
      "    mean_processing_ms: 1.052807568587701\n",
      "  time_since_restore: 814.0276958942413\n",
      "  time_this_iter_s: 2.301616668701172\n",
      "  time_total_s: 814.0276958942413\n",
      "  timestamp: 1595949451\n",
      "  timesteps_since_restore: 398000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 199\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 814 s, 199 iter, 398000 ts, 8.88 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.982403657785119\n",
      "  episode_reward_min: 6.190312295851021\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 200\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.519\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3299001455307007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0859072290259064e-06\n",
      "        policy_loss: -5.5011987569741905e-05\n",
      "        total_loss: 0.03569944202899933\n",
      "        vf_explained_var: 0.46817100048065186\n",
      "        vf_loss: 0.035754457116127014\n",
      "    load_time_ms: 1.395\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "    sample_time_ms: 2711.728\n",
      "    update_time_ms: 5.17\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53333333333334\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.193984793464363\n",
      "    mean_inference_ms: 1.8076659324201636\n",
      "    mean_processing_ms: 1.0444479003087688\n",
      "  time_since_restore: 819.8453643321991\n",
      "  time_this_iter_s: 2.285719156265259\n",
      "  time_total_s: 819.8453643321991\n",
      "  timestamp: 1595949457\n",
      "  timesteps_since_restore: 402000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 201\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 819 s, 201 iter, 402000 ts, 8.98 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 8.982403657785119\n",
      "  episode_reward_min: 6.190312295851021\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 200\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.846\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3820075988769531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0328461687313393e-06\n",
      "        policy_loss: -0.00023574828810524195\n",
      "        total_loss: 0.003107030875980854\n",
      "        vf_explained_var: 0.9492619037628174\n",
      "        vf_loss: 0.003342778654769063\n",
      "    load_time_ms: 1.498\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    sample_time_ms: 2764.459\n",
      "    update_time_ms: 7.16\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.13999999999999\n",
      "    ram_util_percent: 68.66\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.193984793464363\n",
      "    mean_inference_ms: 1.8076659324201636\n",
      "    mean_processing_ms: 1.0444479003087688\n",
      "  time_since_restore: 827.6535966396332\n",
      "  time_this_iter_s: 3.1397206783294678\n",
      "  time_total_s: 827.6535966396332\n",
      "  timestamp: 1595949465\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 204\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 827 s, 204 iter, 408000 ts, 8.98 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.08232091570031\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 205\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.936\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3298211097717285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.734715836704709e-06\n",
      "        policy_loss: -0.00026028251159004867\n",
      "        total_loss: 0.035141732543706894\n",
      "        vf_explained_var: 0.4008041024208069\n",
      "        vf_loss: 0.03540199622511864\n",
      "    load_time_ms: 1.376\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    sample_time_ms: 2748.358\n",
      "    update_time_ms: 6.932\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.675000000000004\n",
      "    ram_util_percent: 68.725\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.063440430962709\n",
      "    mean_inference_ms: 1.7807764969325435\n",
      "    mean_processing_ms: 1.0367482728012922\n",
      "  time_since_restore: 833.2515604496002\n",
      "  time_this_iter_s: 2.296504020690918\n",
      "  time_total_s: 833.2515604496002\n",
      "  timestamp: 1595949470\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 206\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 833 s, 206 iter, 412000 ts, 9.08 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.08232091570031\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 205\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.647\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.364389419555664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7683674943546066e-06\n",
      "        policy_loss: -3.3851621992653236e-05\n",
      "        total_loss: 0.006242743693292141\n",
      "        vf_explained_var: 0.8402992486953735\n",
      "        vf_loss: 0.006276593077927828\n",
      "    load_time_ms: 1.245\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    sample_time_ms: 2638.211\n",
      "    update_time_ms: 6.547\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.475\n",
      "    ram_util_percent: 68.65\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.063440430962709\n",
      "    mean_inference_ms: 1.7807764969325435\n",
      "    mean_processing_ms: 1.0367482728012922\n",
      "  time_since_restore: 838.5530557632446\n",
      "  time_this_iter_s: 2.7139792442321777\n",
      "  time_total_s: 838.5530557632446\n",
      "  timestamp: 1595949476\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 208\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 838 s, 208 iter, 416000 ts, 9.08 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.157647597629948\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 210\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.793\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3893017768859863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0869970285275485e-06\n",
      "        policy_loss: 4.04548654842074e-06\n",
      "        total_loss: 0.0317668616771698\n",
      "        vf_explained_var: 0.2417566180229187\n",
      "        vf_loss: 0.03176281973719597\n",
      "    load_time_ms: 1.211\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    sample_time_ms: 2692.451\n",
      "    update_time_ms: 6.619\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.46000000000001\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.9431490420934825\n",
      "    mean_inference_ms: 1.7560841370044076\n",
      "    mean_processing_ms: 1.0296648418495342\n",
      "  time_since_restore: 845.0036418437958\n",
      "  time_this_iter_s: 4.032524347305298\n",
      "  time_total_s: 845.0036418437958\n",
      "  timestamp: 1595949482\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 210\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 845 s, 210 iter, 420000 ts, 9.16 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.157647597629948\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 210\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.632\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3551007509231567\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.031162330240477e-05\n",
      "        policy_loss: -0.00020885086269117892\n",
      "        total_loss: 0.007774362340569496\n",
      "        vf_explained_var: 0.8209831714630127\n",
      "        vf_loss: 0.007983220741152763\n",
      "    load_time_ms: 1.091\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "    sample_time_ms: 2698.748\n",
      "    update_time_ms: 8.08\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.36666666666666\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.9431490420934825\n",
      "    mean_inference_ms: 1.7560841370044082\n",
      "    mean_processing_ms: 1.0296648418495342\n",
      "  time_since_restore: 851.9921410083771\n",
      "  time_this_iter_s: 2.2087202072143555\n",
      "  time_total_s: 851.9921410083771\n",
      "  timestamp: 1595949489\n",
      "  timesteps_since_restore: 426000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 213\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 851 s, 213 iter, 426000 ts, 9.16 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.27567058628179\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 215\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.486\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3738065958023071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.0015875129029155e-06\n",
      "        policy_loss: -7.535934400948463e-06\n",
      "        total_loss: 0.03792247921228409\n",
      "        vf_explained_var: 0.1877223253250122\n",
      "        vf_loss: 0.03793001547455788\n",
      "    load_time_ms: 1.097\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    sample_time_ms: 2664.213\n",
      "    update_time_ms: 5.647\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.54\n",
      "    ram_util_percent: 68.66\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.831732270375542\n",
      "    mean_inference_ms: 1.7332907531547201\n",
      "    mean_processing_ms: 1.023082687331631\n",
      "  time_since_restore: 858.0603227615356\n",
      "  time_this_iter_s: 3.747924566268921\n",
      "  time_total_s: 858.0603227615356\n",
      "  timestamp: 1595949495\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 215\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 858 s, 215 iter, 430000 ts, 9.28 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.275670586281787\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 215\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.489\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.35269033908844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.033943916212593e-07\n",
      "        policy_loss: 2.5487661332590505e-05\n",
      "        total_loss: 0.006431755144149065\n",
      "        vf_explained_var: 0.8573070764541626\n",
      "        vf_loss: 0.0064062634482979774\n",
      "    load_time_ms: 1.09\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    sample_time_ms: 2589.339\n",
      "    update_time_ms: 5.563\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.831732270375543\n",
      "    mean_inference_ms: 1.7332907531547201\n",
      "    mean_processing_ms: 1.023082687331631\n",
      "  time_since_restore: 864.9068040847778\n",
      "  time_this_iter_s: 2.1318602561950684\n",
      "  time_total_s: 864.9068040847778\n",
      "  timestamp: 1595949502\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 218\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 864 s, 218 iter, 436000 ts, 9.28 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.272343476364355\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 220\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.462\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3776780366897583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5153884760366054e-06\n",
      "        policy_loss: -2.2612572138314135e-05\n",
      "        total_loss: 0.04026082530617714\n",
      "        vf_explained_var: 0.30045342445373535\n",
      "        vf_loss: 0.0402834452688694\n",
      "    load_time_ms: 1.052\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    sample_time_ms: 2552.122\n",
      "    update_time_ms: 5.529\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6\n",
      "    ram_util_percent: 68.80000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.726554732455155\n",
      "    mean_inference_ms: 1.7117865390793137\n",
      "    mean_processing_ms: 1.0168585618114483\n",
      "  time_since_restore: 870.9137523174286\n",
      "  time_this_iter_s: 3.560149669647217\n",
      "  time_total_s: 870.9137523174286\n",
      "  timestamp: 1595949508\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 220\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 870 s, 220 iter, 440000 ts, 9.27 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.272343476364355\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 220\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.876\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.335416316986084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3263374967209529e-05\n",
      "        policy_loss: -0.00014250754611566663\n",
      "        total_loss: 0.014761787839233875\n",
      "        vf_explained_var: 0.7086604833602905\n",
      "        vf_loss: 0.014904300682246685\n",
      "    load_time_ms: 1.063\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    sample_time_ms: 2586.046\n",
      "    update_time_ms: 4.825\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.525\n",
      "    ram_util_percent: 68.725\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.726554732455154\n",
      "    mean_inference_ms: 1.7117865390793137\n",
      "    mean_processing_ms: 1.0168585618114483\n",
      "  time_since_restore: 876.0356986522675\n",
      "  time_this_iter_s: 2.533224582672119\n",
      "  time_total_s: 876.0356986522675\n",
      "  timestamp: 1595949513\n",
      "  timesteps_since_restore: 444000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 222\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 876 s, 222 iter, 444000 ts, 9.27 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.272343476364355\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 220\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.073\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3663586378097534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2244283880136209e-06\n",
      "        policy_loss: -0.0001047668483806774\n",
      "        total_loss: 0.003347322577610612\n",
      "        vf_explained_var: 0.9140421748161316\n",
      "        vf_loss: 0.0034520833287388086\n",
      "    load_time_ms: 1.318\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    sample_time_ms: 2783.435\n",
      "    update_time_ms: 5.217\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.47999999999999\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.726554732455154\n",
      "    mean_inference_ms: 1.7117865390793137\n",
      "    mean_processing_ms: 1.0168585618114483\n",
      "  time_since_restore: 882.6352686882019\n",
      "  time_this_iter_s: 3.392613410949707\n",
      "  time_total_s: 882.6352686882019\n",
      "  timestamp: 1595949520\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 224\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 882 s, 224 iter, 448000 ts, 9.27 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.348099136531903\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 225\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.216\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3136177062988281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6793668692116626e-05\n",
      "        policy_loss: -0.000259057036601007\n",
      "        total_loss: 0.03402729332447052\n",
      "        vf_explained_var: 0.5039041042327881\n",
      "        vf_loss: 0.034286342561244965\n",
      "    load_time_ms: 1.381\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    sample_time_ms: 2787.397\n",
      "    update_time_ms: 6.373\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.66666666666667\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.629128526383641\n",
      "    mean_inference_ms: 1.6919271913972924\n",
      "    mean_processing_ms: 1.0110928298363977\n",
      "  time_since_restore: 888.8589308261871\n",
      "  time_this_iter_s: 2.410759687423706\n",
      "  time_total_s: 888.8589308261871\n",
      "  timestamp: 1595949526\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 226\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 888 s, 226 iter, 452000 ts, 9.35 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.348099136531903\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 225\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.804\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3665920495986938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8650889614946209e-06\n",
      "        policy_loss: -0.00019047355453949422\n",
      "        total_loss: 0.002719196258112788\n",
      "        vf_explained_var: 0.9499526023864746\n",
      "        vf_loss: 0.0029096670914441347\n",
      "    load_time_ms: 1.449\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "    sample_time_ms: 2825.696\n",
      "    update_time_ms: 6.477\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.05\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.629128526383641\n",
      "    mean_inference_ms: 1.6919271913972924\n",
      "    mean_processing_ms: 1.0110928298363977\n",
      "  time_since_restore: 896.1400022506714\n",
      "  time_this_iter_s: 2.3947105407714844\n",
      "  time_total_s: 896.1400022506714\n",
      "  timestamp: 1595949534\n",
      "  timesteps_since_restore: 458000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 229\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 896 s, 229 iter, 458000 ts, 9.35 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.423203887598879\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 230\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.644\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.308448314666748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4984011613705661e-06\n",
      "        policy_loss: -5.343055818229914e-05\n",
      "        total_loss: 0.033947210758924484\n",
      "        vf_explained_var: 0.43706178665161133\n",
      "        vf_loss: 0.03400065004825592\n",
      "    load_time_ms: 1.568\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "    sample_time_ms: 2837.745\n",
      "    update_time_ms: 6.787\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.6\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.538473266306434\n",
      "    mean_inference_ms: 1.6735163826291357\n",
      "    mean_processing_ms: 1.0057225282537798\n",
      "  time_since_restore: 902.4406917095184\n",
      "  time_this_iter_s: 2.4657654762268066\n",
      "  time_total_s: 902.4406917095184\n",
      "  timestamp: 1595949540\n",
      "  timesteps_since_restore: 462000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 231\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 902 s, 231 iter, 462000 ts, 9.42 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.423203887598879\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 230\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.353\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3506733179092407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.527869790152181e-05\n",
      "        policy_loss: -0.000334568991092965\n",
      "        total_loss: 0.0049047451466321945\n",
      "        vf_explained_var: 0.863880455493927\n",
      "        vf_loss: 0.005239300429821014\n",
      "    load_time_ms: 1.386\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "    sample_time_ms: 2777.805\n",
      "    update_time_ms: 7.428\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.17500000000001\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.538473266306434\n",
      "    mean_inference_ms: 1.6735163826291357\n",
      "    mean_processing_ms: 1.0057225282537798\n",
      "  time_since_restore: 907.5699858665466\n",
      "  time_this_iter_s: 2.6142778396606445\n",
      "  time_total_s: 907.5699858665466\n",
      "  timestamp: 1595949545\n",
      "  timesteps_since_restore: 466000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 233\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 907 s, 233 iter, 466000 ts, 9.42 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.489808301343112\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 235\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.69\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.371738314628601\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9935071122745285e-06\n",
      "        policy_loss: -2.441692413412966e-05\n",
      "        total_loss: 0.04753691330552101\n",
      "        vf_explained_var: 0.27674996852874756\n",
      "        vf_loss: 0.047561317682266235\n",
      "    load_time_ms: 1.317\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    sample_time_ms: 2697.542\n",
      "    update_time_ms: 6.434\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.96\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.453969705817782\n",
      "    mean_inference_ms: 1.656416564221944\n",
      "    mean_processing_ms: 1.0007749895908533\n",
      "  time_since_restore: 913.9107823371887\n",
      "  time_this_iter_s: 3.761559009552002\n",
      "  time_total_s: 913.9107823371887\n",
      "  timestamp: 1595949551\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 235\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 913 s, 235 iter, 470000 ts, 9.49 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.489808301343116\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 235\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.71\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3509777784347534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.75429998106847e-07\n",
      "        policy_loss: -0.00012064314068993554\n",
      "        total_loss: 0.0037510786205530167\n",
      "        vf_explained_var: 0.9018101692199707\n",
      "        vf_loss: 0.0038717221468687057\n",
      "    load_time_ms: 1.35\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    sample_time_ms: 2652.931\n",
      "    update_time_ms: 5.812\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.96666666666667\n",
      "    ram_util_percent: 68.66666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.453969705817782\n",
      "    mean_inference_ms: 1.6564165642219442\n",
      "    mean_processing_ms: 1.000774989590853\n",
      "  time_since_restore: 920.7562630176544\n",
      "  time_this_iter_s: 2.2299578189849854\n",
      "  time_total_s: 920.7562630176544\n",
      "  timestamp: 1595949558\n",
      "  timesteps_since_restore: 476000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 238\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 920 s, 238 iter, 476000 ts, 9.49 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.535045224494615\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 240\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.641\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3710964918136597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.859647666919045e-06\n",
      "        policy_loss: -8.463620906695724e-05\n",
      "        total_loss: 0.04866915941238403\n",
      "        vf_explained_var: 0.2804677486419678\n",
      "        vf_loss: 0.04875378683209419\n",
      "    load_time_ms: 1.244\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    sample_time_ms: 2659.41\n",
      "    update_time_ms: 5.798\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.75\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.375023442417716\n",
      "    mean_inference_ms: 1.6405191582602767\n",
      "    mean_processing_ms: 0.9961402966451616\n",
      "  time_since_restore: 927.0527489185333\n",
      "  time_this_iter_s: 3.951157569885254\n",
      "  time_total_s: 927.0527489185333\n",
      "  timestamp: 1595949565\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 240\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 927 s, 240 iter, 480000 ts, 9.54 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.535045224494612\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 240\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.968\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3223072290420532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.1237215049914084e-06\n",
      "        policy_loss: -7.431364065269008e-05\n",
      "        total_loss: 0.013711275532841682\n",
      "        vf_explained_var: 0.7548965215682983\n",
      "        vf_loss: 0.013785598799586296\n",
      "    load_time_ms: 1.189\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    sample_time_ms: 2674.786\n",
      "    update_time_ms: 5.079\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.73333333333333\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.3750234424177155\n",
      "    mean_inference_ms: 1.6405191582602763\n",
      "    mean_processing_ms: 0.9961402966451616\n",
      "  time_since_restore: 932.1052868366241\n",
      "  time_this_iter_s: 2.3833138942718506\n",
      "  time_total_s: 932.1052868366241\n",
      "  timestamp: 1595949570\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 242\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 932 s, 242 iter, 484000 ts, 9.54 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.61422120024245\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 245\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.596\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3700876235961914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8564929834828945e-06\n",
      "        policy_loss: -4.398536839289591e-05\n",
      "        total_loss: 0.05370713770389557\n",
      "        vf_explained_var: 0.23191964626312256\n",
      "        vf_loss: 0.053751133382320404\n",
      "    load_time_ms: 1.25\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    sample_time_ms: 2623.329\n",
      "    update_time_ms: 4.227\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.04\n",
      "    ram_util_percent: 68.88\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.3011981810784246\n",
      "    mean_inference_ms: 1.6257012522173582\n",
      "    mean_processing_ms: 0.9918301045385879\n",
      "  time_since_restore: 940.5596940517426\n",
      "  time_this_iter_s: 3.5579240322113037\n",
      "  time_total_s: 940.5596940517426\n",
      "  timestamp: 1595949578\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 245\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 940 s, 245 iter, 490000 ts, 9.61 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.614221200242453\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 245\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.549\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3136531114578247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.668076846632175e-05\n",
      "        policy_loss: -0.000281434302451089\n",
      "        total_loss: 0.014449200592935085\n",
      "        vf_explained_var: 0.652132511138916\n",
      "        vf_loss: 0.014730635099112988\n",
      "    load_time_ms: 1.362\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "    sample_time_ms: 2721.387\n",
      "    update_time_ms: 4.605\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.83333333333333\n",
      "    ram_util_percent: 69.13333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.301198181078423\n",
      "    mean_inference_ms: 1.6257012522173588\n",
      "    mean_processing_ms: 0.9918301045385881\n",
      "  time_since_restore: 946.1588432788849\n",
      "  time_this_iter_s: 2.3298816680908203\n",
      "  time_total_s: 946.1588432788849\n",
      "  timestamp: 1595949584\n",
      "  timesteps_since_restore: 494000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 247\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 946 s, 247 iter, 494000 ts, 9.61 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.630563260384422\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 250\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.128\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3634657859802246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2830070065538166e-06\n",
      "        policy_loss: -3.3069609344238415e-05\n",
      "        total_loss: 0.05999012291431427\n",
      "        vf_explained_var: 0.12371021509170532\n",
      "        vf_loss: 0.06002315133810043\n",
      "    load_time_ms: 1.452\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    sample_time_ms: 2711.41\n",
      "    update_time_ms: 5.103\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.46\n",
      "    ram_util_percent: 68.82000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.231565633526998\n",
      "    mean_inference_ms: 1.6117418351132837\n",
      "    mean_processing_ms: 0.9877644555714208\n",
      "  time_since_restore: 954.5852208137512\n",
      "  time_this_iter_s: 3.5899901390075684\n",
      "  time_total_s: 954.5852208137512\n",
      "  timestamp: 1595949592\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 250\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 954 s, 250 iter, 500000 ts, 9.63 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.630563260384422\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 250\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.862\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3077542781829834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.384040948934853e-06\n",
      "        policy_loss: 2.2202015315997414e-05\n",
      "        total_loss: 0.01636091060936451\n",
      "        vf_explained_var: 0.6483395099639893\n",
      "        vf_loss: 0.01633869856595993\n",
      "    load_time_ms: 1.486\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    sample_time_ms: 2727.204\n",
      "    update_time_ms: 4.914\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.775\n",
      "    ram_util_percent: 68.725\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.231565633526999\n",
      "    mean_inference_ms: 1.6117418351132833\n",
      "    mean_processing_ms: 0.9877644555714207\n",
      "  time_since_restore: 959.8168187141418\n",
      "  time_this_iter_s: 2.760334014892578\n",
      "  time_total_s: 959.8168187141418\n",
      "  timestamp: 1595949598\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 252\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 959 s, 252 iter, 504000 ts, 9.63 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.679731236655618\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 255\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.222\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3717982769012451\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.201872004865436e-06\n",
      "        policy_loss: -8.2963946624659e-05\n",
      "        total_loss: 0.056218285113573074\n",
      "        vf_explained_var: 0.17370301485061646\n",
      "        vf_loss: 0.05630124732851982\n",
      "    load_time_ms: 1.424\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    sample_time_ms: 2756.384\n",
      "    update_time_ms: 5.256\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.08000000000002\n",
      "    ram_util_percent: 68.55999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.165874249738755\n",
      "    mean_inference_ms: 1.5986260969811297\n",
      "    mean_processing_ms: 0.9839450402022162\n",
      "  time_since_restore: 968.5645279884338\n",
      "  time_this_iter_s: 3.929013967514038\n",
      "  time_total_s: 968.5645279884338\n",
      "  timestamp: 1595949606\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 255\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 968 s, 255 iter, 510000 ts, 9.68 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.679731236655618\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 255\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.452\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3192235231399536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.578883691574447e-06\n",
      "        policy_loss: -0.00017385005776304752\n",
      "        total_loss: 0.008896680548787117\n",
      "        vf_explained_var: 0.7721288800239563\n",
      "        vf_loss: 0.009070520289242268\n",
      "    load_time_ms: 1.294\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "    sample_time_ms: 2794.672\n",
      "    update_time_ms: 5.04\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.44000000000001\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.165874249738755\n",
      "    mean_inference_ms: 1.5986260969811297\n",
      "    mean_processing_ms: 0.9839450402022162\n",
      "  time_since_restore: 974.5469326972961\n",
      "  time_this_iter_s: 3.024548053741455\n",
      "  time_total_s: 974.5469326972961\n",
      "  timestamp: 1595949612\n",
      "  timesteps_since_restore: 514000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 257\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 974 s, 257 iter, 514000 ts, 9.68 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.783670247790553\n",
      "  episode_reward_mean: 9.679731236655618\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 255\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.26\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3608286380767822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.663467279897304e-06\n",
      "        policy_loss: -0.0005331513821147382\n",
      "        total_loss: 0.0012274389155209064\n",
      "        vf_explained_var: 0.9052333235740662\n",
      "        vf_loss: 0.0017605888424441218\n",
      "    load_time_ms: 1.181\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "    sample_time_ms: 2814.395\n",
      "    update_time_ms: 4.795\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.6\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.165874249738755\n",
      "    mean_inference_ms: 1.5986260969811297\n",
      "    mean_processing_ms: 0.9839450402022162\n",
      "  time_since_restore: 979.5569760799408\n",
      "  time_this_iter_s: 2.4964373111724854\n",
      "  time_total_s: 979.5569760799408\n",
      "  timestamp: 1595949617\n",
      "  timesteps_since_restore: 518000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 259\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 979 s, 259 iter, 518000 ts, 9.68 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.647324598124946\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 260\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.222\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2706210613250732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.119928694810369e-06\n",
      "        policy_loss: 3.510188980726525e-05\n",
      "        total_loss: 0.03244008496403694\n",
      "        vf_explained_var: 0.45579034090042114\n",
      "        vf_loss: 0.03240497410297394\n",
      "    load_time_ms: 1.145\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "    sample_time_ms: 2813.254\n",
      "    update_time_ms: 4.583\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.95\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.104761160837459\n",
      "    mean_inference_ms: 1.5865051503913625\n",
      "    mean_processing_ms: 0.9804056878320833\n",
      "  time_since_restore: 985.6028208732605\n",
      "  time_this_iter_s: 2.448857545852661\n",
      "  time_total_s: 985.6028208732605\n",
      "  timestamp: 1595949623\n",
      "  timesteps_since_restore: 522000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 261\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 985 s, 261 iter, 522000 ts, 9.65 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.647324598124946\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 260\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.391\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3273974657058716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.2175605459488e-07\n",
      "        policy_loss: -2.3842811060603708e-05\n",
      "        total_loss: 0.005575559567660093\n",
      "        vf_explained_var: 0.8936604857444763\n",
      "        vf_loss: 0.00559941167011857\n",
      "    load_time_ms: 1.131\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "    sample_time_ms: 2801.724\n",
      "    update_time_ms: 4.555\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.725\n",
      "    ram_util_percent: 68.675\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.104761160837459\n",
      "    mean_inference_ms: 1.5865051503913625\n",
      "    mean_processing_ms: 0.9804056878320833\n",
      "  time_since_restore: 990.6754648685455\n",
      "  time_this_iter_s: 2.4625563621520996\n",
      "  time_total_s: 990.6754648685455\n",
      "  timestamp: 1595949628\n",
      "  timesteps_since_restore: 526000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 263\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 990 s, 263 iter, 526000 ts, 9.65 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.673105616031208\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 265\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.681\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.364965796470642\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0790567961958004e-06\n",
      "        policy_loss: -8.14895611256361e-05\n",
      "        total_loss: 0.061055198311805725\n",
      "        vf_explained_var: 0.31666332483291626\n",
      "        vf_loss: 0.061136651784181595\n",
      "    load_time_ms: 1.115\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    sample_time_ms: 2783.471\n",
      "    update_time_ms: 4.661\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01666666666666\n",
      "    ram_util_percent: 68.68333333333332\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.047101399189296\n",
      "    mean_inference_ms: 1.5750955336540557\n",
      "    mean_processing_ms: 0.9770678090590937\n",
      "  time_since_restore: 996.8263659477234\n",
      "  time_this_iter_s: 3.658918857574463\n",
      "  time_total_s: 996.8263659477234\n",
      "  timestamp: 1595949635\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 265\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 996 s, 265 iter, 530000 ts, 9.67 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.673105616031208\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 265\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.625\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3180967569351196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.767515282670502e-06\n",
      "        policy_loss: -4.844188879360445e-06\n",
      "        total_loss: 0.0065479036420583725\n",
      "        vf_explained_var: 0.8196409940719604\n",
      "        vf_loss: 0.006552750710397959\n",
      "    load_time_ms: 1.135\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    sample_time_ms: 2670.826\n",
      "    update_time_ms: 6.258\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.03333333333333\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.047101399189296\n",
      "    mean_inference_ms: 1.5750955336540553\n",
      "    mean_processing_ms: 0.9770678090590937\n",
      "  time_since_restore: 1004.2719507217407\n",
      "  time_this_iter_s: 2.5874600410461426\n",
      "  time_total_s: 1004.2719507217407\n",
      "  timestamp: 1595949642\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 268\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1004 s, 268 iter, 536000 ts, 9.67 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.78167776086593\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 270\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.371\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.357487678527832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.404553000218584e-07\n",
      "        policy_loss: 4.302978595660534e-06\n",
      "        total_loss: 0.06885334849357605\n",
      "        vf_explained_var: 0.1710377335548401\n",
      "        vf_loss: 0.06884905695915222\n",
      "    load_time_ms: 1.139\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    sample_time_ms: 2646.188\n",
      "    update_time_ms: 6.184\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.99999999999999\n",
      "    ram_util_percent: 68.66666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.992518858518942\n",
      "    mean_inference_ms: 1.5643342113600838\n",
      "    mean_processing_ms: 0.9738985754352754\n",
      "  time_since_restore: 1010.1161494255066\n",
      "  time_this_iter_s: 3.6269586086273193\n",
      "  time_total_s: 1010.1161494255066\n",
      "  timestamp: 1595949648\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 270\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1010 s, 270 iter, 540000 ts, 9.78 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.78167776086593\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 270\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.481\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3202159404754639\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.5248738135560416e-06\n",
      "        policy_loss: -7.618570089107379e-05\n",
      "        total_loss: 0.005684191361069679\n",
      "        vf_explained_var: 0.8932231664657593\n",
      "        vf_loss: 0.005760380532592535\n",
      "    load_time_ms: 1.181\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "    sample_time_ms: 2634.972\n",
      "    update_time_ms: 6.225\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.55\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.992518858518942\n",
      "    mean_inference_ms: 1.5643342113600847\n",
      "    mean_processing_ms: 0.9738985754352756\n",
      "  time_since_restore: 1017.5458099842072\n",
      "  time_this_iter_s: 2.678483724594116\n",
      "  time_total_s: 1017.5458099842072\n",
      "  timestamp: 1595949655\n",
      "  timesteps_since_restore: 546000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 273\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1017 s, 273 iter, 546000 ts, 9.78 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.760502290057257\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 275\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.274\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3671767711639404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1980404198984616e-06\n",
      "        policy_loss: -8.240700117312372e-05\n",
      "        total_loss: 0.06253942847251892\n",
      "        vf_explained_var: 0.2551739811897278\n",
      "        vf_loss: 0.0626218393445015\n",
      "    load_time_ms: 1.151\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    sample_time_ms: 2595.082\n",
      "    update_time_ms: 6.109\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.0\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.941005937822336\n",
      "    mean_inference_ms: 1.5542306023955645\n",
      "    mean_processing_ms: 0.9709196419197407\n",
      "  time_since_restore: 1023.2469098567963\n",
      "  time_this_iter_s: 3.396775245666504\n",
      "  time_total_s: 1023.2469098567963\n",
      "  timestamp: 1595949661\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 275\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1023 s, 275 iter, 550000 ts, 9.76 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.760502290057255\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 275\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.854\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3114309310913086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9602597603807226e-05\n",
      "        policy_loss: -0.00023417663760483265\n",
      "        total_loss: 0.005704447627067566\n",
      "        vf_explained_var: 0.8392568826675415\n",
      "        vf_loss: 0.005938625428825617\n",
      "    load_time_ms: 1.108\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    sample_time_ms: 2564.069\n",
      "    update_time_ms: 4.406\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.05\n",
      "    ram_util_percent: 68.75\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.941005937822337\n",
      "    mean_inference_ms: 1.5542306023955643\n",
      "    mean_processing_ms: 0.9709196419197406\n",
      "  time_since_restore: 1030.3016426563263\n",
      "  time_this_iter_s: 2.3874049186706543\n",
      "  time_total_s: 1030.3016426563263\n",
      "  timestamp: 1595949668\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 278\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1030 s, 278 iter, 556000 ts, 9.76 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.826975240074885\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 280\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.556\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3501726388931274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.268688942625886e-06\n",
      "        policy_loss: -3.725290298461914e-05\n",
      "        total_loss: 0.07633975893259048\n",
      "        vf_explained_var: 0.14392709732055664\n",
      "        vf_loss: 0.0763770118355751\n",
      "    load_time_ms: 1.232\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    sample_time_ms: 2605.439\n",
      "    update_time_ms: 5.069\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.41666666666668\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.8906456034272825\n",
      "    mean_inference_ms: 1.5443192787941529\n",
      "    mean_processing_ms: 0.9680436627805429\n",
      "  time_since_restore: 1036.6361057758331\n",
      "  time_this_iter_s: 3.6174001693725586\n",
      "  time_total_s: 1036.6361057758331\n",
      "  timestamp: 1595949675\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 280\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1036 s, 280 iter, 560000 ts, 9.83 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.826975240074884\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 280\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.451\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2859441041946411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.804423148627393e-06\n",
      "        policy_loss: -6.193447188707069e-05\n",
      "        total_loss: 0.013156216591596603\n",
      "        vf_explained_var: 0.7609609365463257\n",
      "        vf_loss: 0.013218153268098831\n",
      "    load_time_ms: 1.428\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    sample_time_ms: 2647.651\n",
      "    update_time_ms: 5.293\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.275\n",
      "    ram_util_percent: 68.69999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.890645603427281\n",
      "    mean_inference_ms: 1.5443192787941533\n",
      "    mean_processing_ms: 0.9680436627805428\n",
      "  time_since_restore: 1041.8227708339691\n",
      "  time_this_iter_s: 2.7408030033111572\n",
      "  time_total_s: 1041.8227708339691\n",
      "  timestamp: 1595949680\n",
      "  timesteps_since_restore: 564000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 282\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1041 s, 282 iter, 564000 ts, 9.83 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.73245530941248\n",
      "  episode_reward_mean: 9.826975240074884\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 280\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.786\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3296318054199219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.356520432542311e-06\n",
      "        policy_loss: -0.00017090798064600676\n",
      "        total_loss: 0.003786699380725622\n",
      "        vf_explained_var: 0.934402585029602\n",
      "        vf_loss: 0.003957600332796574\n",
      "    load_time_ms: 1.499\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    sample_time_ms: 2703.864\n",
      "    update_time_ms: 5.94\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.95\n",
      "    ram_util_percent: 68.275\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.890645603427281\n",
      "    mean_inference_ms: 1.5443192787941533\n",
      "    mean_processing_ms: 0.9680436627805428\n",
      "  time_since_restore: 1047.3771405220032\n",
      "  time_this_iter_s: 2.6256461143493652\n",
      "  time_total_s: 1047.3771405220032\n",
      "  timestamp: 1595949685\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 284\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1047 s, 284 iter, 568000 ts, 9.83 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.828639323254789\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 285\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2538894414901733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1098354661953636e-05\n",
      "        policy_loss: -0.0003240623336751014\n",
      "        total_loss: 0.03165053576231003\n",
      "        vf_explained_var: 0.32708537578582764\n",
      "        vf_loss: 0.031974609941244125\n",
      "    load_time_ms: 1.492\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    sample_time_ms: 2799.182\n",
      "    update_time_ms: 6.034\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.125\n",
      "    ram_util_percent: 68.27499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.842772259834663\n",
      "    mean_inference_ms: 1.5349321145369583\n",
      "    mean_processing_ms: 0.9653113215181791\n",
      "  time_since_restore: 1054.0506796836853\n",
      "  time_this_iter_s: 2.57692289352417\n",
      "  time_total_s: 1054.0506796836853\n",
      "  timestamp: 1595949692\n",
      "  timesteps_since_restore: 572000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 286\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1054 s, 286 iter, 572000 ts, 9.83 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.828639323254789\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 285\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.703\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3136897087097168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.641302883916069e-06\n",
      "        policy_loss: 9.656906513555441e-06\n",
      "        total_loss: 0.005540479440242052\n",
      "        vf_explained_var: 0.8644092679023743\n",
      "        vf_loss: 0.005530829541385174\n",
      "    load_time_ms: 1.477\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    sample_time_ms: 2860.036\n",
      "    update_time_ms: 6.703\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.56666666666666\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.842772259834663\n",
      "    mean_inference_ms: 1.5349321145369583\n",
      "    mean_processing_ms: 0.9653113215181791\n",
      "  time_since_restore: 1059.4109013080597\n",
      "  time_this_iter_s: 2.6004927158355713\n",
      "  time_total_s: 1059.4109013080597\n",
      "  timestamp: 1595949697\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 288\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1059 s, 288 iter, 576000 ts, 9.83 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.886980737243146\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 290\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.359\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3470121622085571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.873458460177062e-06\n",
      "        policy_loss: -7.536220800830051e-05\n",
      "        total_loss: 0.08383682370185852\n",
      "        vf_explained_var: 0.17201834917068481\n",
      "        vf_loss: 0.0839121863245964\n",
      "    load_time_ms: 1.322\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    sample_time_ms: 2918.685\n",
      "    update_time_ms: 6.201\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.71666666666667\n",
      "    ram_util_percent: 68.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.797579406142528\n",
      "    mean_inference_ms: 1.52609799645372\n",
      "    mean_processing_ms: 0.9627303447282581\n",
      "  time_since_restore: 1066.2606763839722\n",
      "  time_this_iter_s: 4.050703525543213\n",
      "  time_total_s: 1066.2606763839722\n",
      "  timestamp: 1595949704\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 290\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1066 s, 290 iter, 580000 ts, 9.89 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.886980737243148\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 290\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.607\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2964531183242798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0644316716934554e-05\n",
      "        policy_loss: -0.00019334029639139771\n",
      "        total_loss: 0.00963356252759695\n",
      "        vf_explained_var: 0.7049050331115723\n",
      "        vf_loss: 0.00982692837715149\n",
      "    load_time_ms: 1.138\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    sample_time_ms: 2921.105\n",
      "    update_time_ms: 6.092\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.79757940614253\n",
      "    mean_inference_ms: 1.52609799645372\n",
      "    mean_processing_ms: 0.9627303447282582\n",
      "  time_since_restore: 1071.436974287033\n",
      "  time_this_iter_s: 2.5415263175964355\n",
      "  time_total_s: 1071.436974287033\n",
      "  timestamp: 1595949710\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 292\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1071 s, 292 iter, 584000 ts, 9.89 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-21-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.822235777503078\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 295\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.885\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3721565008163452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.887508450541645e-06\n",
      "        policy_loss: -6.764316640328616e-05\n",
      "        total_loss: 0.07719004899263382\n",
      "        vf_explained_var: 0.1147724986076355\n",
      "        vf_loss: 0.0772576853632927\n",
      "    load_time_ms: 1.479\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    sample_time_ms: 2849.551\n",
      "    update_time_ms: 5.047\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.26666666666667\n",
      "    ram_util_percent: 68.28333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.7547835317577425\n",
      "    mean_inference_ms: 1.5177844683326307\n",
      "    mean_processing_ms: 0.9603103920387163\n",
      "  time_since_restore: 1080.3682351112366\n",
      "  time_this_iter_s: 4.17421293258667\n",
      "  time_total_s: 1080.3682351112366\n",
      "  timestamp: 1595949718\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 295\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1080 s, 295 iter, 590000 ts, 9.82 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.82223577750308\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 295\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.455\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3014402389526367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.2931943577714264e-06\n",
      "        policy_loss: -0.0001557421637699008\n",
      "        total_loss: 0.010173271410167217\n",
      "        vf_explained_var: 0.7765362858772278\n",
      "        vf_loss: 0.010329012759029865\n",
      "    load_time_ms: 1.498\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "    sample_time_ms: 2840.029\n",
      "    update_time_ms: 4.412\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.5\n",
      "    ram_util_percent: 68.325\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.754783531757741\n",
      "    mean_inference_ms: 1.5177844683326311\n",
      "    mean_processing_ms: 0.9603103920387166\n",
      "  time_since_restore: 1085.6071555614471\n",
      "  time_this_iter_s: 2.5969884395599365\n",
      "  time_total_s: 1085.6071555614471\n",
      "  timestamp: 1595949724\n",
      "  timesteps_since_restore: 594000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 297\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1085 s, 297 iter, 594000 ts, 9.82 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.82223577750308\n",
      "  episode_reward_min: 6.286269036276553\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 295\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.904\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3450193405151367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0739237040979788e-05\n",
      "        policy_loss: -0.00024871062487363815\n",
      "        total_loss: 0.002674924908205867\n",
      "        vf_explained_var: 0.9139657616615295\n",
      "        vf_loss: 0.002923634136095643\n",
      "    load_time_ms: 1.543\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "    sample_time_ms: 2835.16\n",
      "    update_time_ms: 4.332\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.23333333333333\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.754783531757741\n",
      "    mean_inference_ms: 1.5177844683326311\n",
      "    mean_processing_ms: 0.9603103920387166\n",
      "  time_since_restore: 1090.9499161243439\n",
      "  time_this_iter_s: 2.638092279434204\n",
      "  time_total_s: 1090.9499161243439\n",
      "  timestamp: 1595949729\n",
      "  timesteps_since_restore: 598000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 299\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1090 s, 299 iter, 598000 ts, 9.82 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.881458604034798\n",
      "  episode_reward_min: 7.473642658353353\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 300\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.09\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2686609029769897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.5668780330743175e-06\n",
      "        policy_loss: -9.286308340961114e-05\n",
      "        total_loss: 0.03283005952835083\n",
      "        vf_explained_var: 0.4121782183647156\n",
      "        vf_loss: 0.03292291983962059\n",
      "    load_time_ms: 1.621\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "    sample_time_ms: 2825.077\n",
      "    update_time_ms: 4.245\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.375\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.713595584570356\n",
      "    mean_inference_ms: 1.5097974015751976\n",
      "    mean_processing_ms: 0.9579788039738056\n",
      "  time_since_restore: 1097.5355279445648\n",
      "  time_this_iter_s: 2.6147375106811523\n",
      "  time_total_s: 1097.5355279445648\n",
      "  timestamp: 1595949736\n",
      "  timesteps_since_restore: 602000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 301\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1097 s, 301 iter, 602000 ts, 9.88 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.881458604034798\n",
      "  episode_reward_min: 7.473642658353353\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 300\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.741\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3312913179397583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5995968826464377e-06\n",
      "        policy_loss: -7.59902031859383e-05\n",
      "        total_loss: 0.003908936865627766\n",
      "        vf_explained_var: 0.849859356880188\n",
      "        vf_loss: 0.003984922077506781\n",
      "    load_time_ms: 1.268\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "    sample_time_ms: 2870.455\n",
      "    update_time_ms: 4.491\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.86666666666666\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.713595584570356\n",
      "    mean_inference_ms: 1.5097974015751976\n",
      "    mean_processing_ms: 0.9579788039738056\n",
      "  time_since_restore: 1102.9324011802673\n",
      "  time_this_iter_s: 2.6751081943511963\n",
      "  time_total_s: 1102.9324011802673\n",
      "  timestamp: 1595949741\n",
      "  timesteps_since_restore: 606000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 303\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1102 s, 303 iter, 606000 ts, 9.88 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.934819200838199\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 305\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.99\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.351172924041748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0872057575616054e-06\n",
      "        policy_loss: -8.717537275515497e-05\n",
      "        total_loss: 0.1004565954208374\n",
      "        vf_explained_var: 0.05129450559616089\n",
      "        vf_loss: 0.10054374486207962\n",
      "    load_time_ms: 1.267\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    sample_time_ms: 2842.134\n",
      "    update_time_ms: 4.531\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.06\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.674412364874154\n",
      "    mean_inference_ms: 1.5022395392443855\n",
      "    mean_processing_ms: 0.9557663601726847\n",
      "  time_since_restore: 1109.234439611435\n",
      "  time_this_iter_s: 3.834653615951538\n",
      "  time_total_s: 1109.234439611435\n",
      "  timestamp: 1595949747\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 305\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1109 s, 305 iter, 610000 ts, 9.93 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.934819200838199\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 305\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.605\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.330923318862915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3125538771419087e-06\n",
      "        policy_loss: -1.8505095795262605e-05\n",
      "        total_loss: 0.004897106904536486\n",
      "        vf_explained_var: 0.9303744435310364\n",
      "        vf_loss: 0.004915615078061819\n",
      "    load_time_ms: 1.238\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    sample_time_ms: 2782.596\n",
      "    update_time_ms: 4.367\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.325\n",
      "    ram_util_percent: 68.45\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.674412364874154\n",
      "    mean_inference_ms: 1.5022395392443855\n",
      "    mean_processing_ms: 0.9557663601726847\n",
      "  time_since_restore: 1116.5639832019806\n",
      "  time_this_iter_s: 2.545215368270874\n",
      "  time_total_s: 1116.5639832019806\n",
      "  timestamp: 1595949755\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 308\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1116 s, 308 iter, 616000 ts, 9.93 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.980011877410266\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 310\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.439\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.358681082725525\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9810199773928616e-06\n",
      "        policy_loss: -6.485819903900847e-05\n",
      "        total_loss: 0.09424730390310287\n",
      "        vf_explained_var: 0.22511738538742065\n",
      "        vf_loss: 0.0943121463060379\n",
      "    load_time_ms: 1.21\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    sample_time_ms: 2735.051\n",
      "    update_time_ms: 4.537\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.24\n",
      "    ram_util_percent: 68.42\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.636592610786812\n",
      "    mean_inference_ms: 1.4949402411992616\n",
      "    mean_processing_ms: 0.953613334573034\n",
      "  time_since_restore: 1122.7064394950867\n",
      "  time_this_iter_s: 3.6279237270355225\n",
      "  time_total_s: 1122.7064394950867\n",
      "  timestamp: 1595949761\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 310\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1122 s, 310 iter, 620000 ts, 9.98 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.980011877410266\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 310\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.686\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.332316517829895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.304417065119196e-07\n",
      "        policy_loss: -9.456396219320595e-05\n",
      "        total_loss: 0.004346306435763836\n",
      "        vf_explained_var: 0.9357898831367493\n",
      "        vf_loss: 0.004440865479409695\n",
      "    load_time_ms: 1.059\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "    sample_time_ms: 2685.137\n",
      "    update_time_ms: 4.405\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.10000000000001\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.636592610786813\n",
      "    mean_inference_ms: 1.494940241199262\n",
      "    mean_processing_ms: 0.953613334573034\n",
      "  time_since_restore: 1130.1565053462982\n",
      "  time_this_iter_s: 2.4971890449523926\n",
      "  time_total_s: 1130.1565053462982\n",
      "  timestamp: 1595949768\n",
      "  timesteps_since_restore: 626000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 313\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1130 s, 313 iter, 626000 ts, 9.98 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.971166013431013\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 315\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.782\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3495995998382568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.619845647837792e-06\n",
      "        policy_loss: 7.790041127009317e-05\n",
      "        total_loss: 0.11142698675394058\n",
      "        vf_explained_var: 0.008395075798034668\n",
      "        vf_loss: 0.11134910583496094\n",
      "    load_time_ms: 1.084\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    sample_time_ms: 2703.885\n",
      "    update_time_ms: 4.453\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48333333333334\n",
      "    ram_util_percent: 68.39999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.600596104355884\n",
      "    mean_inference_ms: 1.4880220786679368\n",
      "    mean_processing_ms: 0.9515887306067046\n",
      "  time_since_restore: 1136.6495130062103\n",
      "  time_this_iter_s: 3.8063266277313232\n",
      "  time_total_s: 1136.6495130062103\n",
      "  timestamp: 1595949775\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 315\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1136 s, 315 iter, 630000 ts, 9.97 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.971166013431013\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 315\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.973\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2981655597686768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.471536031924188e-06\n",
      "        policy_loss: -0.00021675610332749784\n",
      "        total_loss: 0.01179012656211853\n",
      "        vf_explained_var: 0.7059823870658875\n",
      "        vf_loss: 0.01200688537210226\n",
      "    load_time_ms: 1.106\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "    sample_time_ms: 2764.867\n",
      "    update_time_ms: 5.469\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.69999999999999\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.600596104355883\n",
      "    mean_inference_ms: 1.4880220786679368\n",
      "    mean_processing_ms: 0.9515887306067047\n",
      "  time_since_restore: 1142.0552017688751\n",
      "  time_this_iter_s: 2.643165349960327\n",
      "  time_total_s: 1142.0552017688751\n",
      "  timestamp: 1595949780\n",
      "  timesteps_since_restore: 634000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 317\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1142 s, 317 iter, 634000 ts, 9.97 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.971166013431013\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 315\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.756\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3438562154769897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.733272807148751e-07\n",
      "        policy_loss: -0.00016093540762085468\n",
      "        total_loss: 0.002432124223560095\n",
      "        vf_explained_var: 0.9224412441253662\n",
      "        vf_loss: 0.002593075856566429\n",
      "    load_time_ms: 1.136\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "    sample_time_ms: 2810.971\n",
      "    update_time_ms: 5.494\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.94999999999999\n",
      "    ram_util_percent: 68.55\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.600596104355883\n",
      "    mean_inference_ms: 1.4880220786679368\n",
      "    mean_processing_ms: 0.9515887306067047\n",
      "  time_since_restore: 1147.5734512805939\n",
      "  time_this_iter_s: 2.7729744911193848\n",
      "  time_total_s: 1147.5734512805939\n",
      "  timestamp: 1595949786\n",
      "  timesteps_since_restore: 638000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 319\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1147 s, 319 iter, 638000 ts, 9.97 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.996166323888177\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 320\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.027\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2613261938095093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4513245080015622e-05\n",
      "        policy_loss: -0.00023775720910634845\n",
      "        total_loss: 0.0302402563393116\n",
      "        vf_explained_var: 0.4114474058151245\n",
      "        vf_loss: 0.03047802299261093\n",
      "    load_time_ms: 1.155\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "    sample_time_ms: 2888.589\n",
      "    update_time_ms: 5.271\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.825\n",
      "    ram_util_percent: 68.44999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.566613926719396\n",
      "    mean_inference_ms: 1.4815530160165487\n",
      "    mean_processing_ms: 0.9496998108130221\n",
      "  time_since_restore: 1154.3687999248505\n",
      "  time_this_iter_s: 2.9210596084594727\n",
      "  time_total_s: 1154.3687999248505\n",
      "  timestamp: 1595949793\n",
      "  timesteps_since_restore: 642000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 321\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1154 s, 321 iter, 642000 ts, 10 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.996166323888177\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 320\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.686\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3259600400924683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.401726416996098e-06\n",
      "        policy_loss: -7.2801711212378e-05\n",
      "        total_loss: 0.004016408231109381\n",
      "        vf_explained_var: 0.8873124122619629\n",
      "        vf_loss: 0.00408921018242836\n",
      "    load_time_ms: 1.523\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "    sample_time_ms: 2914.193\n",
      "    update_time_ms: 5.357\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.175\n",
      "    ram_util_percent: 68.42500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.566613926719396\n",
      "    mean_inference_ms: 1.4815530160165487\n",
      "    mean_processing_ms: 0.9496998108130221\n",
      "  time_since_restore: 1159.7214868068695\n",
      "  time_this_iter_s: 2.7232415676116943\n",
      "  time_total_s: 1159.7214868068695\n",
      "  timestamp: 1595949798\n",
      "  timesteps_since_restore: 646000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 323\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1159 s, 323 iter, 646000 ts, 10 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.991515501458348\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 325\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.48\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.355895757675171\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.5544802560471e-06\n",
      "        policy_loss: -5.3349016525316983e-05\n",
      "        total_loss: 0.10717605799436569\n",
      "        vf_explained_var: 0.13241726160049438\n",
      "        vf_loss: 0.10722938179969788\n",
      "    load_time_ms: 1.503\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    sample_time_ms: 2929.736\n",
      "    update_time_ms: 5.476\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.66666666666667\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.533535047392762\n",
      "    mean_inference_ms: 1.4752850807267253\n",
      "    mean_processing_ms: 0.9478905854205236\n",
      "  time_since_restore: 1166.3607513904572\n",
      "  time_this_iter_s: 3.994389533996582\n",
      "  time_total_s: 1166.3607513904572\n",
      "  timestamp: 1595949805\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 325\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1166 s, 325 iter, 650000 ts, 9.99 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.99151550145835\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 325\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2886476516723633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.529092646203935e-05\n",
      "        policy_loss: -0.00041831086855381727\n",
      "        total_loss: 0.010620202869176865\n",
      "        vf_explained_var: 0.6533377170562744\n",
      "        vf_loss: 0.011038525961339474\n",
      "    load_time_ms: 1.52\n",
      "    num_steps_sampled: 654000\n",
      "    num_steps_trained: 654000\n",
      "    sample_time_ms: 2889.476\n",
      "    update_time_ms: 4.5\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.8\n",
      "    ram_util_percent: 68.66666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.533535047392762\n",
      "    mean_inference_ms: 1.4752850807267257\n",
      "    mean_processing_ms: 0.9478905854205237\n",
      "  time_since_restore: 1171.3575387001038\n",
      "  time_this_iter_s: 2.3683691024780273\n",
      "  time_total_s: 1171.3575387001038\n",
      "  timestamp: 1595949810\n",
      "  timesteps_since_restore: 654000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 654000\n",
      "  training_iteration: 327\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1171 s, 327 iter, 654000 ts, 9.99 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.942752698236877\n",
      "  episode_reward_mean: 9.99151550145835\n",
      "  episode_reward_min: 7.881919441098772\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 325\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.291\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3195173740386963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.590847915622362e-07\n",
      "        policy_loss: -3.986167939729057e-05\n",
      "        total_loss: 0.004135718569159508\n",
      "        vf_explained_var: 0.8492944836616516\n",
      "        vf_loss: 0.004175571259111166\n",
      "    load_time_ms: 1.533\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "    sample_time_ms: 2868.131\n",
      "    update_time_ms: 4.54\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.05\n",
      "    ram_util_percent: 68.775\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.533535047392762\n",
      "    mean_inference_ms: 1.4752850807267257\n",
      "    mean_processing_ms: 0.9478905854205237\n",
      "  time_since_restore: 1176.67884516716\n",
      "  time_this_iter_s: 2.8284080028533936\n",
      "  time_total_s: 1176.67884516716\n",
      "  timestamp: 1595949815\n",
      "  timesteps_since_restore: 658000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 329\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1176 s, 329 iter, 658000 ts, 9.99 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.058399832397354\n",
      "  episode_reward_min: 7.957175163625709\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 330\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.419\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2549890279769897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0955512152577285e-05\n",
      "        policy_loss: -0.00014377593470271677\n",
      "        total_loss: 0.029496928676962852\n",
      "        vf_explained_var: 0.4100516438484192\n",
      "        vf_loss: 0.029640693217515945\n",
      "    load_time_ms: 1.844\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "    sample_time_ms: 2854.29\n",
      "    update_time_ms: 4.462\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.575\n",
      "    ram_util_percent: 68.64999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.501811301846645\n",
      "    mean_inference_ms: 1.4693033505003599\n",
      "    mean_processing_ms: 0.9462149072871892\n",
      "  time_since_restore: 1183.3399622440338\n",
      "  time_this_iter_s: 2.8745884895324707\n",
      "  time_total_s: 1183.3399622440338\n",
      "  timestamp: 1595949822\n",
      "  timesteps_since_restore: 662000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 331\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1183 s, 331 iter, 662000 ts, 10.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.058399832397354\n",
      "  episode_reward_min: 7.957175163625709\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 330\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.249\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3130929470062256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8772096154862083e-05\n",
      "        policy_loss: -0.0002592391974758357\n",
      "        total_loss: 0.004590677097439766\n",
      "        vf_explained_var: 0.8063079118728638\n",
      "        vf_loss: 0.004849917255342007\n",
      "    load_time_ms: 1.454\n",
      "    num_steps_sampled: 666000\n",
      "    num_steps_trained: 666000\n",
      "    sample_time_ms: 2821.305\n",
      "    update_time_ms: 4.353\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.60000000000001\n",
      "    ram_util_percent: 68.525\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.501811301846645\n",
      "    mean_inference_ms: 1.4693033505003599\n",
      "    mean_processing_ms: 0.9462149072871892\n",
      "  time_since_restore: 1188.3502876758575\n",
      "  time_this_iter_s: 2.5146679878234863\n",
      "  time_total_s: 1188.3502876758575\n",
      "  timestamp: 1595949827\n",
      "  timesteps_since_restore: 666000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 666000\n",
      "  training_iteration: 333\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1188 s, 333 iter, 666000 ts, 10.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.091950527941169\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 335\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.286\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3453052043914795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9526182768458966e-06\n",
      "        policy_loss: -6.0850143199786544e-05\n",
      "        total_loss: 0.11720717698335648\n",
      "        vf_explained_var: 0.1930924654006958\n",
      "        vf_loss: 0.11726801842451096\n",
      "    load_time_ms: 1.441\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    sample_time_ms: 2793.181\n",
      "    update_time_ms: 4.437\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.06666666666667\n",
      "    ram_util_percent: 68.38333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4713922643931765\n",
      "    mean_inference_ms: 1.4636107125205737\n",
      "    mean_processing_ms: 0.9445565401358915\n",
      "  time_since_restore: 1194.7103102207184\n",
      "  time_this_iter_s: 3.7574501037597656\n",
      "  time_total_s: 1194.7103102207184\n",
      "  timestamp: 1595949833\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 335\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1194 s, 335 iter, 670000 ts, 10.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.091950527941167\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 335\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.737\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.284977674484253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.015733662119601e-05\n",
      "        policy_loss: -0.0001041240684571676\n",
      "        total_loss: 0.01006393227726221\n",
      "        vf_explained_var: 0.7301177978515625\n",
      "        vf_loss: 0.010168056935071945\n",
      "    load_time_ms: 1.412\n",
      "    num_steps_sampled: 674000\n",
      "    num_steps_trained: 674000\n",
      "    sample_time_ms: 2824.428\n",
      "    update_time_ms: 4.512\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.175\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.471392264393177\n",
      "    mean_inference_ms: 1.463610712520573\n",
      "    mean_processing_ms: 0.9445565401358911\n",
      "  time_since_restore: 1200.0139570236206\n",
      "  time_this_iter_s: 2.707475185394287\n",
      "  time_total_s: 1200.0139570236206\n",
      "  timestamp: 1595949838\n",
      "  timesteps_since_restore: 674000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 674000\n",
      "  training_iteration: 337\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1200 s, 337 iter, 674000 ts, 10.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.091950527941167\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 335\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.623\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3295817375183105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.899893378431443e-06\n",
      "        policy_loss: -0.0005579695571213961\n",
      "        total_loss: 0.0018169613322243094\n",
      "        vf_explained_var: 0.9119032621383667\n",
      "        vf_loss: 0.002374940551817417\n",
      "    load_time_ms: 1.542\n",
      "    num_steps_sampled: 678000\n",
      "    num_steps_trained: 678000\n",
      "    sample_time_ms: 2820.704\n",
      "    update_time_ms: 5.739\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.19999999999999\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.471392264393177\n",
      "    mean_inference_ms: 1.463610712520573\n",
      "    mean_processing_ms: 0.9445565401358911\n",
      "  time_since_restore: 1205.364224910736\n",
      "  time_this_iter_s: 2.7245473861694336\n",
      "  time_total_s: 1205.364224910736\n",
      "  timestamp: 1595949844\n",
      "  timesteps_since_restore: 678000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 678000\n",
      "  training_iteration: 339\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1205 s, 339 iter, 678000 ts, 10.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.127844915107044\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 340\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.993\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.235113501548767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.35656828712672e-05\n",
      "        policy_loss: -0.0010157489450648427\n",
      "        total_loss: 0.02786935679614544\n",
      "        vf_explained_var: 0.42666465044021606\n",
      "        vf_loss: 0.02888510189950466\n",
      "    load_time_ms: 1.238\n",
      "    num_steps_sampled: 682000\n",
      "    num_steps_trained: 682000\n",
      "    sample_time_ms: 2784.573\n",
      "    update_time_ms: 7.802\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.325\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.442483454249791\n",
      "    mean_inference_ms: 1.4582286336245514\n",
      "    mean_processing_ms: 0.9429977855753116\n",
      "  time_since_restore: 1211.6753203868866\n",
      "  time_this_iter_s: 2.57108736038208\n",
      "  time_total_s: 1211.6753203868866\n",
      "  timestamp: 1595949850\n",
      "  timesteps_since_restore: 682000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 682000\n",
      "  training_iteration: 341\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1211 s, 341 iter, 682000 ts, 10.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.127844915107044\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 340\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.286\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3065917491912842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.283117388898972e-06\n",
      "        policy_loss: -4.059743878315203e-05\n",
      "        total_loss: 0.004229820799082518\n",
      "        vf_explained_var: 0.8321422934532166\n",
      "        vf_loss: 0.004270417150110006\n",
      "    load_time_ms: 1.37\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "    sample_time_ms: 2860.178\n",
      "    update_time_ms: 7.853\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.775\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.442483454249791\n",
      "    mean_inference_ms: 1.4582286336245514\n",
      "    mean_processing_ms: 0.9429977855753116\n",
      "  time_since_restore: 1217.4432609081268\n",
      "  time_this_iter_s: 2.764566659927368\n",
      "  time_total_s: 1217.4432609081268\n",
      "  timestamp: 1595949856\n",
      "  timesteps_since_restore: 686000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 343\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1217 s, 343 iter, 686000 ts, 10.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.156296465034993\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 345\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.254\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3500187397003174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4586746601708e-06\n",
      "        policy_loss: 4.466342943487689e-05\n",
      "        total_loss: 0.1192971020936966\n",
      "        vf_explained_var: 0.1429133415222168\n",
      "        vf_loss: 0.11925244331359863\n",
      "    load_time_ms: 1.454\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    sample_time_ms: 2886.489\n",
      "    update_time_ms: 7.967\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.449999999999996\n",
      "    ram_util_percent: 68.46666666666665\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.414992602787188\n",
      "    mean_inference_ms: 1.4531350793714588\n",
      "    mean_processing_ms: 0.9415440891574568\n",
      "  time_since_restore: 1224.0784842967987\n",
      "  time_this_iter_s: 3.921304702758789\n",
      "  time_total_s: 1224.0784842967987\n",
      "  timestamp: 1595949863\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 345\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1224 s, 345 iter, 690000 ts, 10.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.156296465034995\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 345\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2760460376739502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.229957868228666e-06\n",
      "        policy_loss: -0.00010706615285016596\n",
      "        total_loss: 0.01032605953514576\n",
      "        vf_explained_var: 0.6838303804397583\n",
      "        vf_loss: 0.010433118790388107\n",
      "    load_time_ms: 1.471\n",
      "    num_steps_sampled: 694000\n",
      "    num_steps_trained: 694000\n",
      "    sample_time_ms: 2899.196\n",
      "    update_time_ms: 7.893\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.414992602787187\n",
      "    mean_inference_ms: 1.4531350793714588\n",
      "    mean_processing_ms: 0.9415440891574569\n",
      "  time_since_restore: 1229.5242578983307\n",
      "  time_this_iter_s: 2.91221284866333\n",
      "  time_total_s: 1229.5242578983307\n",
      "  timestamp: 1595949868\n",
      "  timesteps_since_restore: 694000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 694000\n",
      "  training_iteration: 347\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1229 s, 347 iter, 694000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.159992910142128\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 350\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.948\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3630852699279785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.300739197380608e-06\n",
      "        policy_loss: -3.432798257563263e-05\n",
      "        total_loss: 0.12086785584688187\n",
      "        vf_explained_var: 0.007353663444519043\n",
      "        vf_loss: 0.12090219557285309\n",
      "    load_time_ms: 1.26\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    sample_time_ms: 2864.983\n",
      "    update_time_ms: 5.669\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.52\n",
      "    ram_util_percent: 68.55999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.388466263147169\n",
      "    mean_inference_ms: 1.4482447084436239\n",
      "    mean_processing_ms: 0.9401479863706718\n",
      "  time_since_restore: 1238.1901097297668\n",
      "  time_this_iter_s: 3.7021217346191406\n",
      "  time_total_s: 1238.1901097297668\n",
      "  timestamp: 1595949877\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 350\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1238 s, 350 iter, 700000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.15999291014213\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 350\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.402\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.280683159828186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010394993296358734\n",
      "        policy_loss: -0.0009492359240539372\n",
      "        total_loss: 0.007934068329632282\n",
      "        vf_explained_var: 0.743375301361084\n",
      "        vf_loss: 0.00888330303132534\n",
      "    load_time_ms: 1.358\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    sample_time_ms: 2866.35\n",
      "    update_time_ms: 7.013\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5\n",
      "    ram_util_percent: 68.57499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.38846626314717\n",
      "    mean_inference_ms: 1.4482447084436245\n",
      "    mean_processing_ms: 0.9401479863706718\n",
      "  time_since_restore: 1243.8117635250092\n",
      "  time_this_iter_s: 2.787729501724243\n",
      "  time_total_s: 1243.8117635250092\n",
      "  timestamp: 1595949882\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 352\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1243 s, 352 iter, 704000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.141309899712613\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 355\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.057\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3484338521957397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.980615009728353e-06\n",
      "        policy_loss: -0.0001348547957604751\n",
      "        total_loss: 0.13543258607387543\n",
      "        vf_explained_var: -0.13009333610534668\n",
      "        vf_loss: 0.13556742668151855\n",
      "    load_time_ms: 1.161\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    sample_time_ms: 2816.021\n",
      "    update_time_ms: 7.212\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.01666666666667\n",
      "    ram_util_percent: 68.63333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.363017683165133\n",
      "    mean_inference_ms: 1.4435451675489048\n",
      "    mean_processing_ms: 0.9388277691266828\n",
      "  time_since_restore: 1252.6861145496368\n",
      "  time_this_iter_s: 3.932067394256592\n",
      "  time_total_s: 1252.6861145496368\n",
      "  timestamp: 1595949891\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 355\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1252 s, 355 iter, 710000 ts, 10.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.141309899712615\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 355\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.548\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2755227088928223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.208433529129252e-05\n",
      "        policy_loss: -0.0002897338999900967\n",
      "        total_loss: 0.008003738708794117\n",
      "        vf_explained_var: 0.8432930111885071\n",
      "        vf_loss: 0.008293463848531246\n",
      "    load_time_ms: 1.141\n",
      "    num_steps_sampled: 714000\n",
      "    num_steps_trained: 714000\n",
      "    sample_time_ms: 2802.682\n",
      "    update_time_ms: 8.571\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.6\n",
      "    ram_util_percent: 68.675\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.363017683165131\n",
      "    mean_inference_ms: 1.4435451675489046\n",
      "    mean_processing_ms: 0.9388277691266829\n",
      "  time_since_restore: 1258.0549354553223\n",
      "  time_this_iter_s: 2.7676315307617188\n",
      "  time_total_s: 1258.0549354553223\n",
      "  timestamp: 1595949897\n",
      "  timesteps_since_restore: 714000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 714000\n",
      "  training_iteration: 357\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1258 s, 357 iter, 714000 ts, 10.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.141309899712615\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 355\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.329\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3296767473220825\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.928691057808464e-06\n",
      "        policy_loss: -0.00034218834480270743\n",
      "        total_loss: 0.0018471183720976114\n",
      "        vf_explained_var: 0.9477704763412476\n",
      "        vf_loss: 0.0021893135271966457\n",
      "    load_time_ms: 1.167\n",
      "    num_steps_sampled: 718000\n",
      "    num_steps_trained: 718000\n",
      "    sample_time_ms: 2823.226\n",
      "    update_time_ms: 7.512\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.2\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.363017683165131\n",
      "    mean_inference_ms: 1.4435451675489046\n",
      "    mean_processing_ms: 0.9388277691266829\n",
      "  time_since_restore: 1263.2106819152832\n",
      "  time_this_iter_s: 2.6186068058013916\n",
      "  time_total_s: 1263.2106819152832\n",
      "  timestamp: 1595949902\n",
      "  timesteps_since_restore: 718000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 718000\n",
      "  training_iteration: 359\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1263 s, 359 iter, 718000 ts, 10.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.179347825515856\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 360\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.14\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2152628898620605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.106802751062787e-06\n",
      "        policy_loss: -9.024381870403886e-05\n",
      "        total_loss: 0.027893301099538803\n",
      "        vf_explained_var: 0.3897538185119629\n",
      "        vf_loss: 0.027983540669083595\n",
      "    load_time_ms: 1.207\n",
      "    num_steps_sampled: 722000\n",
      "    num_steps_trained: 722000\n",
      "    sample_time_ms: 2818.254\n",
      "    update_time_ms: 6.38\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.975\n",
      "    ram_util_percent: 68.675\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.338247101776802\n",
      "    mean_inference_ms: 1.4389788223319573\n",
      "    mean_processing_ms: 0.9375781637403655\n",
      "  time_since_restore: 1269.6840753555298\n",
      "  time_this_iter_s: 2.5744423866271973\n",
      "  time_total_s: 1269.6840753555298\n",
      "  timestamp: 1595949908\n",
      "  timesteps_since_restore: 722000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 722000\n",
      "  training_iteration: 361\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1269 s, 361 iter, 722000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.179347825515856\n",
      "  episode_reward_min: 8.113644238183573\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 360\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.977\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2849973440170288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6524403690709732e-05\n",
      "        policy_loss: -0.0002788553247228265\n",
      "        total_loss: 0.005113185383379459\n",
      "        vf_explained_var: 0.84226393699646\n",
      "        vf_loss: 0.005392040126025677\n",
      "    load_time_ms: 1.149\n",
      "    num_steps_sampled: 726000\n",
      "    num_steps_trained: 726000\n",
      "    sample_time_ms: 2819.201\n",
      "    update_time_ms: 6.473\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.35\n",
      "    ram_util_percent: 68.65\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.338247101776802\n",
      "    mean_inference_ms: 1.4389788223319573\n",
      "    mean_processing_ms: 0.9375781637403655\n",
      "  time_since_restore: 1274.9905638694763\n",
      "  time_this_iter_s: 2.6417810916900635\n",
      "  time_total_s: 1274.9905638694763\n",
      "  timestamp: 1595949914\n",
      "  timesteps_since_restore: 726000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 726000\n",
      "  training_iteration: 363\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1274 s, 363 iter, 726000 ts, 10.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.224589362561424\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 365\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.435\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3307908773422241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1944642867310904e-06\n",
      "        policy_loss: -3.137016392429359e-05\n",
      "        total_loss: 0.13779689371585846\n",
      "        vf_explained_var: 0.06725680828094482\n",
      "        vf_loss: 0.1378282904624939\n",
      "    load_time_ms: 1.15\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    sample_time_ms: 2807.92\n",
      "    update_time_ms: 6.38\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.79999999999999\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.314428421731264\n",
      "    mean_inference_ms: 1.4346157520640097\n",
      "    mean_processing_ms: 0.936382611190994\n",
      "  time_since_restore: 1281.2453093528748\n",
      "  time_this_iter_s: 3.8212950229644775\n",
      "  time_total_s: 1281.2453093528748\n",
      "  timestamp: 1595949920\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 365\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1281 s, 365 iter, 730000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.224589362561424\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 365\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.254052758216858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.067837264505215e-05\n",
      "        policy_loss: -0.00013236665108706802\n",
      "        total_loss: 0.009780339896678925\n",
      "        vf_explained_var: 0.7325751781463623\n",
      "        vf_loss: 0.009912705048918724\n",
      "    load_time_ms: 1.2\n",
      "    num_steps_sampled: 734000\n",
      "    num_steps_trained: 734000\n",
      "    sample_time_ms: 2815.72\n",
      "    update_time_ms: 5.879\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.525\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3144284217312645\n",
      "    mean_inference_ms: 1.4346157520640093\n",
      "    mean_processing_ms: 0.936382611190994\n",
      "  time_since_restore: 1286.6463327407837\n",
      "  time_this_iter_s: 2.7785658836364746\n",
      "  time_total_s: 1286.6463327407837\n",
      "  timestamp: 1595949925\n",
      "  timesteps_since_restore: 734000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 734000\n",
      "  training_iteration: 367\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1286 s, 367 iter, 734000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.224589362561424\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 365\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.946\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3233875036239624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.32503810164053e-05\n",
      "        policy_loss: -0.0006463718600571156\n",
      "        total_loss: 0.0013033056166023016\n",
      "        vf_explained_var: 0.9336264729499817\n",
      "        vf_loss: 0.0019496777094900608\n",
      "    load_time_ms: 1.42\n",
      "    num_steps_sampled: 738000\n",
      "    num_steps_trained: 738000\n",
      "    sample_time_ms: 2837.276\n",
      "    update_time_ms: 6.11\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.725\n",
      "    ram_util_percent: 68.675\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3144284217312645\n",
      "    mean_inference_ms: 1.4346157520640093\n",
      "    mean_processing_ms: 0.936382611190994\n",
      "  time_since_restore: 1292.043131828308\n",
      "  time_this_iter_s: 2.6556694507598877\n",
      "  time_total_s: 1292.043131828308\n",
      "  timestamp: 1595949931\n",
      "  timesteps_since_restore: 738000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 738000\n",
      "  training_iteration: 369\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1292 s, 369 iter, 738000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.215500755829797\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 370\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.807\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2108606100082397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6913413674046751e-06\n",
      "        policy_loss: -7.859039033064619e-05\n",
      "        total_loss: 0.026608377695083618\n",
      "        vf_explained_var: 0.48150140047073364\n",
      "        vf_loss: 0.02668696455657482\n",
      "    load_time_ms: 1.343\n",
      "    num_steps_sampled: 742000\n",
      "    num_steps_trained: 742000\n",
      "    sample_time_ms: 2798.365\n",
      "    update_time_ms: 5.957\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.525\n",
      "    ram_util_percent: 68.75\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.291738186266042\n",
      "    mean_inference_ms: 1.4304893584065308\n",
      "    mean_processing_ms: 0.9352289717905756\n",
      "  time_since_restore: 1298.1054556369781\n",
      "  time_this_iter_s: 2.5105788707733154\n",
      "  time_total_s: 1298.1054556369781\n",
      "  timestamp: 1595949937\n",
      "  timesteps_since_restore: 742000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 742000\n",
      "  training_iteration: 371\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1298 s, 371 iter, 742000 ts, 10.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.215500755829797\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 370\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2985124588012695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.736883400051738e-06\n",
      "        policy_loss: -0.00011372566223144531\n",
      "        total_loss: 0.003012657631188631\n",
      "        vf_explained_var: 0.8800949454307556\n",
      "        vf_loss: 0.0031263851560652256\n",
      "    load_time_ms: 1.328\n",
      "    num_steps_sampled: 746000\n",
      "    num_steps_trained: 746000\n",
      "    sample_time_ms: 2832.003\n",
      "    update_time_ms: 5.665\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.32\n",
      "    ram_util_percent: 68.68\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.291738186266042\n",
      "    mean_inference_ms: 1.4304893584065308\n",
      "    mean_processing_ms: 0.9352289717905756\n",
      "  time_since_restore: 1303.734492778778\n",
      "  time_this_iter_s: 3.0559024810791016\n",
      "  time_total_s: 1303.734492778778\n",
      "  timestamp: 1595949943\n",
      "  timesteps_since_restore: 746000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 746000\n",
      "  training_iteration: 373\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1303 s, 373 iter, 746000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.20933123948838\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 375\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.462\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3393150568008423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7466119263408473e-06\n",
      "        policy_loss: -3.9278744225157425e-05\n",
      "        total_loss: 0.13861806690692902\n",
      "        vf_explained_var: 0.103798508644104\n",
      "        vf_loss: 0.1386573165655136\n",
      "    load_time_ms: 1.329\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    sample_time_ms: 2868.657\n",
      "    update_time_ms: 5.109\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.68\n",
      "    ram_util_percent: 68.58\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.270305181255161\n",
      "    mean_inference_ms: 1.426633304420383\n",
      "    mean_processing_ms: 0.9341415139324585\n",
      "  time_since_restore: 1310.342368364334\n",
      "  time_this_iter_s: 3.9600822925567627\n",
      "  time_total_s: 1310.342368364334\n",
      "  timestamp: 1595949949\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 375\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1310 s, 375 iter, 750000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-25-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.20933123948838\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 375\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.926\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2532283067703247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.81389667786425e-06\n",
      "        policy_loss: 1.6495227100676857e-05\n",
      "        total_loss: 0.009710296988487244\n",
      "        vf_explained_var: 0.7874321937561035\n",
      "        vf_loss: 0.009693792089819908\n",
      "    load_time_ms: 1.356\n",
      "    num_steps_sampled: 754000\n",
      "    num_steps_trained: 754000\n",
      "    sample_time_ms: 2853.045\n",
      "    update_time_ms: 5.483\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.25\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.270305181255161\n",
      "    mean_inference_ms: 1.426633304420383\n",
      "    mean_processing_ms: 0.9341415139324585\n",
      "  time_since_restore: 1315.6077647209167\n",
      "  time_this_iter_s: 2.6343533992767334\n",
      "  time_total_s: 1315.6077647209167\n",
      "  timestamp: 1595949954\n",
      "  timesteps_since_restore: 754000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 754000\n",
      "  training_iteration: 377\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1315 s, 377 iter, 754000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.20933123948838\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 375\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.073\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3141980171203613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.5606622077466454e-06\n",
      "        policy_loss: -0.00026196622638963163\n",
      "        total_loss: 0.0018830470507964492\n",
      "        vf_explained_var: 0.9395613074302673\n",
      "        vf_loss: 0.0021450098138302565\n",
      "    load_time_ms: 1.138\n",
      "    num_steps_sampled: 758000\n",
      "    num_steps_trained: 758000\n",
      "    sample_time_ms: 2859.906\n",
      "    update_time_ms: 6.249\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.125\n",
      "    ram_util_percent: 68.625\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.270305181255161\n",
      "    mean_inference_ms: 1.426633304420383\n",
      "    mean_processing_ms: 0.9341415139324585\n",
      "  time_since_restore: 1321.1169245243073\n",
      "  time_this_iter_s: 2.6705780029296875\n",
      "  time_total_s: 1321.1169245243073\n",
      "  timestamp: 1595949960\n",
      "  timesteps_since_restore: 758000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 758000\n",
      "  training_iteration: 379\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1321 s, 379 iter, 758000 ts, 10.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.205213868700254\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 380\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.721\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1947788000106812\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.7106972235487774e-05\n",
      "        policy_loss: -0.0007107558194547892\n",
      "        total_loss: 0.02875342033803463\n",
      "        vf_explained_var: 0.4063456654548645\n",
      "        vf_loss: 0.029464172199368477\n",
      "    load_time_ms: 1.172\n",
      "    num_steps_sampled: 762000\n",
      "    num_steps_trained: 762000\n",
      "    sample_time_ms: 2892.361\n",
      "    update_time_ms: 6.45\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.775\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.249922281239735\n",
      "    mean_inference_ms: 1.4230114999658092\n",
      "    mean_processing_ms: 0.9331041957591885\n",
      "  time_since_restore: 1327.500156402588\n",
      "  time_this_iter_s: 2.545703887939453\n",
      "  time_total_s: 1327.500156402588\n",
      "  timestamp: 1595949966\n",
      "  timesteps_since_restore: 762000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 762000\n",
      "  training_iteration: 381\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1327 s, 381 iter, 762000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.205213868700254\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 380\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2725014686584473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.5595327062765136e-05\n",
      "        policy_loss: -0.0003758926468435675\n",
      "        total_loss: 0.004111787769943476\n",
      "        vf_explained_var: 0.8388441801071167\n",
      "        vf_loss: 0.0044876812025904655\n",
      "    load_time_ms: 1.167\n",
      "    num_steps_sampled: 766000\n",
      "    num_steps_trained: 766000\n",
      "    sample_time_ms: 2836.108\n",
      "    update_time_ms: 6.387\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.60000000000001\n",
      "    ram_util_percent: 68.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.249922281239735\n",
      "    mean_inference_ms: 1.4230114999658092\n",
      "    mean_processing_ms: 0.9331041957591885\n",
      "  time_since_restore: 1332.5588855743408\n",
      "  time_this_iter_s: 2.475977659225464\n",
      "  time_total_s: 1332.5588855743408\n",
      "  timestamp: 1595949972\n",
      "  timesteps_since_restore: 766000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 766000\n",
      "  training_iteration: 383\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1332 s, 383 iter, 766000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.211799078653188\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 385\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.458\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3272902965545654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.125085327657871e-05\n",
      "        policy_loss: -8.188247738871723e-05\n",
      "        total_loss: 0.15287721157073975\n",
      "        vf_explained_var: 0.0596352219581604\n",
      "        vf_loss: 0.15295907855033875\n",
      "    load_time_ms: 1.248\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    sample_time_ms: 2753.566\n",
      "    update_time_ms: 6.476\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.82000000000001\n",
      "    ram_util_percent: 68.67999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.229803957418583\n",
      "    mean_inference_ms: 1.4194060723566162\n",
      "    mean_processing_ms: 0.932071537887323\n",
      "  time_since_restore: 1338.3826053142548\n",
      "  time_this_iter_s: 3.551502227783203\n",
      "  time_total_s: 1338.3826053142548\n",
      "  timestamp: 1595949977\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 385\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1338 s, 385 iter, 770000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.211799078653188\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 385\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.28410005569458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.731267836177722e-06\n",
      "        policy_loss: -0.00011584853928070515\n",
      "        total_loss: 0.0029168264009058475\n",
      "        vf_explained_var: 0.8724875450134277\n",
      "        vf_loss: 0.0030326596461236477\n",
      "    load_time_ms: 1.238\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    sample_time_ms: 2630.178\n",
      "    update_time_ms: 5.829\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.0\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.229803957418581\n",
      "    mean_inference_ms: 1.4194060723566162\n",
      "    mean_processing_ms: 0.932071537887323\n",
      "  time_since_restore: 1345.158494234085\n",
      "  time_this_iter_s: 2.2575323581695557\n",
      "  time_total_s: 1345.158494234085\n",
      "  timestamp: 1595949984\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 388\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1345 s, 388 iter, 776000 ts, 10.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.202065604929379\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 390\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.215\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3167999982833862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.466001195803983e-06\n",
      "        policy_loss: -8.192539098672569e-05\n",
      "        total_loss: 0.15726357698440552\n",
      "        vf_explained_var: 0.05304998159408569\n",
      "        vf_loss: 0.15734554827213287\n",
      "    load_time_ms: 1.224\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    sample_time_ms: 2594.257\n",
      "    update_time_ms: 4.493\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.9\n",
      "    ram_util_percent: 68.66\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.209818460634325\n",
      "    mean_inference_ms: 1.4157816881383218\n",
      "    mean_processing_ms: 0.9310261565864937\n",
      "  time_since_restore: 1351.289353609085\n",
      "  time_this_iter_s: 3.554706335067749\n",
      "  time_total_s: 1351.289353609085\n",
      "  timestamp: 1595949990\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 390\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1351 s, 390 iter, 780000 ts, 10.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.202065604929379\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 390\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.773\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.245477318763733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.196000989191816e-06\n",
      "        policy_loss: -5.3331375966081396e-05\n",
      "        total_loss: 0.007931417785584927\n",
      "        vf_explained_var: 0.7926468849182129\n",
      "        vf_loss: 0.007984746247529984\n",
      "    load_time_ms: 1.354\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    sample_time_ms: 2612.928\n",
      "    update_time_ms: 4.495\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.46666666666665\n",
      "    ram_util_percent: 68.93333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.209818460634324\n",
      "    mean_inference_ms: 1.4157816881383214\n",
      "    mean_processing_ms: 0.9310261565864936\n",
      "  time_since_restore: 1356.6383287906647\n",
      "  time_this_iter_s: 2.5195233821868896\n",
      "  time_total_s: 1356.6383287906647\n",
      "  timestamp: 1595949996\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 392\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1356 s, 392 iter, 784000 ts, 10.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.260653985312743\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 395\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.944\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.313793659210205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.317870646744268e-05\n",
      "        policy_loss: -0.00014020371600054204\n",
      "        total_loss: 0.16468621790409088\n",
      "        vf_explained_var: 0.03967571258544922\n",
      "        vf_loss: 0.1648264229297638\n",
      "    load_time_ms: 1.304\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    sample_time_ms: 2616.797\n",
      "    update_time_ms: 5.134\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.42\n",
      "    ram_util_percent: 68.78\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.190305999659022\n",
      "    mean_inference_ms: 1.412240515352629\n",
      "    mean_processing_ms: 0.929986537510857\n",
      "  time_since_restore: 1364.9572308063507\n",
      "  time_this_iter_s: 3.4288222789764404\n",
      "  time_total_s: 1364.9572308063507\n",
      "  timestamp: 1595950004\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 395\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1364 s, 395 iter, 790000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.260653985312743\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 395\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.838\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2479557991027832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.9823773780372e-06\n",
      "        policy_loss: -0.0001441311906091869\n",
      "        total_loss: 0.007597309071570635\n",
      "        vf_explained_var: 0.6583671569824219\n",
      "        vf_loss: 0.007741433102637529\n",
      "    load_time_ms: 1.672\n",
      "    num_steps_sampled: 794000\n",
      "    num_steps_trained: 794000\n",
      "    sample_time_ms: 2735.242\n",
      "    update_time_ms: 5.366\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.275\n",
      "    ram_util_percent: 68.92500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.190305999659023\n",
      "    mean_inference_ms: 1.412240515352629\n",
      "    mean_processing_ms: 0.929986537510857\n",
      "  time_since_restore: 1370.7695965766907\n",
      "  time_this_iter_s: 2.64365816116333\n",
      "  time_total_s: 1370.7695965766907\n",
      "  timestamp: 1595950010\n",
      "  timesteps_since_restore: 794000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 794000\n",
      "  training_iteration: 397\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1370 s, 397 iter, 794000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.260653985312743\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 395\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.183\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2953087091445923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1198858323477907e-06\n",
      "        policy_loss: -0.00028148459387011826\n",
      "        total_loss: 0.0023430194705724716\n",
      "        vf_explained_var: 0.8902881741523743\n",
      "        vf_loss: 0.002624501008540392\n",
      "    load_time_ms: 1.713\n",
      "    num_steps_sampled: 798000\n",
      "    num_steps_trained: 798000\n",
      "    sample_time_ms: 2813.696\n",
      "    update_time_ms: 5.447\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10000000000001\n",
      "    ram_util_percent: 68.85000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.190305999659023\n",
      "    mean_inference_ms: 1.412240515352629\n",
      "    mean_processing_ms: 0.929986537510857\n",
      "  time_since_restore: 1376.4288334846497\n",
      "  time_this_iter_s: 2.942056655883789\n",
      "  time_total_s: 1376.4288334846497\n",
      "  timestamp: 1595950016\n",
      "  timesteps_since_restore: 798000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 798000\n",
      "  training_iteration: 399\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1376 s, 399 iter, 798000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.289953028325066\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 400\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.795\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.183196783065796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.260011387988925e-05\n",
      "        policy_loss: -0.0005416602944023907\n",
      "        total_loss: 0.027967147529125214\n",
      "        vf_explained_var: 0.41414159536361694\n",
      "        vf_loss: 0.02850881591439247\n",
      "    load_time_ms: 1.771\n",
      "    num_steps_sampled: 802000\n",
      "    num_steps_trained: 802000\n",
      "    sample_time_ms: 2820.864\n",
      "    update_time_ms: 5.604\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.1\n",
      "    ram_util_percent: 68.875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.171404618482581\n",
      "    mean_inference_ms: 1.408819309452726\n",
      "    mean_processing_ms: 0.9289889734378565\n",
      "  time_since_restore: 1382.911801815033\n",
      "  time_this_iter_s: 2.6332952976226807\n",
      "  time_total_s: 1382.911801815033\n",
      "  timestamp: 1595950022\n",
      "  timesteps_since_restore: 802000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 802000\n",
      "  training_iteration: 401\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1382 s, 401 iter, 802000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.289953028325066\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 400\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.954\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.284505844116211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.109902081632754e-06\n",
      "        policy_loss: -6.748580926796421e-05\n",
      "        total_loss: 0.0032757283188402653\n",
      "        vf_explained_var: 0.8793292045593262\n",
      "        vf_loss: 0.003343213116750121\n",
      "    load_time_ms: 1.609\n",
      "    num_steps_sampled: 806000\n",
      "    num_steps_trained: 806000\n",
      "    sample_time_ms: 2850.389\n",
      "    update_time_ms: 6.087\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.725\n",
      "    ram_util_percent: 68.85\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.171404618482581\n",
      "    mean_inference_ms: 1.408819309452726\n",
      "    mean_processing_ms: 0.9289889734378565\n",
      "  time_since_restore: 1388.1820497512817\n",
      "  time_this_iter_s: 2.5781774520874023\n",
      "  time_total_s: 1388.1820497512817\n",
      "  timestamp: 1595950027\n",
      "  timesteps_since_restore: 806000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 806000\n",
      "  training_iteration: 403\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1388 s, 403 iter, 806000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.313135728971984\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 405\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.273\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.314989447593689\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.152787712679128e-06\n",
      "        policy_loss: -2.116298674081918e-05\n",
      "        total_loss: 0.1742154210805893\n",
      "        vf_explained_var: 0.00399017333984375\n",
      "        vf_loss: 0.17423656582832336\n",
      "    load_time_ms: 1.579\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    sample_time_ms: 2910.8\n",
      "    update_time_ms: 5.9\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.60000000000001\n",
      "    ram_util_percent: 68.78333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1530620658550115\n",
      "    mean_inference_ms: 1.4054909261444772\n",
      "    mean_processing_ms: 0.928035258731507\n",
      "  time_since_restore: 1394.6328983306885\n",
      "  time_this_iter_s: 3.935832977294922\n",
      "  time_total_s: 1394.6328983306885\n",
      "  timestamp: 1595950034\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 405\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1394 s, 405 iter, 810000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.313135728971986\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 405\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.336\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2298613786697388\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.807741738157347e-05\n",
      "        policy_loss: -0.0005650825332850218\n",
      "        total_loss: 0.009365366771817207\n",
      "        vf_explained_var: 0.7334765791893005\n",
      "        vf_loss: 0.00993044301867485\n",
      "    load_time_ms: 1.154\n",
      "    num_steps_sampled: 814000\n",
      "    num_steps_trained: 814000\n",
      "    sample_time_ms: 2851.119\n",
      "    update_time_ms: 5.025\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.3\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1530620658550115\n",
      "    mean_inference_ms: 1.4054909261444768\n",
      "    mean_processing_ms: 0.928035258731507\n",
      "  time_since_restore: 1399.7309725284576\n",
      "  time_this_iter_s: 2.564699649810791\n",
      "  time_total_s: 1399.7309725284576\n",
      "  timestamp: 1595950039\n",
      "  timesteps_since_restore: 814000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 814000\n",
      "  training_iteration: 407\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1399 s, 407 iter, 814000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.313135728971986\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 405\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.406\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3122820854187012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8960705549252452e-06\n",
      "        policy_loss: -0.00026611328939907253\n",
      "        total_loss: 0.0017407397972419858\n",
      "        vf_explained_var: 0.9496525526046753\n",
      "        vf_loss: 0.0020068499725311995\n",
      "    load_time_ms: 1.119\n",
      "    num_steps_sampled: 818000\n",
      "    num_steps_trained: 818000\n",
      "    sample_time_ms: 2790.14\n",
      "    update_time_ms: 5.029\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.63333333333334\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1530620658550115\n",
      "    mean_inference_ms: 1.4054909261444768\n",
      "    mean_processing_ms: 0.928035258731507\n",
      "  time_since_restore: 1404.7451844215393\n",
      "  time_this_iter_s: 2.503492832183838\n",
      "  time_total_s: 1404.7451844215393\n",
      "  timestamp: 1595950044\n",
      "  timesteps_since_restore: 818000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 818000\n",
      "  training_iteration: 409\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1404 s, 409 iter, 818000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.310708708394936\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 410\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.785\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1653237342834473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.611079905065708e-05\n",
      "        policy_loss: -0.0007702233851887286\n",
      "        total_loss: 0.026894139125943184\n",
      "        vf_explained_var: 0.3678411841392517\n",
      "        vf_loss: 0.02766435220837593\n",
      "    load_time_ms: 2.136\n",
      "    num_steps_sampled: 822000\n",
      "    num_steps_trained: 822000\n",
      "    sample_time_ms: 2814.898\n",
      "    update_time_ms: 5.797\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.07499999999999\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.135449326321743\n",
      "    mean_inference_ms: 1.4023077609180117\n",
      "    mean_processing_ms: 0.9271243488489163\n",
      "  time_since_restore: 1411.5432603359222\n",
      "  time_this_iter_s: 2.6247518062591553\n",
      "  time_total_s: 1411.5432603359222\n",
      "  timestamp: 1595950051\n",
      "  timesteps_since_restore: 822000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 822000\n",
      "  training_iteration: 411\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1411 s, 411 iter, 822000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.007756407862903\n",
      "  episode_reward_mean: 10.310708708394936\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 410\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.256\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2632843255996704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.57417156919837e-05\n",
      "        policy_loss: -0.00034190271981060505\n",
      "        total_loss: 0.004038074519485235\n",
      "        vf_explained_var: 0.9093987345695496\n",
      "        vf_loss: 0.004379970487207174\n",
      "    load_time_ms: 2.128\n",
      "    num_steps_sampled: 826000\n",
      "    num_steps_trained: 826000\n",
      "    sample_time_ms: 2818.252\n",
      "    update_time_ms: 6.27\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.32499999999999\n",
      "    ram_util_percent: 68.875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.135449326321743\n",
      "    mean_inference_ms: 1.4023077609180117\n",
      "    mean_processing_ms: 0.9271243488489163\n",
      "  time_since_restore: 1416.8665435314178\n",
      "  time_this_iter_s: 2.708409070968628\n",
      "  time_total_s: 1416.8665435314178\n",
      "  timestamp: 1595950056\n",
      "  timesteps_since_restore: 826000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 826000\n",
      "  training_iteration: 413\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1416 s, 413 iter, 826000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.33552600402138\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 415\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.692\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3038758039474487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.9841064588254085e-06\n",
      "        policy_loss: -5.895709909964353e-05\n",
      "        total_loss: 0.17066346108913422\n",
      "        vf_explained_var: 0.1079217791557312\n",
      "        vf_loss: 0.17072241008281708\n",
      "    load_time_ms: 2.199\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    sample_time_ms: 2822.896\n",
      "    update_time_ms: 6.194\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.3\n",
      "    ram_util_percent: 69.08\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1184369261781795\n",
      "    mean_inference_ms: 1.3992434172659507\n",
      "    mean_processing_ms: 0.9262386221255398\n",
      "  time_since_restore: 1423.3689301013947\n",
      "  time_this_iter_s: 3.680885076522827\n",
      "  time_total_s: 1423.3689301013947\n",
      "  timestamp: 1595950063\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 415\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1423 s, 415 iter, 830000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.33552600402138\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 415\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.517\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2560642957687378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.9546837544767186e-05\n",
      "        policy_loss: -0.00023206710466183722\n",
      "        total_loss: 0.0029117201920598745\n",
      "        vf_explained_var: 0.8943003416061401\n",
      "        vf_loss: 0.0031437715515494347\n",
      "    load_time_ms: 2.234\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    sample_time_ms: 2811.353\n",
      "    update_time_ms: 6.222\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.125\n",
      "    ram_util_percent: 68.95\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1184369261781795\n",
      "    mean_inference_ms: 1.3992434172659503\n",
      "    mean_processing_ms: 0.9262386221255396\n",
      "  time_since_restore: 1430.8842754364014\n",
      "  time_this_iter_s: 2.536449670791626\n",
      "  time_total_s: 1430.8842754364014\n",
      "  timestamp: 1595950070\n",
      "  timesteps_since_restore: 836000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 418\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1430 s, 418 iter, 836000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.332306618867172\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 420\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.474\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3111211061477661\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.545790832504281e-06\n",
      "        policy_loss: -4.573535989038646e-05\n",
      "        total_loss: 0.17432942986488342\n",
      "        vf_explained_var: 0.019352614879608154\n",
      "        vf_loss: 0.17437517642974854\n",
      "    load_time_ms: 1.176\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    sample_time_ms: 2798.142\n",
      "    update_time_ms: 6.428\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7\n",
      "    ram_util_percent: 68.86666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.101688445439221\n",
      "    mean_inference_ms: 1.3962097720839222\n",
      "    mean_processing_ms: 0.9253530101590682\n",
      "  time_since_restore: 1437.3469953536987\n",
      "  time_this_iter_s: 3.8919100761413574\n",
      "  time_total_s: 1437.3469953536987\n",
      "  timestamp: 1595950077\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 420\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1437 s, 420 iter, 840000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.332306618867172\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 420\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.495\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.202193021774292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0435205189860426e-05\n",
      "        policy_loss: -0.0003315124486107379\n",
      "        total_loss: 0.008271572180092335\n",
      "        vf_explained_var: 0.8006287217140198\n",
      "        vf_loss: 0.00860309973359108\n",
      "    load_time_ms: 1.216\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    sample_time_ms: 2801.291\n",
      "    update_time_ms: 5.54\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.0\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.101688445439221\n",
      "    mean_inference_ms: 1.3962097720839222\n",
      "    mean_processing_ms: 0.9253530101590683\n",
      "  time_since_restore: 1442.6115908622742\n",
      "  time_this_iter_s: 2.699801445007324\n",
      "  time_total_s: 1442.6115908622742\n",
      "  timestamp: 1595950082\n",
      "  timesteps_since_restore: 844000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 422\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1442 s, 422 iter, 844000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.332306618867172\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 420\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.983\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.266073226928711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3576239325630013e-05\n",
      "        policy_loss: -0.0008230981766246259\n",
      "        total_loss: 0.0012534226989373565\n",
      "        vf_explained_var: 0.8660944700241089\n",
      "        vf_loss: 0.002076522447168827\n",
      "    load_time_ms: 1.229\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    sample_time_ms: 2766.628\n",
      "    update_time_ms: 5.787\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.07499999999999\n",
      "    ram_util_percent: 68.85000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.101688445439221\n",
      "    mean_inference_ms: 1.3962097720839222\n",
      "    mean_processing_ms: 0.9253530101590683\n",
      "  time_since_restore: 1447.818508863449\n",
      "  time_this_iter_s: 2.643228530883789\n",
      "  time_total_s: 1447.818508863449\n",
      "  timestamp: 1595950087\n",
      "  timesteps_since_restore: 848000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 424\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1447 s, 424 iter, 848000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.357768683303766\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 425\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.568\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1286357641220093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.845103937142994e-06\n",
      "        policy_loss: -0.00018618821923155338\n",
      "        total_loss: 0.026881078258156776\n",
      "        vf_explained_var: 0.443342387676239\n",
      "        vf_loss: 0.027067257091403008\n",
      "    load_time_ms: 1.277\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    sample_time_ms: 2759.492\n",
      "    update_time_ms: 6.374\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.425\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.085177457583052\n",
      "    mean_inference_ms: 1.3931971810597623\n",
      "    mean_processing_ms: 0.9244775579581398\n",
      "  time_since_restore: 1453.9465157985687\n",
      "  time_this_iter_s: 2.3912768363952637\n",
      "  time_total_s: 1453.9465157985687\n",
      "  timestamp: 1595950093\n",
      "  timesteps_since_restore: 852000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 426\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1453 s, 426 iter, 852000 ts, 10.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.357768683303766\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 425\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2324626445770264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2652874374907697e-06\n",
      "        policy_loss: 3.765964356716722e-05\n",
      "        total_loss: 0.004149407614022493\n",
      "        vf_explained_var: 0.8730717897415161\n",
      "        vf_loss: 0.004111745394766331\n",
      "    load_time_ms: 1.277\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    sample_time_ms: 2777.562\n",
      "    update_time_ms: 6.73\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.125\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.085177457583052\n",
      "    mean_inference_ms: 1.3931971810597623\n",
      "    mean_processing_ms: 0.9244775579581398\n",
      "  time_since_restore: 1459.1375584602356\n",
      "  time_this_iter_s: 2.5943868160247803\n",
      "  time_total_s: 1459.1375584602356\n",
      "  timestamp: 1595950099\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 428\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1459 s, 428 iter, 856000 ts, 10.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.34719409298804\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 430\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.904\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2749834060668945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6044377844082192e-06\n",
      "        policy_loss: 2.556991603341885e-05\n",
      "        total_loss: 0.1930803656578064\n",
      "        vf_explained_var: -0.0027681589126586914\n",
      "        vf_loss: 0.19305478036403656\n",
      "    load_time_ms: 1.27\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    sample_time_ms: 2769.234\n",
      "    update_time_ms: 6.473\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.96000000000001\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.069116726675746\n",
      "    mean_inference_ms: 1.390259345565408\n",
      "    mean_processing_ms: 0.9236009353516533\n",
      "  time_since_restore: 1465.5133085250854\n",
      "  time_this_iter_s: 3.766087770462036\n",
      "  time_total_s: 1465.5133085250854\n",
      "  timestamp: 1595950105\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 430\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1465 s, 430 iter, 860000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.347194092988042\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 430\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.642\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1955031156539917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2128582713776268e-05\n",
      "        policy_loss: -4.335308040026575e-05\n",
      "        total_loss: 0.008188245818018913\n",
      "        vf_explained_var: 0.798556923866272\n",
      "        vf_loss: 0.008231599815189838\n",
      "    load_time_ms: 1.253\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    sample_time_ms: 2780.721\n",
      "    update_time_ms: 6.477\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.32499999999999\n",
      "    ram_util_percent: 69.225\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0691167266757455\n",
      "    mean_inference_ms: 1.3902593455654082\n",
      "    mean_processing_ms: 0.9236009353516532\n",
      "  time_since_restore: 1470.8728077411652\n",
      "  time_this_iter_s: 2.4360032081604004\n",
      "  time_total_s: 1470.8728077411652\n",
      "  timestamp: 1595950110\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 432\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1470 s, 432 iter, 864000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.347194092988042\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 430\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.609\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2817285060882568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.352186119329417e-06\n",
      "        policy_loss: -0.00047127914149314165\n",
      "        total_loss: 0.0011835842160508037\n",
      "        vf_explained_var: 0.9364888668060303\n",
      "        vf_loss: 0.0016548645216971636\n",
      "    load_time_ms: 1.166\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    sample_time_ms: 2780.907\n",
      "    update_time_ms: 4.992\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.43333333333334\n",
      "    ram_util_percent: 69.23333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0691167266757455\n",
      "    mean_inference_ms: 1.3902593455654082\n",
      "    mean_processing_ms: 0.9236009353516532\n",
      "  time_since_restore: 1476.026486158371\n",
      "  time_this_iter_s: 2.5171310901641846\n",
      "  time_total_s: 1476.026486158371\n",
      "  timestamp: 1595950115\n",
      "  timesteps_since_restore: 868000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 434\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1476 s, 434 iter, 868000 ts, 10.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.347579739700063\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 435\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.224\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1273826360702515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2863487427239306e-05\n",
      "        policy_loss: -2.6993275241693482e-05\n",
      "        total_loss: 0.026933476328849792\n",
      "        vf_explained_var: 0.5201042294502258\n",
      "        vf_loss: 0.0269604604691267\n",
      "    load_time_ms: 1.14\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    sample_time_ms: 2798.311\n",
      "    update_time_ms: 4.452\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.875\n",
      "    ram_util_percent: 69.17500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0534335994303285\n",
      "    mean_inference_ms: 1.3873746630432036\n",
      "    mean_processing_ms: 0.9227573014875387\n",
      "  time_since_restore: 1482.3077397346497\n",
      "  time_this_iter_s: 2.6993820667266846\n",
      "  time_total_s: 1482.3077397346497\n",
      "  timestamp: 1595950122\n",
      "  timesteps_since_restore: 872000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 436\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1482 s, 436 iter, 872000 ts, 10.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.347579739700063\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 435\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.046\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2674587965011597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.782172876119148e-05\n",
      "        policy_loss: -0.0004628157475963235\n",
      "        total_loss: 0.0017334165750071406\n",
      "        vf_explained_var: 0.9226669073104858\n",
      "        vf_loss: 0.0021962348837405443\n",
      "    load_time_ms: 1.132\n",
      "    num_steps_sampled: 878000\n",
      "    num_steps_trained: 878000\n",
      "    sample_time_ms: 2766.369\n",
      "    update_time_ms: 4.083\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.3\n",
      "    ram_util_percent: 69.025\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0534335994303285\n",
      "    mean_inference_ms: 1.3873746630432036\n",
      "    mean_processing_ms: 0.9227573014875387\n",
      "  time_since_restore: 1489.7787733078003\n",
      "  time_this_iter_s: 2.496439218521118\n",
      "  time_total_s: 1489.7787733078003\n",
      "  timestamp: 1595950129\n",
      "  timesteps_since_restore: 878000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 878000\n",
      "  training_iteration: 439\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1489 s, 439 iter, 878000 ts, 10.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.357921483253586\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 440\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.72\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1231720447540283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.719072497508023e-05\n",
      "        policy_loss: -0.0005007085856050253\n",
      "        total_loss: 0.026845593005418777\n",
      "        vf_explained_var: 0.39759546518325806\n",
      "        vf_loss: 0.027346312999725342\n",
      "    load_time_ms: 1.254\n",
      "    num_steps_sampled: 882000\n",
      "    num_steps_trained: 882000\n",
      "    sample_time_ms: 2756.207\n",
      "    update_time_ms: 4.551\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.45\n",
      "    ram_util_percent: 69.05\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.038055314026345\n",
      "    mean_inference_ms: 1.3845366192363269\n",
      "    mean_processing_ms: 0.9219547519185742\n",
      "  time_since_restore: 1496.4047136306763\n",
      "  time_this_iter_s: 2.7636401653289795\n",
      "  time_total_s: 1496.4047136306763\n",
      "  timestamp: 1595950136\n",
      "  timesteps_since_restore: 882000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 882000\n",
      "  training_iteration: 441\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1496 s, 441 iter, 882000 ts, 10.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.357921483253586\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 440\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2497509717941284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.077890015556477e-06\n",
      "        policy_loss: 9.2796326498501e-05\n",
      "        total_loss: 0.003273519454523921\n",
      "        vf_explained_var: 0.9041861295700073\n",
      "        vf_loss: 0.0031807245686650276\n",
      "    load_time_ms: 1.268\n",
      "    num_steps_sampled: 886000\n",
      "    num_steps_trained: 886000\n",
      "    sample_time_ms: 2773.991\n",
      "    update_time_ms: 6.044\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.625\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.038055314026345\n",
      "    mean_inference_ms: 1.3845366192363269\n",
      "    mean_processing_ms: 0.9219547519185742\n",
      "  time_since_restore: 1501.7186295986176\n",
      "  time_this_iter_s: 2.6410765647888184\n",
      "  time_total_s: 1501.7186295986176\n",
      "  timestamp: 1595950141\n",
      "  timesteps_since_restore: 886000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 886000\n",
      "  training_iteration: 443\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1501 s, 443 iter, 886000 ts, 10.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.3829252224365\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 445\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.033\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2800357341766357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2599646652233787e-05\n",
      "        policy_loss: -0.00012855243403464556\n",
      "        total_loss: 0.19920361042022705\n",
      "        vf_explained_var: 0.04829084873199463\n",
      "        vf_loss: 0.1993321180343628\n",
      "    load_time_ms: 1.294\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    sample_time_ms: 2828.106\n",
      "    update_time_ms: 5.969\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.800000000000004\n",
      "    ram_util_percent: 68.96666666666665\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.023007846454205\n",
      "    mean_inference_ms: 1.3817595380389838\n",
      "    mean_processing_ms: 0.9211375808640987\n",
      "  time_since_restore: 1508.3572149276733\n",
      "  time_this_iter_s: 3.9548962116241455\n",
      "  time_total_s: 1508.3572149276733\n",
      "  timestamp: 1595950148\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 445\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1508 s, 445 iter, 890000 ts, 10.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.3829252224365\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 445\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1893671751022339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.202841930440627e-05\n",
      "        policy_loss: -0.0001482114748796448\n",
      "        total_loss: 0.009002406150102615\n",
      "        vf_explained_var: 0.6766417026519775\n",
      "        vf_loss: 0.009150616824626923\n",
      "    load_time_ms: 1.251\n",
      "    num_steps_sampled: 894000\n",
      "    num_steps_trained: 894000\n",
      "    sample_time_ms: 2850.472\n",
      "    update_time_ms: 7.437\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.625\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.023007846454207\n",
      "    mean_inference_ms: 1.3817595380389838\n",
      "    mean_processing_ms: 0.9211375808640987\n",
      "  time_since_restore: 1513.8651683330536\n",
      "  time_this_iter_s: 2.9413840770721436\n",
      "  time_total_s: 1513.8651683330536\n",
      "  timestamp: 1595950153\n",
      "  timesteps_since_restore: 894000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 894000\n",
      "  training_iteration: 447\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1513 s, 447 iter, 894000 ts, 10.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.3829252224365\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 445\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.426\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2553986310958862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.908569513238035e-05\n",
      "        policy_loss: -0.000546055322047323\n",
      "        total_loss: 0.002014508470892906\n",
      "        vf_explained_var: 0.9253324270248413\n",
      "        vf_loss: 0.002560562454164028\n",
      "    load_time_ms: 1.287\n",
      "    num_steps_sampled: 898000\n",
      "    num_steps_trained: 898000\n",
      "    sample_time_ms: 2901.821\n",
      "    update_time_ms: 7.405\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.55\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.023007846454207\n",
      "    mean_inference_ms: 1.3817595380389838\n",
      "    mean_processing_ms: 0.9211375808640987\n",
      "  time_since_restore: 1519.2962296009064\n",
      "  time_this_iter_s: 2.672250270843506\n",
      "  time_total_s: 1519.2962296009064\n",
      "  timestamp: 1595950159\n",
      "  timesteps_since_restore: 898000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 898000\n",
      "  training_iteration: 449\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1519 s, 449 iter, 898000 ts, 10.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.430772119945704\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 450\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.767\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.116064429283142\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3154804946680088e-05\n",
      "        policy_loss: -0.0003460035368334502\n",
      "        total_loss: 0.02745729684829712\n",
      "        vf_explained_var: 0.4554377794265747\n",
      "        vf_loss: 0.027803296223282814\n",
      "    load_time_ms: 1.13\n",
      "    num_steps_sampled: 902000\n",
      "    num_steps_trained: 902000\n",
      "    sample_time_ms: 2886.354\n",
      "    update_time_ms: 7.037\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.83333333333333\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.00848802057882\n",
      "    mean_inference_ms: 1.3790767799807944\n",
      "    mean_processing_ms: 0.9203464372378789\n",
      "  time_since_restore: 1525.7236194610596\n",
      "  time_this_iter_s: 2.570394515991211\n",
      "  time_total_s: 1525.7236194610596\n",
      "  timestamp: 1595950165\n",
      "  timesteps_since_restore: 902000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 902000\n",
      "  training_iteration: 451\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1525 s, 451 iter, 902000 ts, 10.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.430772119945704\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 450\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.226688265800476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.915092399140121e-06\n",
      "        policy_loss: -0.00010524225217523053\n",
      "        total_loss: 0.00341065414249897\n",
      "        vf_explained_var: 0.8181344270706177\n",
      "        vf_loss: 0.003515896387398243\n",
      "    load_time_ms: 1.119\n",
      "    num_steps_sampled: 906000\n",
      "    num_steps_trained: 906000\n",
      "    sample_time_ms: 2946.644\n",
      "    update_time_ms: 5.572\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.25\n",
      "    ram_util_percent: 68.92500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.00848802057882\n",
      "    mean_inference_ms: 1.3790767799807944\n",
      "    mean_processing_ms: 0.9203464372378789\n",
      "  time_since_restore: 1531.6254923343658\n",
      "  time_this_iter_s: 2.824249744415283\n",
      "  time_total_s: 1531.6254923343658\n",
      "  timestamp: 1595950171\n",
      "  timesteps_since_restore: 906000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 906000\n",
      "  training_iteration: 453\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1531 s, 453 iter, 906000 ts, 10.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.494612668529564\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 455\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.355\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.261584997177124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.859387900069123e-06\n",
      "        policy_loss: -0.00012118911399738863\n",
      "        total_loss: 0.20943473279476166\n",
      "        vf_explained_var: 0.03468143939971924\n",
      "        vf_loss: 0.20955590903759003\n",
      "    load_time_ms: 1.16\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    sample_time_ms: 2873.754\n",
      "    update_time_ms: 6.565\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.999999999999986\n",
      "    ram_util_percent: 68.98\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.99428785496359\n",
      "    mean_inference_ms: 1.3764481190195825\n",
      "    mean_processing_ms: 0.9195657276823276\n",
      "  time_since_restore: 1537.5897152423859\n",
      "  time_this_iter_s: 3.534381628036499\n",
      "  time_total_s: 1537.5897152423859\n",
      "  timestamp: 1595950177\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 455\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1537 s, 455 iter, 910000 ts, 10.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.494612668529564\n",
      "  episode_reward_min: 8.191959177835225\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 455\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.57\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2107268571853638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.375898035708815e-05\n",
      "        policy_loss: -6.701373786199838e-05\n",
      "        total_loss: 0.003630230436101556\n",
      "        vf_explained_var: 0.755587100982666\n",
      "        vf_loss: 0.003697242122143507\n",
      "    load_time_ms: 1.162\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    sample_time_ms: 2778.051\n",
      "    update_time_ms: 5.241\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.64999999999999\n",
      "    ram_util_percent: 68.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.99428785496359\n",
      "    mean_inference_ms: 1.3764481190195825\n",
      "    mean_processing_ms: 0.9195657276823279\n",
      "  time_since_restore: 1544.8734741210938\n",
      "  time_this_iter_s: 2.6257596015930176\n",
      "  time_total_s: 1544.8734741210938\n",
      "  timestamp: 1595950185\n",
      "  timesteps_since_restore: 916000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 458\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1544 s, 458 iter, 916000 ts, 10.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.57396630586667\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 460\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.039\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2278934717178345\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2421310202626046e-06\n",
      "        policy_loss: -1.202130351884989e-05\n",
      "        total_loss: 0.21915718913078308\n",
      "        vf_explained_var: 0.039140164852142334\n",
      "        vf_loss: 0.21916918456554413\n",
      "    load_time_ms: 1.343\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    sample_time_ms: 2779.457\n",
      "    update_time_ms: 5.291\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.92\n",
      "    ram_util_percent: 68.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.980343645119158\n",
      "    mean_inference_ms: 1.3738517797553018\n",
      "    mean_processing_ms: 0.918775804060353\n",
      "  time_since_restore: 1551.492828130722\n",
      "  time_this_iter_s: 3.8858988285064697\n",
      "  time_total_s: 1551.492828130722\n",
      "  timestamp: 1595950191\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 460\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1551 s, 460 iter, 920000 ts, 10.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-29-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.573966305866668\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 460\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.229\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1726397275924683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1377563825808465e-05\n",
      "        policy_loss: -0.00016706275346223265\n",
      "        total_loss: 0.007230278104543686\n",
      "        vf_explained_var: 0.7415329813957214\n",
      "        vf_loss: 0.007397337816655636\n",
      "    load_time_ms: 1.426\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    sample_time_ms: 2790.77\n",
      "    update_time_ms: 6.457\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.02499999999999\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.98034364511916\n",
      "    mean_inference_ms: 1.3738517797553016\n",
      "    mean_processing_ms: 0.9187758040603528\n",
      "  time_since_restore: 1557.3012931346893\n",
      "  time_this_iter_s: 2.80772066116333\n",
      "  time_total_s: 1557.3012931346893\n",
      "  timestamp: 1595950197\n",
      "  timesteps_since_restore: 924000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 462\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1557 s, 462 iter, 924000 ts, 10.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.573966305866668\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 460\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.4\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2569465637207031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8833696231013164e-05\n",
      "        policy_loss: -0.0004703740996774286\n",
      "        total_loss: 0.0012720327358692884\n",
      "        vf_explained_var: 0.9343502521514893\n",
      "        vf_loss: 0.0017423877725377679\n",
      "    load_time_ms: 1.531\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    sample_time_ms: 2801.895\n",
      "    update_time_ms: 6.445\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.53333333333335\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.98034364511916\n",
      "    mean_inference_ms: 1.3738517797553016\n",
      "    mean_processing_ms: 0.9187758040603528\n",
      "  time_since_restore: 1562.6306385993958\n",
      "  time_this_iter_s: 2.5789682865142822\n",
      "  time_total_s: 1562.6306385993958\n",
      "  timestamp: 1595950202\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 464\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1562 s, 464 iter, 928000 ts, 10.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.573858909444303\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 465\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0937057733535767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.426442658063024e-05\n",
      "        policy_loss: -0.0006983766797930002\n",
      "        total_loss: 0.027425352483987808\n",
      "        vf_explained_var: 0.40067940950393677\n",
      "        vf_loss: 0.028123721480369568\n",
      "    load_time_ms: 1.47\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    sample_time_ms: 2887.527\n",
      "    update_time_ms: 5.641\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.525\n",
      "    ram_util_percent: 68.92500000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.966913342013515\n",
      "    mean_inference_ms: 1.3713494195644689\n",
      "    mean_processing_ms: 0.918029227194602\n",
      "  time_since_restore: 1569.1878101825714\n",
      "  time_this_iter_s: 2.578688144683838\n",
      "  time_total_s: 1569.1878101825714\n",
      "  timestamp: 1595950209\n",
      "  timesteps_since_restore: 932000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 466\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1569 s, 466 iter, 932000 ts, 10.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.573858909444303\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 465\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.334\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2126127481460571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6149153200094588e-05\n",
      "        policy_loss: -6.845664756838232e-05\n",
      "        total_loss: 0.0038644475862383842\n",
      "        vf_explained_var: 0.8587120771408081\n",
      "        vf_loss: 0.00393291050568223\n",
      "    load_time_ms: 1.466\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    sample_time_ms: 2937.732\n",
      "    update_time_ms: 5.628\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.475\n",
      "    ram_util_percent: 68.95\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.966913342013515\n",
      "    mean_inference_ms: 1.3713494195644689\n",
      "    mean_processing_ms: 0.918029227194602\n",
      "  time_since_restore: 1574.7566576004028\n",
      "  time_this_iter_s: 2.6162586212158203\n",
      "  time_total_s: 1574.7566576004028\n",
      "  timestamp: 1595950215\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 468\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1574 s, 468 iter, 936000 ts, 10.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.62839646698245\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 470\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.211\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2473605871200562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.92995501292171e-06\n",
      "        policy_loss: -0.00010759496944956481\n",
      "        total_loss: 0.22013680636882782\n",
      "        vf_explained_var: 0.03783297538757324\n",
      "        vf_loss: 0.2202444225549698\n",
      "    load_time_ms: 1.342\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    sample_time_ms: 2919.552\n",
      "    update_time_ms: 5.825\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.48333333333334\n",
      "    ram_util_percent: 68.94999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.953849547110885\n",
      "    mean_inference_ms: 1.3689302816624518\n",
      "    mean_processing_ms: 0.9173072347115068\n",
      "  time_since_restore: 1581.1374044418335\n",
      "  time_this_iter_s: 3.8012475967407227\n",
      "  time_total_s: 1581.1374044418335\n",
      "  timestamp: 1595950221\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 470\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1581 s, 470 iter, 940000 ts, 10.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.628396466982446\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 470\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.786\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1694347858428955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.121583697269671e-05\n",
      "        policy_loss: -0.00032532261684536934\n",
      "        total_loss: 0.008226976729929447\n",
      "        vf_explained_var: 0.7941328287124634\n",
      "        vf_loss: 0.008552291430532932\n",
      "    load_time_ms: 1.23\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    sample_time_ms: 2887.58\n",
      "    update_time_ms: 4.612\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.44000000000001\n",
      "    ram_util_percent: 68.94\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.953849547110885\n",
      "    mean_inference_ms: 1.3689302816624516\n",
      "    mean_processing_ms: 0.9173072347115067\n",
      "  time_since_restore: 1586.5749115943909\n",
      "  time_this_iter_s: 2.8391287326812744\n",
      "  time_total_s: 1586.5749115943909\n",
      "  timestamp: 1595950226\n",
      "  timesteps_since_restore: 944000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 472\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1586 s, 472 iter, 944000 ts, 10.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.628396466982446\n",
      "  episode_reward_min: 8.753233756311687\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 470\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2547922134399414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.532490260724444e-06\n",
      "        policy_loss: -0.0003827858017757535\n",
      "        total_loss: 0.001405161339789629\n",
      "        vf_explained_var: 0.9323396682739258\n",
      "        vf_loss: 0.001787954824976623\n",
      "    load_time_ms: 1.18\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    sample_time_ms: 3057.354\n",
      "    update_time_ms: 4.698\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.99999999999999\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.953849547110885\n",
      "    mean_inference_ms: 1.3689302816624516\n",
      "    mean_processing_ms: 0.9173072347115067\n",
      "  time_since_restore: 1593.5984489917755\n",
      "  time_this_iter_s: 3.481934070587158\n",
      "  time_total_s: 1593.5984489917755\n",
      "  timestamp: 1595950233\n",
      "  timesteps_since_restore: 948000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 474\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1593 s, 474 iter, 948000 ts, 10.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.678799006824214\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 475\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.648\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0857112407684326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4364560886169784e-05\n",
      "        policy_loss: -0.0005059370887465775\n",
      "        total_loss: 0.026158783584833145\n",
      "        vf_explained_var: 0.3972970247268677\n",
      "        vf_loss: 0.026664704084396362\n",
      "    load_time_ms: 1.383\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    sample_time_ms: 3237.841\n",
      "    update_time_ms: 5.968\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.86666666666667\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.941524326084037\n",
      "    mean_inference_ms: 1.366669400975019\n",
      "    mean_processing_ms: 0.916635969061095\n",
      "  time_since_restore: 1602.0332658290863\n",
      "  time_this_iter_s: 3.6778976917266846\n",
      "  time_total_s: 1602.0332658290863\n",
      "  timestamp: 1595950242\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 476\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1602 s, 476 iter, 952000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.678799006824214\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 475\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.08\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2176942825317383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.082202849531313e-06\n",
      "        policy_loss: 6.530189421027899e-05\n",
      "        total_loss: 0.002938455669209361\n",
      "        vf_explained_var: 0.8820479512214661\n",
      "        vf_loss: 0.0028731576167047024\n",
      "    load_time_ms: 1.523\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    sample_time_ms: 3367.142\n",
      "    update_time_ms: 6.861\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.04\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.941524326084037\n",
      "    mean_inference_ms: 1.366669400975019\n",
      "    mean_processing_ms: 0.916635969061095\n",
      "  time_since_restore: 1608.983875989914\n",
      "  time_this_iter_s: 3.394523859024048\n",
      "  time_total_s: 1608.983875989914\n",
      "  timestamp: 1595950249\n",
      "  timesteps_since_restore: 956000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 478\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1608 s, 478 iter, 956000 ts, 10.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-30-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.69512690771944\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 480\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2768722772598267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.583554411714431e-06\n",
      "        policy_loss: -0.00017642068269196898\n",
      "        total_loss: 0.2168351411819458\n",
      "        vf_explained_var: 0.01568460464477539\n",
      "        vf_loss: 0.2170114815235138\n",
      "    load_time_ms: 1.786\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    sample_time_ms: 3615.875\n",
      "    update_time_ms: 7.631\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.39999999999999\n",
      "    ram_util_percent: 69.05714285714285\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.930418858993281\n",
      "    mean_inference_ms: 1.364684283153627\n",
      "    mean_processing_ms: 0.9160393506969249\n",
      "  time_since_restore: 1617.9877099990845\n",
      "  time_this_iter_s: 5.265479803085327\n",
      "  time_total_s: 1617.9877099990845\n",
      "  timestamp: 1595950258\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 480\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1617 s, 480 iter, 960000 ts, 10.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.695126907719441\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 480\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.955\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1555930376052856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.180859731510282e-05\n",
      "        policy_loss: -0.00018458938575349748\n",
      "        total_loss: 0.006763671990483999\n",
      "        vf_explained_var: 0.6630679965019226\n",
      "        vf_loss: 0.006948278285562992\n",
      "    load_time_ms: 1.929\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    sample_time_ms: 3700.08\n",
      "    update_time_ms: 9.405\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.22\n",
      "    ram_util_percent: 69.04\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.930418858993281\n",
      "    mean_inference_ms: 1.364684283153627\n",
      "    mean_processing_ms: 0.9160393506969245\n",
      "  time_since_restore: 1624.3388476371765\n",
      "  time_this_iter_s: 3.0764427185058594\n",
      "  time_total_s: 1624.3388476371765\n",
      "  timestamp: 1595950264\n",
      "  timesteps_since_restore: 964000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 482\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1624 s, 482 iter, 964000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.695126907719441\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 480\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.626\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2641487121582031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.887001861992758e-06\n",
      "        policy_loss: -0.0005156898405402899\n",
      "        total_loss: 0.0008431816240772605\n",
      "        vf_explained_var: 0.9459310173988342\n",
      "        vf_loss: 0.0013588748406618834\n",
      "    load_time_ms: 1.984\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    sample_time_ms: 3707.742\n",
      "    update_time_ms: 10.397\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.82000000000001\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.930418858993281\n",
      "    mean_inference_ms: 1.364684283153627\n",
      "    mean_processing_ms: 0.9160393506969245\n",
      "  time_since_restore: 1631.4428627490997\n",
      "  time_this_iter_s: 3.579209327697754\n",
      "  time_total_s: 1631.4428627490997\n",
      "  timestamp: 1595950272\n",
      "  timesteps_since_restore: 968000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 484\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1631 s, 484 iter, 968000 ts, 10.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.689578385362045\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 485\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.122\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0622174739837646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5949089831556194e-05\n",
      "        policy_loss: -0.0003159856714773923\n",
      "        total_loss: 0.025577988475561142\n",
      "        vf_explained_var: 0.4484666585922241\n",
      "        vf_loss: 0.025893958285450935\n",
      "    load_time_ms: 1.962\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    sample_time_ms: 3670.874\n",
      "    update_time_ms: 10.557\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.075\n",
      "    ram_util_percent: 69.15\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.920554659534811\n",
      "    mean_inference_ms: 1.3629766325630084\n",
      "    mean_processing_ms: 0.9155377263066913\n",
      "  time_since_restore: 1639.514449596405\n",
      "  time_this_iter_s: 3.1354517936706543\n",
      "  time_total_s: 1639.514449596405\n",
      "  timestamp: 1595950280\n",
      "  timesteps_since_restore: 972000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 486\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1639 s, 486 iter, 972000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.689578385362045\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 485\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.807\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1962933540344238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.664869968313724e-05\n",
      "        policy_loss: -0.0004949340946041048\n",
      "        total_loss: 0.002683158963918686\n",
      "        vf_explained_var: 0.9086658358573914\n",
      "        vf_loss: 0.0031780831050127745\n",
      "    load_time_ms: 1.913\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    sample_time_ms: 3667.066\n",
      "    update_time_ms: 10.92\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.94\n",
      "    ram_util_percent: 69.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.920554659534811\n",
      "    mean_inference_ms: 1.3629766325630084\n",
      "    mean_processing_ms: 0.9155377263066913\n",
      "  time_since_restore: 1646.3867075443268\n",
      "  time_this_iter_s: 3.6026182174682617\n",
      "  time_total_s: 1646.3867075443268\n",
      "  timestamp: 1595950286\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 488\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1646 s, 488 iter, 976000 ts, 10.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.716927740896073\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 490\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.032\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2496931552886963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.814867447246797e-06\n",
      "        policy_loss: -5.05762109241914e-05\n",
      "        total_loss: 0.22069217264652252\n",
      "        vf_explained_var: 0.07830238342285156\n",
      "        vf_loss: 0.22074276208877563\n",
      "    load_time_ms: 2.065\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    sample_time_ms: 3603.798\n",
      "    update_time_ms: 10.534\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.44285714285714\n",
      "    ram_util_percent: 69.17142857142856\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.912029741069541\n",
      "    mean_inference_ms: 1.361560904884642\n",
      "    mean_processing_ms: 0.9151289045090419\n",
      "  time_since_restore: 1654.6937592029572\n",
      "  time_this_iter_s: 4.795025825500488\n",
      "  time_total_s: 1654.6937592029572\n",
      "  timestamp: 1595950295\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 490\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1654 s, 490 iter, 980000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.716927740896073\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 490\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.524\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1519083976745605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.017591330310097e-05\n",
      "        policy_loss: -7.62939453125e-05\n",
      "        total_loss: 0.006054897326976061\n",
      "        vf_explained_var: 0.7805359959602356\n",
      "        vf_loss: 0.006131185684353113\n",
      "    load_time_ms: 1.99\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    sample_time_ms: 3650.363\n",
      "    update_time_ms: 9.821\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.13999999999999\n",
      "    ram_util_percent: 69.16\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.912029741069541\n",
      "    mean_inference_ms: 1.361560904884642\n",
      "    mean_processing_ms: 0.915128904509042\n",
      "  time_since_restore: 1661.4777085781097\n",
      "  time_this_iter_s: 3.5655152797698975\n",
      "  time_total_s: 1661.4777085781097\n",
      "  timestamp: 1595950302\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 492\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1661 s, 492 iter, 984000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.716927740896073\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 490\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.717\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.23288893699646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8810292860725895e-05\n",
      "        policy_loss: -0.0007177467341534793\n",
      "        total_loss: 0.0011671342654153705\n",
      "        vf_explained_var: 0.9221542477607727\n",
      "        vf_loss: 0.0018848760519176722\n",
      "    load_time_ms: 1.935\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    sample_time_ms: 3606.703\n",
      "    update_time_ms: 9.099\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69999999999999\n",
      "    ram_util_percent: 69.275\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.912029741069541\n",
      "    mean_inference_ms: 1.361560904884642\n",
      "    mean_processing_ms: 0.915128904509042\n",
      "  time_since_restore: 1668.1404340267181\n",
      "  time_this_iter_s: 3.300537586212158\n",
      "  time_total_s: 1668.1404340267181\n",
      "  timestamp: 1595950308\n",
      "  timesteps_since_restore: 988000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 494\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1668 s, 494 iter, 988000 ts, 10.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.730374735405322\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 495\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.867\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.050189733505249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.216086745145731e-05\n",
      "        policy_loss: -0.0005694632418453693\n",
      "        total_loss: 0.026816032826900482\n",
      "        vf_explained_var: 0.4030609130859375\n",
      "        vf_loss: 0.027385497465729713\n",
      "    load_time_ms: 1.99\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    sample_time_ms: 3627.218\n",
      "    update_time_ms: 8.637\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.28\n",
      "    ram_util_percent: 69.14000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9046151262162665\n",
      "    mean_inference_ms: 1.3603776847110556\n",
      "    mean_processing_ms: 0.9148005936241447\n",
      "  time_since_restore: 1676.4360041618347\n",
      "  time_this_iter_s: 3.603971242904663\n",
      "  time_total_s: 1676.4360041618347\n",
      "  timestamp: 1595950317\n",
      "  timesteps_since_restore: 992000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 496\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1676 s, 496 iter, 992000 ts, 10.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.730374735405322\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 495\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.755\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1910805702209473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7138301700470038e-05\n",
      "        policy_loss: 0.0001434726727893576\n",
      "        total_loss: 0.0030568866059184074\n",
      "        vf_explained_var: 0.8259185552597046\n",
      "        vf_loss: 0.002913400763645768\n",
      "    load_time_ms: 1.956\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    sample_time_ms: 3587.248\n",
      "    update_time_ms: 8.177\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.125\n",
      "    ram_util_percent: 69.125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9046151262162665\n",
      "    mean_inference_ms: 1.3603776847110556\n",
      "    mean_processing_ms: 0.9148005936241447\n",
      "  time_since_restore: 1682.8827424049377\n",
      "  time_this_iter_s: 3.227916717529297\n",
      "  time_total_s: 1682.8827424049377\n",
      "  timestamp: 1595950323\n",
      "  timesteps_since_restore: 996000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 498\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1682 s, 498 iter, 996000 ts, 10.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.734825641088584\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 500\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.718\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.246609091758728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5971532977564493e-06\n",
      "        policy_loss: 6.567763921339065e-05\n",
      "        total_loss: 0.23488250374794006\n",
      "        vf_explained_var: -0.023194432258605957\n",
      "        vf_loss: 0.23481684923171997\n",
      "    load_time_ms: 1.677\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    sample_time_ms: 3534.587\n",
      "    update_time_ms: 7.861\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.85714285714286\n",
      "    ram_util_percent: 69.15714285714286\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.897911461088757\n",
      "    mean_inference_ms: 1.3593202274128067\n",
      "    mean_processing_ms: 0.9145330002647742\n",
      "  time_since_restore: 1690.6492097377777\n",
      "  time_this_iter_s: 4.408935546875\n",
      "  time_total_s: 1690.6492097377777\n",
      "  timestamp: 1595950331\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 500\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1690 s, 500 iter, 1000000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.734825641088584\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 500\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.213\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1518124341964722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5123208615696058e-05\n",
      "        policy_loss: -0.00011760759662138298\n",
      "        total_loss: 0.005443069618195295\n",
      "        vf_explained_var: 0.7759957313537598\n",
      "        vf_loss: 0.005560697056353092\n",
      "    load_time_ms: 1.676\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "    sample_time_ms: 3472.401\n",
      "    update_time_ms: 7.799\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.8\n",
      "    ram_util_percent: 69.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.897911461088759\n",
      "    mean_inference_ms: 1.359320227412807\n",
      "    mean_processing_ms: 0.914533000264774\n",
      "  time_since_restore: 1696.8068182468414\n",
      "  time_this_iter_s: 3.183710813522339\n",
      "  time_total_s: 1696.8068182468414\n",
      "  timestamp: 1595950337\n",
      "  timesteps_since_restore: 1004000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 502\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1696 s, 502 iter, 1004000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.734825641088584\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 500\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.055\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2096554040908813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2441963008313905e-05\n",
      "        policy_loss: -0.0004528360441327095\n",
      "        total_loss: 0.002017067512497306\n",
      "        vf_explained_var: 0.9427803158760071\n",
      "        vf_loss: 0.0024699177592992783\n",
      "    load_time_ms: 1.825\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    sample_time_ms: 3450.808\n",
      "    update_time_ms: 7.841\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.7\n",
      "    ram_util_percent: 69.62\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.897911461088759\n",
      "    mean_inference_ms: 1.359320227412807\n",
      "    mean_processing_ms: 0.914533000264774\n",
      "  time_since_restore: 1703.2773950099945\n",
      "  time_this_iter_s: 3.2675282955169678\n",
      "  time_total_s: 1703.2773950099945\n",
      "  timestamp: 1595950344\n",
      "  timesteps_since_restore: 1008000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 504\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1703 s, 504 iter, 1008000 ts, 10.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.741982516423445\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 505\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.187\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0358781814575195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.265345003455877e-05\n",
      "        policy_loss: -0.00040482330950908363\n",
      "        total_loss: 0.02634016051888466\n",
      "        vf_explained_var: 0.44715821743011475\n",
      "        vf_loss: 0.026744980365037918\n",
      "    load_time_ms: 1.711\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "    sample_time_ms: 3427.647\n",
      "    update_time_ms: 8.017\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.225\n",
      "    ram_util_percent: 69.875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.891986406645762\n",
      "    mean_inference_ms: 1.358424234699607\n",
      "    mean_processing_ms: 0.9143058381407235\n",
      "  time_since_restore: 1711.3257830142975\n",
      "  time_this_iter_s: 3.1141245365142822\n",
      "  time_total_s: 1711.3257830142975\n",
      "  timestamp: 1595950352\n",
      "  timesteps_since_restore: 1012000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 506\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1711 s, 506 iter, 1012000 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.741982516423445\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 505\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.326\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1862263679504395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0959973312528746e-07\n",
      "        policy_loss: -4.506492768996395e-05\n",
      "        total_loss: 0.00257282261736691\n",
      "        vf_explained_var: 0.854932427406311\n",
      "        vf_loss: 0.0026178890839219093\n",
      "    load_time_ms: 1.7\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    sample_time_ms: 3428.327\n",
      "    update_time_ms: 8.019\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.875\n",
      "    ram_util_percent: 69.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.891986406645762\n",
      "    mean_inference_ms: 1.358424234699607\n",
      "    mean_processing_ms: 0.9143058381407235\n",
      "  time_since_restore: 1717.7846629619598\n",
      "  time_this_iter_s: 3.1999967098236084\n",
      "  time_total_s: 1717.7846629619598\n",
      "  timestamp: 1595950358\n",
      "  timesteps_since_restore: 1016000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 508\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1717 s, 508 iter, 1016000 ts, 10.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.77170863787159\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 510\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.189\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.221005916595459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.32763389046886e-05\n",
      "        policy_loss: 5.483389031724073e-05\n",
      "        total_loss: 0.24381965398788452\n",
      "        vf_explained_var: 0.01364445686340332\n",
      "        vf_loss: 0.24376486241817474\n",
      "    load_time_ms: 1.862\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    sample_time_ms: 3428.733\n",
      "    update_time_ms: 9.072\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.90000000000001\n",
      "    ram_util_percent: 69.97142857142856\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.886841053610685\n",
      "    mean_inference_ms: 1.3576731424077184\n",
      "    mean_processing_ms: 0.914127266529421\n",
      "  time_since_restore: 1725.5504088401794\n",
      "  time_this_iter_s: 4.37835431098938\n",
      "  time_total_s: 1725.5504088401794\n",
      "  timestamp: 1595950366\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 510\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1725 s, 510 iter, 1020000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.771708637871589\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 510\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.365\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1295514106750488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.688658272149041e-05\n",
      "        policy_loss: -0.000795144063886255\n",
      "        total_loss: 0.005924932658672333\n",
      "        vf_explained_var: 0.7798507809638977\n",
      "        vf_loss: 0.006720077246427536\n",
      "    load_time_ms: 1.875\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    sample_time_ms: 3462.952\n",
      "    update_time_ms: 9.14\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.3\n",
      "    ram_util_percent: 69.94000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.886841053610684\n",
      "    mean_inference_ms: 1.3576731424077184\n",
      "    mean_processing_ms: 0.914127266529421\n",
      "  time_since_restore: 1732.0533978939056\n",
      "  time_this_iter_s: 3.3355002403259277\n",
      "  time_total_s: 1732.0533978939056\n",
      "  timestamp: 1595950373\n",
      "  timesteps_since_restore: 1024000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 512\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1732 s, 512 iter, 1024000 ts, 10.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-32-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.308646449322124\n",
      "  episode_reward_mean: 10.771708637871589\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 510\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.207\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1954296827316284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4335631931317039e-05\n",
      "        policy_loss: -0.0005642354371957481\n",
      "        total_loss: 0.001617103349417448\n",
      "        vf_explained_var: 0.872527003288269\n",
      "        vf_loss: 0.002181329997256398\n",
      "    load_time_ms: 1.772\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "    sample_time_ms: 3495.859\n",
      "    update_time_ms: 9.203\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35999999999999\n",
      "    ram_util_percent: 70.05999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.886841053610684\n",
      "    mean_inference_ms: 1.3576731424077184\n",
      "    mean_processing_ms: 0.914127266529421\n",
      "  time_since_restore: 1738.8357067108154\n",
      "  time_this_iter_s: 3.1482810974121094\n",
      "  time_total_s: 1738.8357067108154\n",
      "  timestamp: 1595950379\n",
      "  timesteps_since_restore: 1028000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 514\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1738 s, 514 iter, 1028000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.788739711018257\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 515\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.71\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0374823808670044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3387500985118095e-05\n",
      "        policy_loss: -0.00017618465062696487\n",
      "        total_loss: 0.02555646188557148\n",
      "        vf_explained_var: 0.5244995951652527\n",
      "        vf_loss: 0.02573264017701149\n",
      "    load_time_ms: 1.799\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    sample_time_ms: 3458.294\n",
      "    update_time_ms: 10.738\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.34\n",
      "    ram_util_percent: 70.38\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.882466567435484\n",
      "    mean_inference_ms: 1.3570779518365215\n",
      "    mean_processing_ms: 0.9140032757964259\n",
      "  time_since_restore: 1746.5127668380737\n",
      "  time_this_iter_s: 3.1987526416778564\n",
      "  time_total_s: 1746.5127668380737\n",
      "  timestamp: 1595950387\n",
      "  timesteps_since_restore: 1032000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 516\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1746 s, 516 iter, 1032000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.788739711018257\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 515\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.751\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1940407752990723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.662918283633189e-06\n",
      "        policy_loss: -0.0001334609987679869\n",
      "        total_loss: 0.0021113071125000715\n",
      "        vf_explained_var: 0.9176822304725647\n",
      "        vf_loss: 0.002244768664240837\n",
      "    load_time_ms: 1.881\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "    sample_time_ms: 3542.489\n",
      "    update_time_ms: 10.646\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.64\n",
      "    ram_util_percent: 70.72\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.882466567435484\n",
      "    mean_inference_ms: 1.3570779518365215\n",
      "    mean_processing_ms: 0.9140032757964259\n",
      "  time_since_restore: 1753.8312768936157\n",
      "  time_this_iter_s: 3.718118190765381\n",
      "  time_total_s: 1753.8312768936157\n",
      "  timestamp: 1595950394\n",
      "  timesteps_since_restore: 1036000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 518\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1753 s, 518 iter, 1036000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.796843142411984\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 520\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.334\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2254189252853394\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.794499075913336e-05\n",
      "        policy_loss: -0.0001453855074942112\n",
      "        total_loss: 0.24635250866413116\n",
      "        vf_explained_var: -0.002979874610900879\n",
      "        vf_loss: 0.24649789929389954\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    sample_time_ms: 3543.96\n",
      "    update_time_ms: 10.226\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.86666666666667\n",
      "    ram_util_percent: 70.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8790507736586495\n",
      "    mean_inference_ms: 1.3566831922069138\n",
      "    mean_processing_ms: 0.9139349833021886\n",
      "  time_since_restore: 1761.632533788681\n",
      "  time_this_iter_s: 4.514345169067383\n",
      "  time_total_s: 1761.632533788681\n",
      "  timestamp: 1595950402\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 520\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1761 s, 520 iter, 1040000 ts, 10.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.796843142411984\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 520\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.112\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.12739098072052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.6652984211687e-05\n",
      "        policy_loss: -0.00039534259121865034\n",
      "        total_loss: 0.005460167769342661\n",
      "        vf_explained_var: 0.7375046014785767\n",
      "        vf_loss: 0.005855516530573368\n",
      "    load_time_ms: 1.734\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "    sample_time_ms: 3570.83\n",
      "    update_time_ms: 10.304\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.76\n",
      "    ram_util_percent: 70.16\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8790507736586495\n",
      "    mean_inference_ms: 1.3566831922069145\n",
      "    mean_processing_ms: 0.9139349833021891\n",
      "  time_since_restore: 1768.4081246852875\n",
      "  time_this_iter_s: 3.3486616611480713\n",
      "  time_total_s: 1768.4081246852875\n",
      "  timestamp: 1595950409\n",
      "  timesteps_since_restore: 1044000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 522\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1768 s, 522 iter, 1044000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.796843142411984\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 520\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.62\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2228785753250122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7728567399899475e-05\n",
      "        policy_loss: -0.0007653161301277578\n",
      "        total_loss: 0.000644246581941843\n",
      "        vf_explained_var: 0.9348657727241516\n",
      "        vf_loss: 0.0014095514779910445\n",
      "    load_time_ms: 1.676\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    sample_time_ms: 3548.941\n",
      "    update_time_ms: 10.866\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.75\n",
      "    ram_util_percent: 70.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8790507736586495\n",
      "    mean_inference_ms: 1.3566831922069145\n",
      "    mean_processing_ms: 0.9139349833021891\n",
      "  time_since_restore: 1774.9783611297607\n",
      "  time_this_iter_s: 3.0729551315307617\n",
      "  time_total_s: 1774.9783611297607\n",
      "  timestamp: 1595950416\n",
      "  timesteps_since_restore: 1048000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 524\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1774 s, 524 iter, 1048000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.799977327510694\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 525\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.303\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.030645489692688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5860288840485737e-05\n",
      "        policy_loss: -0.0003620138159021735\n",
      "        total_loss: 0.025242280215024948\n",
      "        vf_explained_var: 0.49651193618774414\n",
      "        vf_loss: 0.02560429833829403\n",
      "    load_time_ms: 1.897\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "    sample_time_ms: 3624.2\n",
      "    update_time_ms: 14.65\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.68333333333334\n",
      "    ram_util_percent: 70.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.876530694630264\n",
      "    mean_inference_ms: 1.3564659677788202\n",
      "    mean_processing_ms: 0.9139138395477627\n",
      "  time_since_restore: 1783.5543808937073\n",
      "  time_this_iter_s: 3.7078354358673096\n",
      "  time_total_s: 1783.5543808937073\n",
      "  timestamp: 1595950424\n",
      "  timesteps_since_restore: 1052000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 526\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1783 s, 526 iter, 1052000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.799977327510694\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 525\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.874\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1719087362289429\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.68638022034429e-05\n",
      "        policy_loss: -0.00017414093599654734\n",
      "        total_loss: 0.002583299530670047\n",
      "        vf_explained_var: 0.8614322543144226\n",
      "        vf_loss: 0.002757446374744177\n",
      "    load_time_ms: 1.963\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    sample_time_ms: 3593.796\n",
      "    update_time_ms: 14.748\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06000000000002\n",
      "    ram_util_percent: 70.32000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.876530694630264\n",
      "    mean_inference_ms: 1.3564659677788202\n",
      "    mean_processing_ms: 0.9139138395477627\n",
      "  time_since_restore: 1790.5988132953644\n",
      "  time_this_iter_s: 3.5855064392089844\n",
      "  time_total_s: 1790.5988132953644\n",
      "  timestamp: 1595950431\n",
      "  timesteps_since_restore: 1056000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 528\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1790 s, 528 iter, 1056000 ts, 10.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.800415689740518\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 530\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.691\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2042219638824463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3738810594077222e-05\n",
      "        policy_loss: -8.349418931175023e-05\n",
      "        total_loss: 0.2569122016429901\n",
      "        vf_explained_var: -0.00029528141021728516\n",
      "        vf_loss: 0.25699564814567566\n",
      "    load_time_ms: 2.053\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    sample_time_ms: 3698.267\n",
      "    update_time_ms: 14.566\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.97142857142858\n",
      "    ram_util_percent: 70.38571428571429\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8750976803743535\n",
      "    mean_inference_ms: 1.356496753877981\n",
      "    mean_processing_ms: 0.9139493029611794\n",
      "  time_since_restore: 1799.4070270061493\n",
      "  time_this_iter_s: 5.197812080383301\n",
      "  time_total_s: 1799.4070270061493\n",
      "  timestamp: 1595950440\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 530\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1799 s, 530 iter, 1060000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.800415689740522\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 530\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.86\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1234245300292969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.0756098062265664e-05\n",
      "        policy_loss: -0.0005533923977054656\n",
      "        total_loss: 0.0048241992481052876\n",
      "        vf_explained_var: 0.7721892595291138\n",
      "        vf_loss: 0.005377589724957943\n",
      "    load_time_ms: 2.077\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    sample_time_ms: 3641.385\n",
      "    update_time_ms: 13.57\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.78\n",
      "    ram_util_percent: 70.35999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875097680374355\n",
      "    mean_inference_ms: 1.356496753877981\n",
      "    mean_processing_ms: 0.9139493029611794\n",
      "  time_since_restore: 1805.6240112781525\n",
      "  time_this_iter_s: 3.2739953994750977\n",
      "  time_total_s: 1805.6240112781525\n",
      "  timestamp: 1595950446\n",
      "  timesteps_since_restore: 1064000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 532\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1805 s, 532 iter, 1064000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.800415689740522\n",
      "  episode_reward_min: 8.96486702911218\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 530\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.872\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1963342428207397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.523726288927719e-05\n",
      "        policy_loss: -0.0007457752362824976\n",
      "        total_loss: 0.000828873657155782\n",
      "        vf_explained_var: 0.904617190361023\n",
      "        vf_loss: 0.0015746388817206025\n",
      "    load_time_ms: 2.106\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    sample_time_ms: 3672.8\n",
      "    update_time_ms: 13.356\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.54\n",
      "    ram_util_percent: 70.42\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875097680374355\n",
      "    mean_inference_ms: 1.356496753877981\n",
      "    mean_processing_ms: 0.9139493029611794\n",
      "  time_since_restore: 1812.5026323795319\n",
      "  time_this_iter_s: 3.4495606422424316\n",
      "  time_total_s: 1812.5026323795319\n",
      "  timestamp: 1595950453\n",
      "  timesteps_since_restore: 1068000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 534\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1812 s, 534 iter, 1068000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.819850191632804\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 535\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.813\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9915600419044495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.273495894973166e-05\n",
      "        policy_loss: -0.00014373016892932355\n",
      "        total_loss: 0.026735039427876472\n",
      "        vf_explained_var: 0.46328073740005493\n",
      "        vf_loss: 0.026878776028752327\n",
      "    load_time_ms: 2.17\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "    sample_time_ms: 3640.163\n",
      "    update_time_ms: 7.968\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.38000000000001\n",
      "    ram_util_percent: 70.64000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8745078650594165\n",
      "    mean_inference_ms: 1.3566857194069168\n",
      "    mean_processing_ms: 0.9140428850339113\n",
      "  time_since_restore: 1820.6329984664917\n",
      "  time_this_iter_s: 3.1575698852539062\n",
      "  time_total_s: 1820.6329984664917\n",
      "  timestamp: 1595950461\n",
      "  timesteps_since_restore: 1072000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 536\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1820 s, 536 iter, 1072000 ts, 10.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.819850191632804\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 535\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.898\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1739423274993896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1069804713770282e-05\n",
      "        policy_loss: -0.00033226393861696124\n",
      "        total_loss: 0.0015288409776985645\n",
      "        vf_explained_var: 0.9183663129806519\n",
      "        vf_loss: 0.0018610936822369695\n",
      "    load_time_ms: 2.181\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    sample_time_ms: 3659.823\n",
      "    update_time_ms: 9.436\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.84\n",
      "    ram_util_percent: 70.72\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8745078650594165\n",
      "    mean_inference_ms: 1.3566857194069168\n",
      "    mean_processing_ms: 0.9140428850339113\n",
      "  time_since_restore: 1827.882592201233\n",
      "  time_this_iter_s: 3.6407179832458496\n",
      "  time_total_s: 1827.882592201233\n",
      "  timestamp: 1595950469\n",
      "  timesteps_since_restore: 1076000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 538\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1827 s, 538 iter, 1076000 ts, 10.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.836643034601515\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 540\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.894\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1986792087554932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.6883830034639686e-05\n",
      "        policy_loss: -0.0002214736887253821\n",
      "        total_loss: 0.26053765416145325\n",
      "        vf_explained_var: -0.022449135780334473\n",
      "        vf_loss: 0.2607591450214386\n",
      "    load_time_ms: 2.04\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    sample_time_ms: 3589.112\n",
      "    update_time_ms: 9.184\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.48571428571428\n",
      "    ram_util_percent: 70.55714285714285\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.874826499603288\n",
      "    mean_inference_ms: 1.3570575172336086\n",
      "    mean_processing_ms: 0.9141718501236197\n",
      "  time_since_restore: 1835.9521186351776\n",
      "  time_this_iter_s: 4.648667097091675\n",
      "  time_total_s: 1835.9521186351776\n",
      "  timestamp: 1595950477\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 540\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1835 s, 540 iter, 1080000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.83664303460152\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 540\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.452\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.095672845840454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.366810051375069e-05\n",
      "        policy_loss: -0.00018223380902782083\n",
      "        total_loss: 0.005980930291116238\n",
      "        vf_explained_var: 0.7214866876602173\n",
      "        vf_loss: 0.006163171026855707\n",
      "    load_time_ms: 2.05\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    sample_time_ms: 3697.66\n",
      "    update_time_ms: 9.432\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.75\n",
      "    ram_util_percent: 70.51666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.874826499603286\n",
      "    mean_inference_ms: 1.3570575172336083\n",
      "    mean_processing_ms: 0.9141718501236196\n",
      "  time_since_restore: 1843.248019695282\n",
      "  time_this_iter_s: 4.045450925827026\n",
      "  time_total_s: 1843.248019695282\n",
      "  timestamp: 1595950484\n",
      "  timesteps_since_restore: 1084000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 542\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1843 s, 542 iter, 1084000 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.83664303460152\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 540\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.395\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.178958773612976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3586095267091878e-05\n",
      "        policy_loss: -0.00047036504838615656\n",
      "        total_loss: 0.001308560837060213\n",
      "        vf_explained_var: 0.883411169052124\n",
      "        vf_loss: 0.0017789292614907026\n",
      "    load_time_ms: 2.026\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "    sample_time_ms: 3693.523\n",
      "    update_time_ms: 9.218\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.18\n",
      "    ram_util_percent: 70.46000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.874826499603286\n",
      "    mean_inference_ms: 1.3570575172336083\n",
      "    mean_processing_ms: 0.9141718501236196\n",
      "  time_since_restore: 1850.09312748909\n",
      "  time_this_iter_s: 3.340493679046631\n",
      "  time_total_s: 1850.09312748909\n",
      "  timestamp: 1595950491\n",
      "  timesteps_since_restore: 1088000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 544\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1850 s, 544 iter, 1088000 ts, 10.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-34-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.858929117760345\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 545\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.642\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.975119948387146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.020618573325919e-06\n",
      "        policy_loss: -0.00013247394235804677\n",
      "        total_loss: 0.026571232825517654\n",
      "        vf_explained_var: 0.4240609407424927\n",
      "        vf_loss: 0.026703711599111557\n",
      "    load_time_ms: 1.83\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    sample_time_ms: 3697.107\n",
      "    update_time_ms: 10.574\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.94\n",
      "    ram_util_percent: 70.52000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875943688082658\n",
      "    mean_inference_ms: 1.3575939697098358\n",
      "    mean_processing_ms: 0.9143477319439717\n",
      "  time_since_restore: 1858.2627778053284\n",
      "  time_this_iter_s: 3.429680824279785\n",
      "  time_total_s: 1858.2627778053284\n",
      "  timestamp: 1595950499\n",
      "  timesteps_since_restore: 1092000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 546\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1858 s, 546 iter, 1092000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.858929117760345\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 545\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.201\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1296918392181396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.970784306759015e-05\n",
      "        policy_loss: -0.0002673950220923871\n",
      "        total_loss: 0.002748018829151988\n",
      "        vf_explained_var: 0.8035551309585571\n",
      "        vf_loss: 0.003015408292412758\n",
      "    load_time_ms: 1.72\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "    sample_time_ms: 3674.236\n",
      "    update_time_ms: 9.324\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.25\n",
      "    ram_util_percent: 70.58333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875943688082658\n",
      "    mean_inference_ms: 1.3575939697098358\n",
      "    mean_processing_ms: 0.9143477319439717\n",
      "  time_since_restore: 1865.2309646606445\n",
      "  time_this_iter_s: 3.5790767669677734\n",
      "  time_total_s: 1865.2309646606445\n",
      "  timestamp: 1595950506\n",
      "  timesteps_since_restore: 1096000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 548\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1865 s, 548 iter, 1096000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.866376579055208\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 550\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.215\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2157384157180786\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.437944532284746e-06\n",
      "        policy_loss: 4.1496990888845176e-05\n",
      "        total_loss: 0.2552182972431183\n",
      "        vf_explained_var: 0.025329113006591797\n",
      "        vf_loss: 0.255176842212677\n",
      "    load_time_ms: 1.79\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    sample_time_ms: 3705.625\n",
      "    update_time_ms: 9.349\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.4\n",
      "    ram_util_percent: 70.74285714285715\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8778095556309085\n",
      "    mean_inference_ms: 1.3582940424120589\n",
      "    mean_processing_ms: 0.9145654962735454\n",
      "  time_since_restore: 1873.6267001628876\n",
      "  time_this_iter_s: 4.8051536083221436\n",
      "  time_total_s: 1873.6267001628876\n",
      "  timestamp: 1595950515\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 550\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1873 s, 550 iter, 1100000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.866376579055208\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 550\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.523\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0767502784729004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.541729766875505e-05\n",
      "        policy_loss: -0.00045120049617253244\n",
      "        total_loss: 0.005780301988124847\n",
      "        vf_explained_var: 0.739465057849884\n",
      "        vf_loss: 0.006231480743736029\n",
      "    load_time_ms: 1.762\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    sample_time_ms: 3635.175\n",
      "    update_time_ms: 9.358\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.72\n",
      "    ram_util_percent: 70.88\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.877809555630909\n",
      "    mean_inference_ms: 1.3582940424120593\n",
      "    mean_processing_ms: 0.9145654962735452\n",
      "  time_since_restore: 1880.2085762023926\n",
      "  time_this_iter_s: 3.169013500213623\n",
      "  time_total_s: 1880.2085762023926\n",
      "  timestamp: 1595950521\n",
      "  timesteps_since_restore: 1104000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 552\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1880 s, 552 iter, 1104000 ts, 10.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.866376579055208\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 550\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.535\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1689461469650269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.016747955873143e-05\n",
      "        policy_loss: -0.0005288133397698402\n",
      "        total_loss: 0.001108469907194376\n",
      "        vf_explained_var: 0.9302800893783569\n",
      "        vf_loss: 0.0016372936079278588\n",
      "    load_time_ms: 2.072\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    sample_time_ms: 3677.358\n",
      "    update_time_ms: 8.729\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.58\n",
      "    ram_util_percent: 70.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.877809555630909\n",
      "    mean_inference_ms: 1.3582940424120593\n",
      "    mean_processing_ms: 0.9145654962735452\n",
      "  time_since_restore: 1887.5120611190796\n",
      "  time_this_iter_s: 3.943942070007324\n",
      "  time_total_s: 1887.5120611190796\n",
      "  timestamp: 1595950529\n",
      "  timesteps_since_restore: 1108000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 554\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1887 s, 554 iter, 1108000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.868739188938925\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 555\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.7\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9559978246688843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00013781592133454978\n",
      "        policy_loss: -0.0011245174100622535\n",
      "        total_loss: 0.025418145582079887\n",
      "        vf_explained_var: 0.45846831798553467\n",
      "        vf_loss: 0.026542648673057556\n",
      "    load_time_ms: 2.019\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "    sample_time_ms: 3669.842\n",
      "    update_time_ms: 8.0\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.7\n",
      "    ram_util_percent: 70.86\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8804790914512175\n",
      "    mean_inference_ms: 1.3591621408271461\n",
      "    mean_processing_ms: 0.9148335617251107\n",
      "  time_since_restore: 1895.5776274204254\n",
      "  time_this_iter_s: 3.2104616165161133\n",
      "  time_total_s: 1895.5776274204254\n",
      "  timestamp: 1595950537\n",
      "  timesteps_since_restore: 1112000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 556\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1895 s, 556 iter, 1112000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.868739188938925\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 555\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.523\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.120690941810608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.3833275160286576e-05\n",
      "        policy_loss: -0.0003374929365236312\n",
      "        total_loss: 0.002294293837621808\n",
      "        vf_explained_var: 0.8573762774467468\n",
      "        vf_loss: 0.0026317923329770565\n",
      "    load_time_ms: 2.154\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    sample_time_ms: 3638.651\n",
      "    update_time_ms: 8.59\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.38\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8804790914512175\n",
      "    mean_inference_ms: 1.3591621408271461\n",
      "    mean_processing_ms: 0.9148335617251107\n",
      "  time_since_restore: 1902.3419604301453\n",
      "  time_this_iter_s: 3.452467679977417\n",
      "  time_total_s: 1902.3419604301453\n",
      "  timestamp: 1595950544\n",
      "  timesteps_since_restore: 1116000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 558\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1902 s, 558 iter, 1116000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.861048736404191\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 560\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.782\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1564594507217407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4548033504979685e-05\n",
      "        policy_loss: -9.988975216401741e-05\n",
      "        total_loss: 0.26803869009017944\n",
      "        vf_explained_var: 0.04558652639389038\n",
      "        vf_loss: 0.2681386172771454\n",
      "    load_time_ms: 2.186\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    sample_time_ms: 3586.823\n",
      "    update_time_ms: 8.88\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58571428571429\n",
      "    ram_util_percent: 70.78571428571429\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.883889774704161\n",
      "    mean_inference_ms: 1.3601866125783821\n",
      "    mean_processing_ms: 0.9151481846242966\n",
      "  time_since_restore: 1910.2924330234528\n",
      "  time_this_iter_s: 4.6587440967559814\n",
      "  time_total_s: 1910.2924330234528\n",
      "  timestamp: 1595950552\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 560\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1910 s, 560 iter, 1120000 ts, 10.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-35-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.861048736404191\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 560\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.458\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0619112253189087\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.410821126541123e-05\n",
      "        policy_loss: -9.069537918549031e-05\n",
      "        total_loss: 0.005380129907280207\n",
      "        vf_explained_var: 0.7626311182975769\n",
      "        vf_loss: 0.005470830947160721\n",
      "    load_time_ms: 2.192\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    sample_time_ms: 3605.108\n",
      "    update_time_ms: 9.438\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.11999999999999\n",
      "    ram_util_percent: 70.84\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.883889774704161\n",
      "    mean_inference_ms: 1.3601866125783821\n",
      "    mean_processing_ms: 0.9151481846242966\n",
      "  time_since_restore: 1917.0798227787018\n",
      "  time_this_iter_s: 3.5074450969696045\n",
      "  time_total_s: 1917.0798227787018\n",
      "  timestamp: 1595950558\n",
      "  timesteps_since_restore: 1124000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 562\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1917 s, 562 iter, 1124000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.861048736404191\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 560\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.563\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1652636528015137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.6543201935710385e-05\n",
      "        policy_loss: -0.0011677102884277701\n",
      "        total_loss: 0.00026424217503517866\n",
      "        vf_explained_var: 0.9003349542617798\n",
      "        vf_loss: 0.0014319752808660269\n",
      "    load_time_ms: 1.88\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    sample_time_ms: 3558.673\n",
      "    update_time_ms: 10.213\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.32000000000001\n",
      "    ram_util_percent: 70.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.883889774704161\n",
      "    mean_inference_ms: 1.3601866125783821\n",
      "    mean_processing_ms: 0.9151481846242966\n",
      "  time_since_restore: 1923.8741731643677\n",
      "  time_this_iter_s: 3.3251137733459473\n",
      "  time_total_s: 1923.8741731643677\n",
      "  timestamp: 1595950565\n",
      "  timesteps_since_restore: 1128000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 564\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1923 s, 564 iter, 1128000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.866171954994089\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 565\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.094\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9322348237037659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.723527571419254e-06\n",
      "        policy_loss: -0.00015006065950728953\n",
      "        total_loss: 0.027762390673160553\n",
      "        vf_explained_var: 0.45072370767593384\n",
      "        vf_loss: 0.02791243977844715\n",
      "    load_time_ms: 1.898\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    sample_time_ms: 3599.693\n",
      "    update_time_ms: 9.738\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.5\n",
      "    ram_util_percent: 70.93333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.887868422111483\n",
      "    mean_inference_ms: 1.3613272629350002\n",
      "    mean_processing_ms: 0.9154849981787224\n",
      "  time_since_restore: 1932.3452196121216\n",
      "  time_this_iter_s: 3.8381876945495605\n",
      "  time_total_s: 1932.3452196121216\n",
      "  timestamp: 1595950574\n",
      "  timesteps_since_restore: 1132000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 566\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1932 s, 566 iter, 1132000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.866171954994089\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 565\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.614\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1280220746994019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2004375093965791e-05\n",
      "        policy_loss: -0.0002846727438736707\n",
      "        total_loss: 0.001917762798257172\n",
      "        vf_explained_var: 0.8171331882476807\n",
      "        vf_loss: 0.002202441217377782\n",
      "    load_time_ms: 1.728\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "    sample_time_ms: 3617.287\n",
      "    update_time_ms: 9.89\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75\n",
      "    ram_util_percent: 70.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.887868422111483\n",
      "    mean_inference_ms: 1.3613272629350002\n",
      "    mean_processing_ms: 0.9154849981787224\n",
      "  time_since_restore: 1939.2042531967163\n",
      "  time_this_iter_s: 3.441929578781128\n",
      "  time_total_s: 1939.2042531967163\n",
      "  timestamp: 1595950581\n",
      "  timesteps_since_restore: 1136000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 568\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1939 s, 568 iter, 1136000 ts, 10.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.855442175765402\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 570\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.684\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1525070667266846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5131026884773746e-05\n",
      "        policy_loss: -0.00014634703984484076\n",
      "        total_loss: 0.2773517072200775\n",
      "        vf_explained_var: -0.005384206771850586\n",
      "        vf_loss: 0.2774980068206787\n",
      "    load_time_ms: 1.674\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    sample_time_ms: 3669.851\n",
      "    update_time_ms: 9.873\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7\n",
      "    ram_util_percent: 71.01428571428572\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.892628704342006\n",
      "    mean_inference_ms: 1.3626281546457661\n",
      "    mean_processing_ms: 0.9158769510820782\n",
      "  time_since_restore: 1947.6678025722504\n",
      "  time_this_iter_s: 4.824721336364746\n",
      "  time_total_s: 1947.6678025722504\n",
      "  timestamp: 1595950589\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 570\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1947 s, 570 iter, 1140000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.855442175765399\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 570\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.309\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.044265627861023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.441852459218353e-05\n",
      "        policy_loss: -1.2579917893162929e-05\n",
      "        total_loss: 0.007055759429931641\n",
      "        vf_explained_var: 0.783733606338501\n",
      "        vf_loss: 0.007068340200930834\n",
      "    load_time_ms: 1.748\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "    sample_time_ms: 3637.701\n",
      "    update_time_ms: 9.602\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.47999999999999\n",
      "    ram_util_percent: 70.76\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8926287043420045\n",
      "    mean_inference_ms: 1.3626281546457661\n",
      "    mean_processing_ms: 0.9158769510820783\n",
      "  time_since_restore: 1954.1340749263763\n",
      "  time_this_iter_s: 3.297960042953491\n",
      "  time_total_s: 1954.1340749263763\n",
      "  timestamp: 1595950596\n",
      "  timesteps_since_restore: 1144000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 572\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1954 s, 572 iter, 1144000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.855442175765399\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 570\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.898\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.145730972290039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0558974685845897e-05\n",
      "        policy_loss: -0.00026438903296366334\n",
      "        total_loss: 0.0015291471499949694\n",
      "        vf_explained_var: 0.8804198503494263\n",
      "        vf_loss: 0.0017935116775333881\n",
      "    load_time_ms: 1.801\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    sample_time_ms: 3640.616\n",
      "    update_time_ms: 9.497\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.64000000000001\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8926287043420045\n",
      "    mean_inference_ms: 1.3626281546457661\n",
      "    mean_processing_ms: 0.9158769510820783\n",
      "  time_since_restore: 1960.9631571769714\n",
      "  time_this_iter_s: 3.533390522003174\n",
      "  time_total_s: 1960.9631571769714\n",
      "  timestamp: 1595950602\n",
      "  timesteps_since_restore: 1148000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 574\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1960 s, 574 iter, 1148000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.880201340450558\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 575\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.27\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9320220947265625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0249167644360568e-05\n",
      "        policy_loss: -0.00018837786046788096\n",
      "        total_loss: 0.026936007663607597\n",
      "        vf_explained_var: 0.4874013066291809\n",
      "        vf_loss: 0.02712439000606537\n",
      "    load_time_ms: 1.867\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    sample_time_ms: 3552.251\n",
      "    update_time_ms: 9.11\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.7\n",
      "    ram_util_percent: 70.74000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.897391851787474\n",
      "    mean_inference_ms: 1.363902883487087\n",
      "    mean_processing_ms: 0.9162753586585539\n",
      "  time_since_restore: 1968.6010422706604\n",
      "  time_this_iter_s: 3.3172647953033447\n",
      "  time_total_s: 1968.6010422706604\n",
      "  timestamp: 1595950610\n",
      "  timesteps_since_restore: 1152000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 576\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1968 s, 576 iter, 1152000 ts, 10.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-36-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.916973527527926\n",
      "  episode_reward_mean: 10.880201340450558\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 575\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.986\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1165120601654053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5685081962146796e-05\n",
      "        policy_loss: -9.241962106898427e-05\n",
      "        total_loss: 0.0023114834912121296\n",
      "        vf_explained_var: 0.874774694442749\n",
      "        vf_loss: 0.002403904916718602\n",
      "    load_time_ms: 1.885\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    sample_time_ms: 3559.859\n",
      "    update_time_ms: 8.078\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.16666666666667\n",
      "    ram_util_percent: 70.64999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.897391851787474\n",
      "    mean_inference_ms: 1.363902883487087\n",
      "    mean_processing_ms: 0.9162753586585539\n",
      "  time_since_restore: 1975.5698766708374\n",
      "  time_this_iter_s: 3.7853405475616455\n",
      "  time_total_s: 1975.5698766708374\n",
      "  timestamp: 1595950617\n",
      "  timesteps_since_restore: 1156000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 578\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1975 s, 578 iter, 1156000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.90998483259797\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 580\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.212\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1649456024169922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.889394863814232e-06\n",
      "        policy_loss: 6.979465251788497e-05\n",
      "        total_loss: 0.28190311789512634\n",
      "        vf_explained_var: -0.008060812950134277\n",
      "        vf_loss: 0.28183332085609436\n",
      "    load_time_ms: 1.907\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    sample_time_ms: 3561.881\n",
      "    update_time_ms: 8.725\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.6\n",
      "    ram_util_percent: 70.875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.90189651027503\n",
      "    mean_inference_ms: 1.3650903342035363\n",
      "    mean_processing_ms: 0.9166680239916893\n",
      "  time_since_restore: 1984.0569446086884\n",
      "  time_this_iter_s: 5.202585697174072\n",
      "  time_total_s: 1984.0569446086884\n",
      "  timestamp: 1595950626\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 580\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1984 s, 580 iter, 1160000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.909984832597972\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 580\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 53.14\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0642001628875732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5220453608199023e-05\n",
      "        policy_loss: -0.0002076492237392813\n",
      "        total_loss: 0.005184573121368885\n",
      "        vf_explained_var: 0.7995905876159668\n",
      "        vf_loss: 0.0053922138176858425\n",
      "    load_time_ms: 2.141\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    sample_time_ms: 3687.123\n",
      "    update_time_ms: 9.71\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.31666666666666\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.90189651027503\n",
      "    mean_inference_ms: 1.3650903342035365\n",
      "    mean_processing_ms: 0.9166680239916891\n",
      "  time_since_restore: 1991.8239061832428\n",
      "  time_this_iter_s: 4.088188409805298\n",
      "  time_total_s: 1991.8239061832428\n",
      "  timestamp: 1595950633\n",
      "  timesteps_since_restore: 1164000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 582\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1991 s, 582 iter, 1164000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.909984832597972\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 580\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.816\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.143530011177063\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2517770048580132e-05\n",
      "        policy_loss: -0.0003034686960745603\n",
      "        total_loss: 0.0014831990702077746\n",
      "        vf_explained_var: 0.8770009279251099\n",
      "        vf_loss: 0.001786665408872068\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1168000\n",
      "    sample_time_ms: 3744.9\n",
      "    update_time_ms: 9.443\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.97999999999999\n",
      "    ram_util_percent: 70.86\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.90189651027503\n",
      "    mean_inference_ms: 1.3650903342035365\n",
      "    mean_processing_ms: 0.9166680239916891\n",
      "  time_since_restore: 1999.2601191997528\n",
      "  time_this_iter_s: 3.779836654663086\n",
      "  time_total_s: 1999.2601191997528\n",
      "  timestamp: 1595950641\n",
      "  timesteps_since_restore: 1168000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 584\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 1999 s, 584 iter, 1168000 ts, 10.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.945248905851996\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 585\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.851\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9328655004501343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001646094024181366\n",
      "        policy_loss: -0.0016574420733377337\n",
      "        total_loss: 0.0251899566501379\n",
      "        vf_explained_var: 0.46435266733169556\n",
      "        vf_loss: 0.026847384870052338\n",
      "    load_time_ms: 2.198\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    sample_time_ms: 3833.949\n",
      "    update_time_ms: 10.088\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.96000000000001\n",
      "    ram_util_percent: 71.63999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.906587277985303\n",
      "    mean_inference_ms: 1.3663047574704787\n",
      "    mean_processing_ms: 0.9170512788007773\n",
      "  time_since_restore: 2007.739848613739\n",
      "  time_this_iter_s: 3.55025577545166\n",
      "  time_total_s: 2007.739848613739\n",
      "  timestamp: 1595950649\n",
      "  timesteps_since_restore: 1172000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 586\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2007 s, 586 iter, 1172000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.945248905851996\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 585\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.614\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1165517568588257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.422534352168441e-05\n",
      "        policy_loss: -0.00046919132000766695\n",
      "        total_loss: 0.00178627111017704\n",
      "        vf_explained_var: 0.8615450859069824\n",
      "        vf_loss: 0.0022554530296474695\n",
      "    load_time_ms: 2.186\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    sample_time_ms: 3850.64\n",
      "    update_time_ms: 10.871\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.04\n",
      "    ram_util_percent: 70.82000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.906587277985303\n",
      "    mean_inference_ms: 1.3663047574704787\n",
      "    mean_processing_ms: 0.9170512788007773\n",
      "  time_since_restore: 2014.8337721824646\n",
      "  time_this_iter_s: 3.5065197944641113\n",
      "  time_total_s: 2014.8337721824646\n",
      "  timestamp: 1595950657\n",
      "  timesteps_since_restore: 1176000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 588\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2014 s, 588 iter, 1176000 ts, 10.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.966213159991035\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 590\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.205\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1389423608779907\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7904758351505734e-05\n",
      "        policy_loss: -0.00016539765056222677\n",
      "        total_loss: 0.28331050276756287\n",
      "        vf_explained_var: 0.049533069133758545\n",
      "        vf_loss: 0.28347596526145935\n",
      "    load_time_ms: 2.446\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    sample_time_ms: 3924.929\n",
      "    update_time_ms: 10.46\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.55\n",
      "    ram_util_percent: 70.9875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.911449386458602\n",
      "    mean_inference_ms: 1.3675527612849339\n",
      "    mean_processing_ms: 0.9174230615737275\n",
      "  time_since_restore: 2024.0809366703033\n",
      "  time_this_iter_s: 5.31481146812439\n",
      "  time_total_s: 2024.0809366703033\n",
      "  timestamp: 1595950666\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 590\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2024 s, 590 iter, 1180000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.966213159991035\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 590\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.889\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0531001091003418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.711046752461698e-05\n",
      "        policy_loss: -9.474849503021687e-05\n",
      "        total_loss: 0.005269286222755909\n",
      "        vf_explained_var: 0.8170250058174133\n",
      "        vf_loss: 0.005364041309803724\n",
      "    load_time_ms: 2.65\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    sample_time_ms: 3841.898\n",
      "    update_time_ms: 9.783\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.46666666666667\n",
      "    ram_util_percent: 70.94999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9114493864586\n",
      "    mean_inference_ms: 1.3675527612849336\n",
      "    mean_processing_ms: 0.9174230615737272\n",
      "  time_since_restore: 2031.0412275791168\n",
      "  time_this_iter_s: 3.8509373664855957\n",
      "  time_total_s: 2031.0412275791168\n",
      "  timestamp: 1595950673\n",
      "  timesteps_since_restore: 1184000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 592\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2031 s, 592 iter, 1184000 ts, 11 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.966213159991035\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 590\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.415\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1580332517623901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.281883280375041e-05\n",
      "        policy_loss: -0.0009098167647607625\n",
      "        total_loss: 0.0005026512080803514\n",
      "        vf_explained_var: 0.9347376823425293\n",
      "        vf_loss: 0.0014124676818028092\n",
      "    load_time_ms: 2.449\n",
      "    num_steps_sampled: 1188000\n",
      "    num_steps_trained: 1188000\n",
      "    sample_time_ms: 3895.954\n",
      "    update_time_ms: 10.257\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.76\n",
      "    ram_util_percent: 70.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9114493864586\n",
      "    mean_inference_ms: 1.3675527612849336\n",
      "    mean_processing_ms: 0.9174230615737272\n",
      "  time_since_restore: 2038.9841153621674\n",
      "  time_this_iter_s: 3.8050994873046875\n",
      "  time_total_s: 2038.9841153621674\n",
      "  timestamp: 1595950681\n",
      "  timesteps_since_restore: 1188000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1188000\n",
      "  training_iteration: 594\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2038 s, 594 iter, 1188000 ts, 11 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.97209543227143\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 595\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.216\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1667934656143188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.46257808007067e-06\n",
      "        policy_loss: 8.89606453711167e-05\n",
      "        total_loss: 0.2803705334663391\n",
      "        vf_explained_var: 0.043133318424224854\n",
      "        vf_loss: 0.28028157353401184\n",
      "    load_time_ms: 2.721\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    sample_time_ms: 3943.601\n",
      "    update_time_ms: 10.374\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.3875\n",
      "    ram_util_percent: 70.8875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.916566030083308\n",
      "    mean_inference_ms: 1.3688345566504803\n",
      "    mean_processing_ms: 0.9178126480454298\n",
      "  time_since_restore: 2044.4729969501495\n",
      "  time_this_iter_s: 5.488881587982178\n",
      "  time_total_s: 2044.4729969501495\n",
      "  timestamp: 1595950686\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 595\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2044 s, 595 iter, 1190000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.972095432271432\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 595\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 56.309\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0109258890151978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00014774102601222694\n",
      "        policy_loss: -0.0006604309310205281\n",
      "        total_loss: 0.006496700458228588\n",
      "        vf_explained_var: 0.6828920841217041\n",
      "        vf_loss: 0.007157123647630215\n",
      "    load_time_ms: 2.829\n",
      "    num_steps_sampled: 1194000\n",
      "    num_steps_trained: 1194000\n",
      "    sample_time_ms: 4096.569\n",
      "    update_time_ms: 9.679\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.17142857142856\n",
      "    ram_util_percent: 70.89999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.916566030083308\n",
      "    mean_inference_ms: 1.3688345566504805\n",
      "    mean_processing_ms: 0.9178126480454302\n",
      "  time_since_restore: 2053.1869688034058\n",
      "  time_this_iter_s: 4.487339019775391\n",
      "  time_total_s: 2053.1869688034058\n",
      "  timestamp: 1595950695\n",
      "  timesteps_since_restore: 1194000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1194000\n",
      "  training_iteration: 597\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2053 s, 597 iter, 1194000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 10.972095432271432\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 595\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 60.495\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1321759223937988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8324871891527437e-05\n",
      "        policy_loss: -0.00045953941298648715\n",
      "        total_loss: 0.0010006427764892578\n",
      "        vf_explained_var: 0.919106662273407\n",
      "        vf_loss: 0.0014601972652599216\n",
      "    load_time_ms: 2.938\n",
      "    num_steps_sampled: 1198000\n",
      "    num_steps_trained: 1198000\n",
      "    sample_time_ms: 4231.04\n",
      "    update_time_ms: 9.911\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.4\n",
      "    ram_util_percent: 70.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.916566030083308\n",
      "    mean_inference_ms: 1.3688345566504805\n",
      "    mean_processing_ms: 0.9178126480454302\n",
      "  time_since_restore: 2062.0007588863373\n",
      "  time_this_iter_s: 4.066355228424072\n",
      "  time_total_s: 2062.0007588863373\n",
      "  timestamp: 1595950704\n",
      "  timesteps_since_restore: 1198000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1198000\n",
      "  training_iteration: 599\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2062 s, 599 iter, 1198000 ts, 11 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.002274397627849\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 600\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 58.895\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9005972743034363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.409898484591395e-05\n",
      "        policy_loss: -0.0009040806326083839\n",
      "        total_loss: 0.025993317365646362\n",
      "        vf_explained_var: 0.48131054639816284\n",
      "        vf_loss: 0.02689739316701889\n",
      "    load_time_ms: 2.537\n",
      "    num_steps_sampled: 1202000\n",
      "    num_steps_trained: 1202000\n",
      "    sample_time_ms: 4230.752\n",
      "    update_time_ms: 9.258\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.75999999999999\n",
      "    ram_util_percent: 71.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9223239795535525\n",
      "    mean_inference_ms: 1.3702450286348418\n",
      "    mean_processing_ms: 0.9182031437392741\n",
      "  time_since_restore: 2070.390641927719\n",
      "  time_this_iter_s: 3.6066854000091553\n",
      "  time_total_s: 2070.390641927719\n",
      "  timestamp: 1595950712\n",
      "  timesteps_since_restore: 1202000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1202000\n",
      "  training_iteration: 601\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2070 s, 601 iter, 1202000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.002274397627849\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 600\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.466\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1030354499816895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2068450132574071e-06\n",
      "        policy_loss: 1.3691902495338582e-05\n",
      "        total_loss: 0.0020419759675860405\n",
      "        vf_explained_var: 0.8618720769882202\n",
      "        vf_loss: 0.002028277376666665\n",
      "    load_time_ms: 2.082\n",
      "    num_steps_sampled: 1206000\n",
      "    num_steps_trained: 1206000\n",
      "    sample_time_ms: 4274.712\n",
      "    update_time_ms: 11.328\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.85000000000001\n",
      "    ram_util_percent: 71.05\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9223239795535525\n",
      "    mean_inference_ms: 1.3702450286348418\n",
      "    mean_processing_ms: 0.9182031437392741\n",
      "  time_since_restore: 2078.7727177143097\n",
      "  time_this_iter_s: 4.293766021728516\n",
      "  time_total_s: 2078.7727177143097\n",
      "  timestamp: 1595950721\n",
      "  timesteps_since_restore: 1206000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1206000\n",
      "  training_iteration: 603\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2078 s, 603 iter, 1206000 ts, 11 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.008221139743242\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 605\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.667\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1408342123031616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1597832528641447e-05\n",
      "        policy_loss: -1.853323010436725e-05\n",
      "        total_loss: 0.287425696849823\n",
      "        vf_explained_var: 0.0561712384223938\n",
      "        vf_loss: 0.28744420409202576\n",
      "    load_time_ms: 1.885\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    sample_time_ms: 4212.557\n",
      "    update_time_ms: 11.56\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.11428571428571\n",
      "    ram_util_percent: 71.01428571428572\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9285191610700005\n",
      "    mean_inference_ms: 1.3717313462092022\n",
      "    mean_processing_ms: 0.9186093490077764\n",
      "  time_since_restore: 2087.3985946178436\n",
      "  time_this_iter_s: 4.619192838668823\n",
      "  time_total_s: 2087.3985946178436\n",
      "  timestamp: 1595950729\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 605\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2087 s, 605 iter, 1210000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.008221139743242\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 605\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.901\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0338517427444458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.683910032734275e-05\n",
      "        policy_loss: 9.717273496789858e-05\n",
      "        total_loss: 0.005638299975544214\n",
      "        vf_explained_var: 0.8250818252563477\n",
      "        vf_loss: 0.005541146732866764\n",
      "    load_time_ms: 1.705\n",
      "    num_steps_sampled: 1214000\n",
      "    num_steps_trained: 1214000\n",
      "    sample_time_ms: 4004.843\n",
      "    update_time_ms: 11.347\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.97999999999999\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9285191610700005\n",
      "    mean_inference_ms: 1.371731346209202\n",
      "    mean_processing_ms: 0.9186093490077762\n",
      "  time_since_restore: 2093.935092687607\n",
      "  time_this_iter_s: 3.3940062522888184\n",
      "  time_total_s: 2093.935092687607\n",
      "  timestamp: 1595950736\n",
      "  timesteps_since_restore: 1214000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1214000\n",
      "  training_iteration: 607\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2093 s, 607 iter, 1214000 ts, 11 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.008221139743242\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 605\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.344\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1054939031600952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.216192559804767e-05\n",
      "        policy_loss: -0.0012294092448428273\n",
      "        total_loss: 0.0009706182754598558\n",
      "        vf_explained_var: 0.8902115821838379\n",
      "        vf_loss: 0.002200038405135274\n",
      "    load_time_ms: 1.597\n",
      "    num_steps_sampled: 1218000\n",
      "    num_steps_trained: 1218000\n",
      "    sample_time_ms: 3757.145\n",
      "    update_time_ms: 11.257\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.75\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9285191610700005\n",
      "    mean_inference_ms: 1.371731346209202\n",
      "    mean_processing_ms: 0.9186093490077762\n",
      "  time_since_restore: 2100.2298069000244\n",
      "  time_this_iter_s: 3.1323940753936768\n",
      "  time_total_s: 2100.2298069000244\n",
      "  timestamp: 1595950742\n",
      "  timesteps_since_restore: 1218000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1218000\n",
      "  training_iteration: 609\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2100 s, 609 iter, 1218000 ts, 11 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.029292446572537\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 610\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.701\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9078266024589539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.0135560993803665e-05\n",
      "        policy_loss: -0.00027454947121441364\n",
      "        total_loss: 0.026028351858258247\n",
      "        vf_explained_var: 0.5193330645561218\n",
      "        vf_loss: 0.026302892714738846\n",
      "    load_time_ms: 1.592\n",
      "    num_steps_sampled: 1222000\n",
      "    num_steps_trained: 1222000\n",
      "    sample_time_ms: 3724.352\n",
      "    update_time_ms: 11.108\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.26666666666667\n",
      "    ram_util_percent: 70.98333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.934627295370551\n",
      "    mean_inference_ms: 1.3731975848032039\n",
      "    mean_processing_ms: 0.9190060484665008\n",
      "  time_since_restore: 2108.272795200348\n",
      "  time_this_iter_s: 3.6580586433410645\n",
      "  time_total_s: 2108.272795200348\n",
      "  timestamp: 1595950750\n",
      "  timesteps_since_restore: 1222000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1222000\n",
      "  training_iteration: 611\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2108 s, 611 iter, 1222000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.029292446572537\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 610\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.035\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.106661319732666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4681964785268065e-05\n",
      "        policy_loss: -0.0001734218531055376\n",
      "        total_loss: 0.002015081001445651\n",
      "        vf_explained_var: 0.8793838024139404\n",
      "        vf_loss: 0.00218850071541965\n",
      "    load_time_ms: 1.616\n",
      "    num_steps_sampled: 1226000\n",
      "    num_steps_trained: 1226000\n",
      "    sample_time_ms: 3545.761\n",
      "    update_time_ms: 8.354\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.34\n",
      "    ram_util_percent: 70.97999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.934627295370551\n",
      "    mean_inference_ms: 1.3731975848032039\n",
      "    mean_processing_ms: 0.9190060484665008\n",
      "  time_since_restore: 2114.824642896652\n",
      "  time_this_iter_s: 3.3577351570129395\n",
      "  time_total_s: 2114.824642896652\n",
      "  timestamp: 1595950757\n",
      "  timesteps_since_restore: 1226000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1226000\n",
      "  training_iteration: 613\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2114 s, 613 iter, 1226000 ts, 11 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.025822996502802\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 615\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.212\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1318984031677246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.689462027978152e-05\n",
      "        policy_loss: -0.00020759964536409825\n",
      "        total_loss: 0.30113649368286133\n",
      "        vf_explained_var: -0.0007637739181518555\n",
      "        vf_loss: 0.3013441562652588\n",
      "    load_time_ms: 2.206\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    sample_time_ms: 3785.916\n",
      "    update_time_ms: 8.067\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.26\n",
      "    ram_util_percent: 71.33\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.941194895617279\n",
      "    mean_inference_ms: 1.3747651179810219\n",
      "    mean_processing_ms: 0.9194136048677182\n",
      "  time_since_restore: 2125.9410977363586\n",
      "  time_this_iter_s: 6.8713905811309814\n",
      "  time_total_s: 2125.9410977363586\n",
      "  timestamp: 1595950768\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 615\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2125 s, 615 iter, 1230000 ts, 11 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.025822996502804\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 615\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.272\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0386401414871216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.808487473288551e-05\n",
      "        policy_loss: -0.0006604738300666213\n",
      "        total_loss: 0.00473405234515667\n",
      "        vf_explained_var: 0.7253221869468689\n",
      "        vf_loss: 0.005394541192799807\n",
      "    load_time_ms: 2.182\n",
      "    num_steps_sampled: 1234000\n",
      "    num_steps_trained: 1234000\n",
      "    sample_time_ms: 3938.617\n",
      "    update_time_ms: 8.851\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.82000000000001\n",
      "    ram_util_percent: 71.63999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.941194895617281\n",
      "    mean_inference_ms: 1.3747651179810223\n",
      "    mean_processing_ms: 0.9194136048677182\n",
      "  time_since_restore: 2134.0255007743835\n",
      "  time_this_iter_s: 3.857419967651367\n",
      "  time_total_s: 2134.0255007743835\n",
      "  timestamp: 1595950776\n",
      "  timesteps_since_restore: 1234000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1234000\n",
      "  training_iteration: 617\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2134 s, 617 iter, 1234000 ts, 11 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.025822996502804\n",
      "  episode_reward_min: 9.263280905548632\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 615\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.623\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1266452074050903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2949764823133592e-05\n",
      "        policy_loss: -0.00015457821427844465\n",
      "        total_loss: 0.0016130352159962058\n",
      "        vf_explained_var: 0.906836211681366\n",
      "        vf_loss: 0.0017676153220236301\n",
      "    load_time_ms: 2.191\n",
      "    num_steps_sampled: 1238000\n",
      "    num_steps_trained: 1238000\n",
      "    sample_time_ms: 4044.088\n",
      "    update_time_ms: 8.197\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.4\n",
      "    ram_util_percent: 71.45\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.941194895617281\n",
      "    mean_inference_ms: 1.3747651179810223\n",
      "    mean_processing_ms: 0.9194136048677182\n",
      "  time_since_restore: 2141.3743286132812\n",
      "  time_this_iter_s: 4.033599615097046\n",
      "  time_total_s: 2141.3743286132812\n",
      "  timestamp: 1595950784\n",
      "  timesteps_since_restore: 1238000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1238000\n",
      "  training_iteration: 619\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2141 s, 619 iter, 1238000 ts, 11 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.071077894450582\n",
      "  episode_reward_min: 9.948785406980893\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 620\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.013\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9063055515289307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.9949714846443385e-05\n",
      "        policy_loss: -0.0006701183156110346\n",
      "        total_loss: 0.024471741169691086\n",
      "        vf_explained_var: 0.474668025970459\n",
      "        vf_loss: 0.025141870602965355\n",
      "    load_time_ms: 2.129\n",
      "    num_steps_sampled: 1242000\n",
      "    num_steps_trained: 1242000\n",
      "    sample_time_ms: 4151.553\n",
      "    update_time_ms: 8.196\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.28571428571429\n",
      "    ram_util_percent: 71.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.947950593697731\n",
      "    mean_inference_ms: 1.3763519110947937\n",
      "    mean_processing_ms: 0.9198150955509012\n",
      "  time_since_restore: 2150.4741723537445\n",
      "  time_this_iter_s: 4.66949987411499\n",
      "  time_total_s: 2150.4741723537445\n",
      "  timestamp: 1595950793\n",
      "  timesteps_since_restore: 1242000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1242000\n",
      "  training_iteration: 621\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2150 s, 621 iter, 1242000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.071077894450582\n",
      "  episode_reward_min: 9.948785406980893\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 620\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.623\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.114648699760437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.444965533892173e-07\n",
      "        policy_loss: -9.773254532774445e-06\n",
      "        total_loss: 0.0018651867285370827\n",
      "        vf_explained_var: 0.8555026650428772\n",
      "        vf_loss: 0.00187497038859874\n",
      "    load_time_ms: 2.094\n",
      "    num_steps_sampled: 1246000\n",
      "    num_steps_trained: 1246000\n",
      "    sample_time_ms: 4222.655\n",
      "    update_time_ms: 8.716\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.06\n",
      "    ram_util_percent: 71.12\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.947950593697731\n",
      "    mean_inference_ms: 1.3763519110947937\n",
      "    mean_processing_ms: 0.9198150955509012\n",
      "  time_since_restore: 2157.727871656418\n",
      "  time_this_iter_s: 4.0124945640563965\n",
      "  time_total_s: 2157.727871656418\n",
      "  timestamp: 1595950800\n",
      "  timesteps_since_restore: 1246000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1246000\n",
      "  training_iteration: 623\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2157 s, 623 iter, 1246000 ts, 11.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.071077894450582\n",
      "  episode_reward_min: 9.948785406980893\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 620\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.451\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1073118448257446\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.596587935637217e-05\n",
      "        policy_loss: -0.0005080108530819416\n",
      "        total_loss: 0.001596189453266561\n",
      "        vf_explained_var: 0.8822010159492493\n",
      "        vf_loss: 0.0021041971631348133\n",
      "    load_time_ms: 1.729\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    sample_time_ms: 4302.559\n",
      "    update_time_ms: 8.671\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.7125\n",
      "    ram_util_percent: 71.08749999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.947950593697731\n",
      "    mean_inference_ms: 1.3763519110947937\n",
      "    mean_processing_ms: 0.9198150955509012\n",
      "  time_since_restore: 2162.753559112549\n",
      "  time_this_iter_s: 5.0256874561309814\n",
      "  time_total_s: 2162.753559112549\n",
      "  timestamp: 1595950805\n",
      "  timesteps_since_restore: 1248000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 624\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2162 s, 624 iter, 1248000 ts, 11.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.099145038332193\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 625\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.19\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1282309293746948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.322494406660553e-06\n",
      "        policy_loss: -5.182933819014579e-05\n",
      "        total_loss: 0.30324214696884155\n",
      "        vf_explained_var: 0.030226171016693115\n",
      "        vf_loss: 0.30329403281211853\n",
      "    load_time_ms: 1.675\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    sample_time_ms: 4170.015\n",
      "    update_time_ms: 8.858\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.92857142857143\n",
      "    ram_util_percent: 71.07142857142857\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9552492620287305\n",
      "    mean_inference_ms: 1.3780666428241937\n",
      "    mean_processing_ms: 0.9202340623375745\n",
      "  time_since_restore: 2168.261373758316\n",
      "  time_this_iter_s: 5.507814645767212\n",
      "  time_total_s: 2168.261373758316\n",
      "  timestamp: 1595950811\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 625\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2168 s, 625 iter, 1250000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.099145038332187\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 625\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.738\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0451855659484863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011688497761497274\n",
      "        policy_loss: -0.0005492219934239984\n",
      "        total_loss: 0.0039261505007743835\n",
      "        vf_explained_var: 0.802892804145813\n",
      "        vf_loss: 0.004475374240428209\n",
      "    load_time_ms: 1.767\n",
      "    num_steps_sampled: 1254000\n",
      "    num_steps_trained: 1254000\n",
      "    sample_time_ms: 4153.088\n",
      "    update_time_ms: 7.463\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.95\n",
      "    ram_util_percent: 71.14999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9552492620287305\n",
      "    mean_inference_ms: 1.3780666428241937\n",
      "    mean_processing_ms: 0.9202340623375744\n",
      "  time_since_restore: 2176.1521821022034\n",
      "  time_this_iter_s: 4.203381299972534\n",
      "  time_total_s: 2176.1521821022034\n",
      "  timestamp: 1595950819\n",
      "  timesteps_since_restore: 1254000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1254000\n",
      "  training_iteration: 627\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2176 s, 627 iter, 1254000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.099145038332187\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 625\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.586\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.135693073272705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.16476939083077e-05\n",
      "        policy_loss: -0.0010381498141214252\n",
      "        total_loss: 0.00022994041501078755\n",
      "        vf_explained_var: 0.8587355613708496\n",
      "        vf_loss: 0.0012680874206125736\n",
      "    load_time_ms: 1.764\n",
      "    num_steps_sampled: 1258000\n",
      "    num_steps_trained: 1258000\n",
      "    sample_time_ms: 4225.612\n",
      "    update_time_ms: 7.458\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.4\n",
      "    ram_util_percent: 71.11999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9552492620287305\n",
      "    mean_inference_ms: 1.3780666428241937\n",
      "    mean_processing_ms: 0.9202340623375744\n",
      "  time_since_restore: 2184.2222216129303\n",
      "  time_this_iter_s: 3.706047534942627\n",
      "  time_total_s: 2184.2222216129303\n",
      "  timestamp: 1595950827\n",
      "  timesteps_since_restore: 1258000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1258000\n",
      "  training_iteration: 629\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2184 s, 629 iter, 1258000 ts, 11.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.105577531918273\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 630\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.904\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.132610559463501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.088818215881474e-05\n",
      "        policy_loss: -0.0004228620673529804\n",
      "        total_loss: 0.3033719062805176\n",
      "        vf_explained_var: 0.01629197597503662\n",
      "        vf_loss: 0.3037947416305542\n",
      "    load_time_ms: 1.882\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    sample_time_ms: 4303.334\n",
      "    update_time_ms: 7.071\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.3125\n",
      "    ram_util_percent: 71.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.962769874325866\n",
      "    mean_inference_ms: 1.3797969563600927\n",
      "    mean_processing_ms: 0.9206422703222605\n",
      "  time_since_restore: 2189.49551486969\n",
      "  time_this_iter_s: 5.2732932567596436\n",
      "  time_total_s: 2189.49551486969\n",
      "  timestamp: 1595950832\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 630\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2189 s, 630 iter, 1260000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.105577531918273\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 630\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.798\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0244152545928955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.751928362471517e-06\n",
      "        policy_loss: -2.4128437871695496e-05\n",
      "        total_loss: 0.005704260431230068\n",
      "        vf_explained_var: 0.8146941661834717\n",
      "        vf_loss: 0.005728388670831919\n",
      "    load_time_ms: 1.932\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1264000\n",
      "    sample_time_ms: 4239.103\n",
      "    update_time_ms: 9.397\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.325\n",
      "    ram_util_percent: 71.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.962769874325865\n",
      "    mean_inference_ms: 1.3797969563600927\n",
      "    mean_processing_ms: 0.9206422703222604\n",
      "  time_since_restore: 2196.8093819618225\n",
      "  time_this_iter_s: 3.258065938949585\n",
      "  time_total_s: 2196.8093819618225\n",
      "  timestamp: 1595950839\n",
      "  timesteps_since_restore: 1264000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 632\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2196 s, 632 iter, 1264000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.105577531918273\n",
      "  episode_reward_min: 10.230429859163488\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 630\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.633\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1223582029342651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.027599844557699e-06\n",
      "        policy_loss: -0.00038491154555231333\n",
      "        total_loss: 0.0012893066741526127\n",
      "        vf_explained_var: 0.9068050384521484\n",
      "        vf_loss: 0.0016742425505071878\n",
      "    load_time_ms: 1.698\n",
      "    num_steps_sampled: 1268000\n",
      "    num_steps_trained: 1268000\n",
      "    sample_time_ms: 4059.089\n",
      "    update_time_ms: 9.574\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.88000000000001\n",
      "    ram_util_percent: 71.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.962769874325865\n",
      "    mean_inference_ms: 1.3797969563600927\n",
      "    mean_processing_ms: 0.9206422703222604\n",
      "  time_since_restore: 2204.0014810562134\n",
      "  time_this_iter_s: 3.40952205657959\n",
      "  time_total_s: 2204.0014810562134\n",
      "  timestamp: 1595950847\n",
      "  timesteps_since_restore: 1268000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1268000\n",
      "  training_iteration: 634\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2204 s, 634 iter, 1268000 ts, 11.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.132452844927425\n",
      "  episode_reward_min: 10.261786008758232\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 635\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.971\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1313092708587646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6087403966812417e-05\n",
      "        policy_loss: -0.0001684474991634488\n",
      "        total_loss: 0.3084542751312256\n",
      "        vf_explained_var: 0.001323699951171875\n",
      "        vf_loss: 0.30862268805503845\n",
      "    load_time_ms: 2.15\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    sample_time_ms: 4018.557\n",
      "    update_time_ms: 9.216\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4857142857143\n",
      "    ram_util_percent: 71.04285714285713\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.970409172840693\n",
      "    mean_inference_ms: 1.3815471882340151\n",
      "    mean_processing_ms: 0.9210515897088625\n",
      "  time_since_restore: 2209.1514961719513\n",
      "  time_this_iter_s: 5.150015115737915\n",
      "  time_total_s: 2209.1514961719513\n",
      "  timestamp: 1595950852\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 635\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2209 s, 635 iter, 1270000 ts, 11.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.132452844927426\n",
      "  episode_reward_min: 10.261786008758232\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 635\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.598\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0131961107254028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.177843559067696e-05\n",
      "        policy_loss: -9.477186540607363e-05\n",
      "        total_loss: 0.00568698113784194\n",
      "        vf_explained_var: 0.7410953044891357\n",
      "        vf_loss: 0.0057817683555185795\n",
      "    load_time_ms: 2.086\n",
      "    num_steps_sampled: 1274000\n",
      "    num_steps_trained: 1274000\n",
      "    sample_time_ms: 4083.386\n",
      "    update_time_ms: 10.664\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16666666666667\n",
      "    ram_util_percent: 71.03333333333335\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.970409172840695\n",
      "    mean_inference_ms: 1.3815471882340145\n",
      "    mean_processing_ms: 0.9210515897088625\n",
      "  time_since_restore: 2217.7502253055573\n",
      "  time_this_iter_s: 4.087721109390259\n",
      "  time_total_s: 2217.7502253055573\n",
      "  timestamp: 1595950860\n",
      "  timesteps_since_restore: 1274000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1274000\n",
      "  training_iteration: 637\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2217 s, 637 iter, 1274000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.132452844927426\n",
      "  episode_reward_min: 10.261786008758232\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 635\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0798081159591675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3541281305151642e-06\n",
      "        policy_loss: -5.447864532470703e-05\n",
      "        total_loss: 0.0026392927393317223\n",
      "        vf_explained_var: 0.8346327543258667\n",
      "        vf_loss: 0.0026937630027532578\n",
      "    load_time_ms: 2.093\n",
      "    num_steps_sampled: 1276000\n",
      "    num_steps_trained: 1276000\n",
      "    sample_time_ms: 4150.441\n",
      "    update_time_ms: 11.645\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.4125\n",
      "    ram_util_percent: 71.15\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.970409172840695\n",
      "    mean_inference_ms: 1.3815471882340145\n",
      "    mean_processing_ms: 0.9210515897088625\n",
      "  time_since_restore: 2222.8176050186157\n",
      "  time_this_iter_s: 5.067379713058472\n",
      "  time_total_s: 2222.8176050186157\n",
      "  timestamp: 1595950865\n",
      "  timesteps_since_restore: 1276000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1276000\n",
      "  training_iteration: 638\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2222 s, 638 iter, 1276000 ts, 11.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161403739429074\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 640\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.803\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1292548179626465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.28093288873788e-06\n",
      "        policy_loss: -5.540537677006796e-05\n",
      "        total_loss: 0.3052240014076233\n",
      "        vf_explained_var: 0.058776915073394775\n",
      "        vf_loss: 0.3052794635295868\n",
      "    load_time_ms: 2.092\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    sample_time_ms: 4218.036\n",
      "    update_time_ms: 13.311\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.475\n",
      "    ram_util_percent: 71.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.978666146501399\n",
      "    mean_inference_ms: 1.3834198141928662\n",
      "    mean_processing_ms: 0.9215082904182342\n",
      "  time_since_restore: 2232.4874515533447\n",
      "  time_this_iter_s: 5.7232747077941895\n",
      "  time_total_s: 2232.4874515533447\n",
      "  timestamp: 1595950875\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 640\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2232 s, 640 iter, 1280000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161403739429076\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 640\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.798\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0102126598358154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.6032298743957654e-05\n",
      "        policy_loss: -0.0001527996064396575\n",
      "        total_loss: 0.005608566105365753\n",
      "        vf_explained_var: 0.7010234594345093\n",
      "        vf_loss: 0.005761364474892616\n",
      "    load_time_ms: 2.062\n",
      "    num_steps_sampled: 1284000\n",
      "    num_steps_trained: 1284000\n",
      "    sample_time_ms: 4298.321\n",
      "    update_time_ms: 11.556\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.86666666666666\n",
      "    ram_util_percent: 71.05000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.978666146501399\n",
      "    mean_inference_ms: 1.3834198141928664\n",
      "    mean_processing_ms: 0.9215082904182346\n",
      "  time_since_restore: 2240.5739569664\n",
      "  time_this_iter_s: 4.015550851821899\n",
      "  time_total_s: 2240.5739569664\n",
      "  timestamp: 1595950883\n",
      "  timesteps_since_restore: 1284000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1284000\n",
      "  training_iteration: 642\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2240 s, 642 iter, 1284000 ts, 11.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161403739429076\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 640\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.945\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1192197799682617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.746356429066509e-05\n",
      "        policy_loss: -0.0007401139591820538\n",
      "        total_loss: 0.0008207604987546802\n",
      "        vf_explained_var: 0.8992769718170166\n",
      "        vf_loss: 0.001560875796712935\n",
      "    load_time_ms: 2.182\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1288000\n",
      "    sample_time_ms: 4414.837\n",
      "    update_time_ms: 12.209\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.01666666666667\n",
      "    ram_util_percent: 71.10000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.978666146501399\n",
      "    mean_inference_ms: 1.3834198141928664\n",
      "    mean_processing_ms: 0.9215082904182346\n",
      "  time_since_restore: 2248.9698333740234\n",
      "  time_this_iter_s: 3.941486358642578\n",
      "  time_total_s: 2248.9698333740234\n",
      "  timestamp: 1595950892\n",
      "  timesteps_since_restore: 1288000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 644\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2248 s, 644 iter, 1288000 ts, 11.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161551177587267\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 645\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.416\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1105083227157593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.808040335366968e-05\n",
      "        policy_loss: -0.00017717742593958974\n",
      "        total_loss: 0.3174086809158325\n",
      "        vf_explained_var: -0.014466643333435059\n",
      "        vf_loss: 0.31758588552474976\n",
      "    load_time_ms: 1.784\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    sample_time_ms: 4450.746\n",
      "    update_time_ms: 12.869\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1125\n",
      "    ram_util_percent: 71.07499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.9872811049755175\n",
      "    mean_inference_ms: 1.3853476503768627\n",
      "    mean_processing_ms: 0.9219963850963745\n",
      "  time_since_restore: 2254.4552783966064\n",
      "  time_this_iter_s: 5.485445022583008\n",
      "  time_total_s: 2254.4552783966064\n",
      "  timestamp: 1595950897\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 645\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2254 s, 645 iter, 1290000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161551177587263\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 645\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.963\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.010176181793213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.054716191603802e-05\n",
      "        policy_loss: -9.222030348610133e-05\n",
      "        total_loss: 0.005621983204036951\n",
      "        vf_explained_var: 0.7448244094848633\n",
      "        vf_loss: 0.005714209750294685\n",
      "    load_time_ms: 2.049\n",
      "    num_steps_sampled: 1294000\n",
      "    num_steps_trained: 1294000\n",
      "    sample_time_ms: 4450.865\n",
      "    update_time_ms: 12.371\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.80000000000001\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.987281104975516\n",
      "    mean_inference_ms: 1.3853476503768627\n",
      "    mean_processing_ms: 0.9219963850963745\n",
      "  time_since_restore: 2263.062352657318\n",
      "  time_this_iter_s: 4.189446210861206\n",
      "  time_total_s: 2263.062352657318\n",
      "  timestamp: 1595950906\n",
      "  timesteps_since_restore: 1294000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1294000\n",
      "  training_iteration: 647\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2263 s, 647 iter, 1294000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 11.980456165225856\n",
      "  episode_reward_mean: 11.161551177587263\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 645\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.376\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0934799909591675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1673193007009104e-05\n",
      "        policy_loss: -0.0008092146017588675\n",
      "        total_loss: 0.0013457807945087552\n",
      "        vf_explained_var: 0.9001034498214722\n",
      "        vf_loss: 0.0021550001110881567\n",
      "    load_time_ms: 2.147\n",
      "    num_steps_sampled: 1298000\n",
      "    num_steps_trained: 1298000\n",
      "    sample_time_ms: 4417.889\n",
      "    update_time_ms: 12.614\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.33333333333333\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.987281104975516\n",
      "    mean_inference_ms: 1.3853476503768627\n",
      "    mean_processing_ms: 0.9219963850963745\n",
      "  time_since_restore: 2271.7648820877075\n",
      "  time_this_iter_s: 4.116649627685547\n",
      "  time_total_s: 2271.7648820877075\n",
      "  timestamp: 1595950915\n",
      "  timesteps_since_restore: 1298000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1298000\n",
      "  training_iteration: 649\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2271 s, 649 iter, 1298000 ts, 11.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.181458543273173\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 650\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.51\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.111682415008545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4434129298024345e-05\n",
      "        policy_loss: 5.790138311567716e-05\n",
      "        total_loss: 0.3173736035823822\n",
      "        vf_explained_var: 0.008004367351531982\n",
      "        vf_loss: 0.31731563806533813\n",
      "    load_time_ms: 2.159\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    sample_time_ms: 4430.893\n",
      "    update_time_ms: 11.75\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.7875\n",
      "    ram_util_percent: 71.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.996393009897153\n",
      "    mean_inference_ms: 1.387375749292215\n",
      "    mean_processing_ms: 0.9225239759619491\n",
      "  time_since_restore: 2277.6188604831696\n",
      "  time_this_iter_s: 5.853978395462036\n",
      "  time_total_s: 2277.6188604831696\n",
      "  timestamp: 1595950921\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 650\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2277 s, 650 iter, 1300000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.181458543273177\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 650\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.373\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0118093490600586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.79869917803444e-05\n",
      "        policy_loss: -0.0005411768215708435\n",
      "        total_loss: 0.004459185525774956\n",
      "        vf_explained_var: 0.7975971102714539\n",
      "        vf_loss: 0.005000367294996977\n",
      "    load_time_ms: 2.167\n",
      "    num_steps_sampled: 1304000\n",
      "    num_steps_trained: 1304000\n",
      "    sample_time_ms: 4280.699\n",
      "    update_time_ms: 12.176\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.625\n",
      "    ram_util_percent: 71.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.996393009897154\n",
      "    mean_inference_ms: 1.3873757492922152\n",
      "    mean_processing_ms: 0.9225239759619491\n",
      "  time_since_restore: 2284.229017019272\n",
      "  time_this_iter_s: 3.415226459503174\n",
      "  time_total_s: 2284.229017019272\n",
      "  timestamp: 1595950927\n",
      "  timesteps_since_restore: 1304000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1304000\n",
      "  training_iteration: 652\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2284 s, 652 iter, 1304000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.181458543273177\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 650\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.447\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1034349203109741\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.992820767161902e-06\n",
      "        policy_loss: -0.00040015412378124893\n",
      "        total_loss: 0.001272910158149898\n",
      "        vf_explained_var: 0.8493687510490417\n",
      "        vf_loss: 0.001673064660280943\n",
      "    load_time_ms: 2.087\n",
      "    num_steps_sampled: 1308000\n",
      "    num_steps_trained: 1308000\n",
      "    sample_time_ms: 4267.221\n",
      "    update_time_ms: 13.953\n",
      "  iterations_since_restore: 654\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.80000000000001\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.996393009897154\n",
      "    mean_inference_ms: 1.3873757492922152\n",
      "    mean_processing_ms: 0.9225239759619491\n",
      "  time_since_restore: 2292.467775583267\n",
      "  time_this_iter_s: 4.057788133621216\n",
      "  time_total_s: 2292.467775583267\n",
      "  timestamp: 1595950935\n",
      "  timesteps_since_restore: 1308000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1308000\n",
      "  training_iteration: 654\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2292 s, 654 iter, 1308000 ts, 11.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.196483602947094\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 655\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.528\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0751194953918457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7198950445163064e-05\n",
      "        policy_loss: -0.00024068022321444005\n",
      "        total_loss: 0.32102662324905396\n",
      "        vf_explained_var: 0.030593454837799072\n",
      "        vf_loss: 0.32126733660697937\n",
      "    load_time_ms: 2.274\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    sample_time_ms: 4244.117\n",
      "    update_time_ms: 13.681\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.55\n",
      "    ram_util_percent: 70.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.005609866213502\n",
      "    mean_inference_ms: 1.3894038697932811\n",
      "    mean_processing_ms: 0.9230594915755974\n",
      "  time_since_restore: 2297.7526438236237\n",
      "  time_this_iter_s: 5.284868240356445\n",
      "  time_total_s: 2297.7526438236237\n",
      "  timestamp: 1595950941\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 655\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2297 s, 655 iter, 1310000 ts, 11.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.196483602947092\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 655\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 53.468\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9866384267807007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001584009878570214\n",
      "        policy_loss: -0.00056317332200706\n",
      "        total_loss: 0.004776307847350836\n",
      "        vf_explained_var: 0.7894218564033508\n",
      "        vf_loss: 0.005339473951607943\n",
      "    load_time_ms: 2.152\n",
      "    num_steps_sampled: 1314000\n",
      "    num_steps_trained: 1314000\n",
      "    sample_time_ms: 4263.128\n",
      "    update_time_ms: 16.181\n",
      "  iterations_since_restore: 657\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.57142857142856\n",
      "    ram_util_percent: 71.08571428571429\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.005609866213501\n",
      "    mean_inference_ms: 1.389403869793281\n",
      "    mean_processing_ms: 0.9230594915755972\n",
      "  time_since_restore: 2306.621119260788\n",
      "  time_this_iter_s: 4.811047792434692\n",
      "  time_total_s: 2306.621119260788\n",
      "  timestamp: 1595950950\n",
      "  timesteps_since_restore: 1314000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1314000\n",
      "  training_iteration: 657\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2306 s, 657 iter, 1314000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.196483602947092\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 655\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.48\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0867196321487427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.899805647553876e-05\n",
      "        policy_loss: -0.000754432228859514\n",
      "        total_loss: 0.0008410200825892389\n",
      "        vf_explained_var: 0.9184755086898804\n",
      "        vf_loss: 0.0015954718692228198\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 1318000\n",
      "    num_steps_trained: 1318000\n",
      "    sample_time_ms: 4260.495\n",
      "    update_time_ms: 14.856\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.56666666666666\n",
      "    ram_util_percent: 71.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.005609866213501\n",
      "    mean_inference_ms: 1.389403869793281\n",
      "    mean_processing_ms: 0.9230594915755972\n",
      "  time_since_restore: 2315.229991674423\n",
      "  time_this_iter_s: 4.111375331878662\n",
      "  time_total_s: 2315.229991674423\n",
      "  timestamp: 1595950958\n",
      "  timesteps_since_restore: 1318000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1318000\n",
      "  training_iteration: 659\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2315 s, 659 iter, 1318000 ts, 11.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.205345784808532\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 660\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.995\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0884252786636353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.797168447927106e-06\n",
      "        policy_loss: 0.00011798381456173956\n",
      "        total_loss: 0.3265932500362396\n",
      "        vf_explained_var: -0.02160465717315674\n",
      "        vf_loss: 0.3264752924442291\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    sample_time_ms: 4191.939\n",
      "    update_time_ms: 14.341\n",
      "  iterations_since_restore: 660\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.62857142857142\n",
      "    ram_util_percent: 71.32857142857144\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.015430499498713\n",
      "    mean_inference_ms: 1.3915604715391614\n",
      "    mean_processing_ms: 0.9236302075732357\n",
      "  time_since_restore: 2320.3296298980713\n",
      "  time_this_iter_s: 5.099638223648071\n",
      "  time_total_s: 2320.3296298980713\n",
      "  timestamp: 1595950963\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 660\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2320 s, 660 iter, 1320000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.205345784808536\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 660\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.042\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.962921679019928\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00040194010944105685\n",
      "        policy_loss: -0.0020935945212841034\n",
      "        total_loss: 0.003646190743893385\n",
      "        vf_explained_var: 0.6882704496383667\n",
      "        vf_loss: 0.005739780142903328\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 1324000\n",
      "    num_steps_trained: 1324000\n",
      "    sample_time_ms: 4283.487\n",
      "    update_time_ms: 13.914\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.9\n",
      "    ram_util_percent: 70.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.015430499498711\n",
      "    mean_inference_ms: 1.3915604715391612\n",
      "    mean_processing_ms: 0.9236302075732358\n",
      "  time_since_restore: 2327.8289647102356\n",
      "  time_this_iter_s: 3.4182629585266113\n",
      "  time_total_s: 2327.8289647102356\n",
      "  timestamp: 1595950971\n",
      "  timesteps_since_restore: 1324000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1324000\n",
      "  training_iteration: 662\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2327 s, 662 iter, 1324000 ts, 11.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.205345784808536\n",
      "  episode_reward_min: 10.386016247427902\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 660\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.198\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0663671493530273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2332171536399983e-05\n",
      "        policy_loss: -0.00043609191197901964\n",
      "        total_loss: 0.0012464523315429688\n",
      "        vf_explained_var: 0.9161909818649292\n",
      "        vf_loss: 0.0016825358616188169\n",
      "    load_time_ms: 2.341\n",
      "    num_steps_sampled: 1328000\n",
      "    num_steps_trained: 1328000\n",
      "    sample_time_ms: 4259.591\n",
      "    update_time_ms: 12.111\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.2\n",
      "    ram_util_percent: 71.25\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.015430499498711\n",
      "    mean_inference_ms: 1.3915604715391612\n",
      "    mean_processing_ms: 0.9236302075732358\n",
      "  time_since_restore: 2335.827623128891\n",
      "  time_this_iter_s: 4.48795747756958\n",
      "  time_total_s: 2335.827623128891\n",
      "  timestamp: 1595950979\n",
      "  timesteps_since_restore: 1328000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1328000\n",
      "  training_iteration: 664\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2335 s, 664 iter, 1328000 ts, 11.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.246500812724026\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 665\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.595\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.081053376197815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.178981129778549e-05\n",
      "        policy_loss: -0.00038389014662243426\n",
      "        total_loss: 0.32214754819869995\n",
      "        vf_explained_var: 0.013200223445892334\n",
      "        vf_loss: 0.32253140211105347\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    sample_time_ms: 4378.232\n",
      "    update_time_ms: 11.735\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.81\n",
      "    ram_util_percent: 71.17000000000002\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.025683270049117\n",
      "    mean_inference_ms: 1.3937960585294575\n",
      "    mean_processing_ms: 0.9242384204773816\n",
      "  time_since_restore: 2342.2669694423676\n",
      "  time_this_iter_s: 6.4393463134765625\n",
      "  time_total_s: 2342.2669694423676\n",
      "  timestamp: 1595950985\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 665\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2342 s, 665 iter, 1330000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.246500812724026\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 665\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.643\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.983207643032074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.426734787761234e-06\n",
      "        policy_loss: 6.912231583555695e-06\n",
      "        total_loss: 0.0049654110334813595\n",
      "        vf_explained_var: 0.783616304397583\n",
      "        vf_loss: 0.004958490841090679\n",
      "    load_time_ms: 2.061\n",
      "    num_steps_sampled: 1334000\n",
      "    num_steps_trained: 1334000\n",
      "    sample_time_ms: 4252.976\n",
      "    update_time_ms: 9.538\n",
      "  iterations_since_restore: 667\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.2\n",
      "    ram_util_percent: 71.32000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.025683270049118\n",
      "    mean_inference_ms: 1.3937960585294573\n",
      "    mean_processing_ms: 0.9242384204773816\n",
      "  time_since_restore: 2349.797441005707\n",
      "  time_this_iter_s: 3.601428747177124\n",
      "  time_total_s: 2349.797441005707\n",
      "  timestamp: 1595950993\n",
      "  timesteps_since_restore: 1334000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1334000\n",
      "  training_iteration: 667\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2349 s, 667 iter, 1334000 ts, 11.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.246500812724026\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 665\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.277\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0693544149398804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.326000635046512e-05\n",
      "        policy_loss: -0.0008925628499127924\n",
      "        total_loss: 0.0008915860671550035\n",
      "        vf_explained_var: 0.8623021245002747\n",
      "        vf_loss: 0.0017841599183157086\n",
      "    load_time_ms: 1.914\n",
      "    num_steps_sampled: 1338000\n",
      "    num_steps_trained: 1338000\n",
      "    sample_time_ms: 4009.631\n",
      "    update_time_ms: 8.844\n",
      "  iterations_since_restore: 669\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.45\n",
      "    ram_util_percent: 71.275\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.025683270049118\n",
      "    mean_inference_ms: 1.3937960585294573\n",
      "    mean_processing_ms: 0.9242384204773816\n",
      "  time_since_restore: 2355.974757671356\n",
      "  time_this_iter_s: 3.0649759769439697\n",
      "  time_total_s: 2355.974757671356\n",
      "  timestamp: 1595950999\n",
      "  timesteps_since_restore: 1338000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1338000\n",
      "  training_iteration: 669\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2355 s, 669 iter, 1338000 ts, 11.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.25930258667032\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 670\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.038\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.067016363143921\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1857986464747228e-05\n",
      "        policy_loss: 9.349823085358366e-05\n",
      "        total_loss: 0.3317064940929413\n",
      "        vf_explained_var: -0.005301237106323242\n",
      "        vf_loss: 0.3316129446029663\n",
      "    load_time_ms: 1.886\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    sample_time_ms: 4045.736\n",
      "    update_time_ms: 9.009\n",
      "  iterations_since_restore: 670\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.725\n",
      "    ram_util_percent: 71.1875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.035842820566776\n",
      "    mean_inference_ms: 1.3959903875850106\n",
      "    mean_processing_ms: 0.9248381056367649\n",
      "  time_since_restore: 2361.47394824028\n",
      "  time_this_iter_s: 5.49919056892395\n",
      "  time_total_s: 2361.47394824028\n",
      "  timestamp: 1595951005\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 670\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2361 s, 670 iter, 1340000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.259302586670318\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 670\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9515277147293091\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001819422177504748\n",
      "        policy_loss: -0.00109034962952137\n",
      "        total_loss: 0.004992961883544922\n",
      "        vf_explained_var: 0.6827511787414551\n",
      "        vf_loss: 0.006083332933485508\n",
      "    load_time_ms: 2.312\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    sample_time_ms: 4142.138\n",
      "    update_time_ms: 9.254\n",
      "  iterations_since_restore: 672\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.61428571428573\n",
      "    ram_util_percent: 70.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.035842820566774\n",
      "    mean_inference_ms: 1.3959903875850106\n",
      "    mean_processing_ms: 0.9248381056367652\n",
      "  time_since_restore: 2370.0030636787415\n",
      "  time_this_iter_s: 4.764418601989746\n",
      "  time_total_s: 2370.0030636787415\n",
      "  timestamp: 1595951013\n",
      "  timesteps_since_restore: 1344000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 672\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2370 s, 672 iter, 1344000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.259302586670318\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 670\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.908\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0324763059616089\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.3246775021543726e-05\n",
      "        policy_loss: -0.0004889917327091098\n",
      "        total_loss: 0.001730783493258059\n",
      "        vf_explained_var: 0.9135289788246155\n",
      "        vf_loss: 0.0022197698708623648\n",
      "    load_time_ms: 2.298\n",
      "    num_steps_sampled: 1348000\n",
      "    num_steps_trained: 1348000\n",
      "    sample_time_ms: 4096.175\n",
      "    update_time_ms: 9.878\n",
      "  iterations_since_restore: 674\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.22\n",
      "    ram_util_percent: 70.86\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.035842820566774\n",
      "    mean_inference_ms: 1.3959903875850106\n",
      "    mean_processing_ms: 0.9248381056367652\n",
      "  time_since_restore: 2377.579409122467\n",
      "  time_this_iter_s: 3.7928874492645264\n",
      "  time_total_s: 2377.579409122467\n",
      "  timestamp: 1595951021\n",
      "  timesteps_since_restore: 1348000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1348000\n",
      "  training_iteration: 674\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2377 s, 674 iter, 1348000 ts, 11.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.279591299676849\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 675\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.339\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0543349981307983\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5531808458035812e-05\n",
      "        policy_loss: -1.1059284588554874e-05\n",
      "        total_loss: 0.32696056365966797\n",
      "        vf_explained_var: 0.03262835741043091\n",
      "        vf_loss: 0.3269716203212738\n",
      "    load_time_ms: 2.371\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    sample_time_ms: 3999.636\n",
      "    update_time_ms: 10.022\n",
      "  iterations_since_restore: 675\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.9\n",
      "    ram_util_percent: 70.8125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.046446859074025\n",
      "    mean_inference_ms: 1.398285306116717\n",
      "    mean_processing_ms: 0.9254614375191105\n",
      "  time_since_restore: 2383.0846016407013\n",
      "  time_this_iter_s: 5.505192518234253\n",
      "  time_total_s: 2383.0846016407013\n",
      "  timestamp: 1595951026\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 675\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2383 s, 675 iter, 1350000 ts, 11.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.279591299676849\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 675\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.068\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9188870787620544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.92648788774386e-05\n",
      "        policy_loss: 9.483337635174394e-05\n",
      "        total_loss: 0.0071290950290858746\n",
      "        vf_explained_var: 0.7133393883705139\n",
      "        vf_loss: 0.007034269627183676\n",
      "    load_time_ms: 2.341\n",
      "    num_steps_sampled: 1354000\n",
      "    num_steps_trained: 1354000\n",
      "    sample_time_ms: 4016.672\n",
      "    update_time_ms: 10.898\n",
      "  iterations_since_restore: 677\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.23333333333333\n",
      "    ram_util_percent: 70.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.046446859074025\n",
      "    mean_inference_ms: 1.3982853061167173\n",
      "    mean_processing_ms: 0.9254614375191101\n",
      "  time_since_restore: 2390.7852840423584\n",
      "  time_this_iter_s: 4.00424861907959\n",
      "  time_total_s: 2390.7852840423584\n",
      "  timestamp: 1595951034\n",
      "  timesteps_since_restore: 1354000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1354000\n",
      "  training_iteration: 677\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2390 s, 677 iter, 1354000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.279591299676849\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 675\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.995\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0303593873977661\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.727893868519459e-05\n",
      "        policy_loss: -6.195306923473254e-05\n",
      "        total_loss: 0.0019203124102205038\n",
      "        vf_explained_var: 0.8982299566268921\n",
      "        vf_loss: 0.001982263056561351\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 1358000\n",
      "    num_steps_trained: 1358000\n",
      "    sample_time_ms: 4386.605\n",
      "    update_time_ms: 12.065\n",
      "  iterations_since_restore: 679\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.2375\n",
      "    ram_util_percent: 70.825\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.046446859074025\n",
      "    mean_inference_ms: 1.3982853061167173\n",
      "    mean_processing_ms: 0.9254614375191101\n",
      "  time_since_restore: 2400.6983618736267\n",
      "  time_this_iter_s: 5.588543891906738\n",
      "  time_total_s: 2400.6983618736267\n",
      "  timestamp: 1595951044\n",
      "  timesteps_since_restore: 1358000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1358000\n",
      "  training_iteration: 679\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2400 s, 679 iter, 1358000 ts, 11.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.312766548517907\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 680\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.193\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.779244065284729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.126198422862217e-05\n",
      "        policy_loss: -0.0003641777148004621\n",
      "        total_loss: 0.028678126633167267\n",
      "        vf_explained_var: 0.4942290186882019\n",
      "        vf_loss: 0.029042301699519157\n",
      "    load_time_ms: 1.901\n",
      "    num_steps_sampled: 1362000\n",
      "    num_steps_trained: 1362000\n",
      "    sample_time_ms: 4293.034\n",
      "    update_time_ms: 14.617\n",
      "  iterations_since_restore: 681\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.8\n",
      "    ram_util_percent: 70.84\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.057448900678346\n",
      "    mean_inference_ms: 1.400654654362581\n",
      "    mean_processing_ms: 0.9260882710991887\n",
      "  time_since_restore: 2409.017639398575\n",
      "  time_this_iter_s: 3.7784013748168945\n",
      "  time_total_s: 2409.017639398575\n",
      "  timestamp: 1595951053\n",
      "  timesteps_since_restore: 1362000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1362000\n",
      "  training_iteration: 681\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2409 s, 681 iter, 1362000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.312766548517907\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 680\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.182\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.005545735359192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1026680112991016e-05\n",
      "        policy_loss: -0.00011326217645546421\n",
      "        total_loss: 0.0021222743671387434\n",
      "        vf_explained_var: 0.9120404124259949\n",
      "        vf_loss: 0.0022355541586875916\n",
      "    load_time_ms: 1.821\n",
      "    num_steps_sampled: 1366000\n",
      "    num_steps_trained: 1366000\n",
      "    sample_time_ms: 4201.416\n",
      "    update_time_ms: 14.038\n",
      "  iterations_since_restore: 683\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.61666666666667\n",
      "    ram_util_percent: 70.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.057448900678346\n",
      "    mean_inference_ms: 1.400654654362581\n",
      "    mean_processing_ms: 0.9260882710991887\n",
      "  time_since_restore: 2416.5839636325836\n",
      "  time_this_iter_s: 4.226748704910278\n",
      "  time_total_s: 2416.5839636325836\n",
      "  timestamp: 1595951060\n",
      "  timesteps_since_restore: 1366000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1366000\n",
      "  training_iteration: 683\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2416 s, 683 iter, 1366000 ts, 11.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.336787162532914\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 685\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0248581171035767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012420103303156793\n",
      "        policy_loss: -0.00076245644595474\n",
      "        total_loss: 0.3360406458377838\n",
      "        vf_explained_var: 0.0031284093856811523\n",
      "        vf_loss: 0.33680322766304016\n",
      "    load_time_ms: 1.676\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    sample_time_ms: 4116.777\n",
      "    update_time_ms: 12.909\n",
      "  iterations_since_restore: 685\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.55714285714286\n",
      "    ram_util_percent: 70.58571428571429\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.068291610460102\n",
      "    mean_inference_ms: 1.4029790900259078\n",
      "    mean_processing_ms: 0.9266980539137813\n",
      "  time_since_restore: 2424.9811000823975\n",
      "  time_this_iter_s: 4.624136686325073\n",
      "  time_total_s: 2424.9811000823975\n",
      "  timestamp: 1595951069\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 685\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2424 s, 685 iter, 1370000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.336787162532909\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 685\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.244\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9119548201560974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001253859663847834\n",
      "        policy_loss: 9.308338485425338e-05\n",
      "        total_loss: 0.0064238584600389\n",
      "        vf_explained_var: 0.7466120719909668\n",
      "        vf_loss: 0.006330762058496475\n",
      "    load_time_ms: 1.699\n",
      "    num_steps_sampled: 1374000\n",
      "    num_steps_trained: 1374000\n",
      "    sample_time_ms: 4008.1\n",
      "    update_time_ms: 11.583\n",
      "  iterations_since_restore: 687\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.72\n",
      "    ram_util_percent: 70.76\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.068291610460103\n",
      "    mean_inference_ms: 1.4029790900259078\n",
      "    mean_processing_ms: 0.9266980539137815\n",
      "  time_since_restore: 2431.60920381546\n",
      "  time_this_iter_s: 3.5089480876922607\n",
      "  time_total_s: 2431.60920381546\n",
      "  timestamp: 1595951075\n",
      "  timesteps_since_restore: 1374000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1374000\n",
      "  training_iteration: 687\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2431 s, 687 iter, 1374000 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.336787162532909\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 685\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.868\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0175608396530151\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1654248484992422e-05\n",
      "        policy_loss: -0.00015818930114619434\n",
      "        total_loss: 0.001749619492329657\n",
      "        vf_explained_var: 0.9189382791519165\n",
      "        vf_loss: 0.001907804748043418\n",
      "    load_time_ms: 1.737\n",
      "    num_steps_sampled: 1378000\n",
      "    num_steps_trained: 1378000\n",
      "    sample_time_ms: 3907.618\n",
      "    update_time_ms: 13.206\n",
      "  iterations_since_restore: 689\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.07142857142857\n",
      "    ram_util_percent: 70.84285714285714\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.068291610460103\n",
      "    mean_inference_ms: 1.4029790900259078\n",
      "    mean_processing_ms: 0.9266980539137815\n",
      "  time_since_restore: 2440.549329996109\n",
      "  time_this_iter_s: 4.878030300140381\n",
      "  time_total_s: 2440.549329996109\n",
      "  timestamp: 1595951084\n",
      "  timesteps_since_restore: 1378000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1378000\n",
      "  training_iteration: 689\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2440 s, 689 iter, 1378000 ts, 11.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.363480573591925\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 690\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.938\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0298573970794678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.87395219958853e-06\n",
      "        policy_loss: -5.5099488236010075e-05\n",
      "        total_loss: 0.3352597951889038\n",
      "        vf_explained_var: 0.011073768138885498\n",
      "        vf_loss: 0.3353148102760315\n",
      "    load_time_ms: 1.683\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    sample_time_ms: 4016.428\n",
      "    update_time_ms: 12.331\n",
      "  iterations_since_restore: 690\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.275\n",
      "    ram_util_percent: 70.9125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.079159984342547\n",
      "    mean_inference_ms: 1.4053061880193782\n",
      "    mean_processing_ms: 0.9273143826721767\n",
      "  time_since_restore: 2446.1744816303253\n",
      "  time_this_iter_s: 5.625151634216309\n",
      "  time_total_s: 2446.1744816303253\n",
      "  timestamp: 1595951090\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 690\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2446 s, 690 iter, 1380000 ts, 11.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.363480573591925\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 690\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.003\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9084152579307556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.816696986788884e-05\n",
      "        policy_loss: -0.000668325403239578\n",
      "        total_loss: 0.0050969733856618404\n",
      "        vf_explained_var: 0.6726292371749878\n",
      "        vf_loss: 0.005765300709754229\n",
      "    load_time_ms: 1.677\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1384000\n",
      "    sample_time_ms: 3945.402\n",
      "    update_time_ms: 11.394\n",
      "  iterations_since_restore: 692\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.03999999999999\n",
      "    ram_util_percent: 70.68\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.079159984342544\n",
      "    mean_inference_ms: 1.4053061880193782\n",
      "    mean_processing_ms: 0.9273143826721767\n",
      "  time_since_restore: 2452.57683467865\n",
      "  time_this_iter_s: 3.322634220123291\n",
      "  time_total_s: 2452.57683467865\n",
      "  timestamp: 1595951096\n",
      "  timesteps_since_restore: 1384000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 692\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2452 s, 692 iter, 1384000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.363480573591925\n",
      "  episode_reward_min: 10.432605221020813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 690\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.164\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9967361688613892\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.3978401916101575e-05\n",
      "        policy_loss: 7.331848337344127e-06\n",
      "        total_loss: 0.0020850428845733404\n",
      "        vf_explained_var: 0.8784856796264648\n",
      "        vf_loss: 0.0020777094177901745\n",
      "    load_time_ms: 1.733\n",
      "    num_steps_sampled: 1388000\n",
      "    num_steps_trained: 1388000\n",
      "    sample_time_ms: 3947.747\n",
      "    update_time_ms: 11.359\n",
      "  iterations_since_restore: 694\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60000000000001\n",
      "    ram_util_percent: 70.78333333333335\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.079159984342544\n",
      "    mean_inference_ms: 1.4053061880193782\n",
      "    mean_processing_ms: 0.9273143826721767\n",
      "  time_since_restore: 2460.614417076111\n",
      "  time_this_iter_s: 4.434085845947266\n",
      "  time_total_s: 2460.614417076111\n",
      "  timestamp: 1595951104\n",
      "  timesteps_since_restore: 1388000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1388000\n",
      "  training_iteration: 694\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2460 s, 694 iter, 1388000 ts, 11.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.420245979382813\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 695\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.056\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0084952116012573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7495453903393354e-06\n",
      "        policy_loss: 3.590774576878175e-05\n",
      "        total_loss: 0.3407120108604431\n",
      "        vf_explained_var: 0.017475366592407227\n",
      "        vf_loss: 0.34067612886428833\n",
      "    load_time_ms: 1.664\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    sample_time_ms: 4001.111\n",
      "    update_time_ms: 11.634\n",
      "  iterations_since_restore: 695\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.95714285714287\n",
      "    ram_util_percent: 70.8142857142857\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0897932495928275\n",
      "    mean_inference_ms: 1.4075815916282644\n",
      "    mean_processing_ms: 0.9279164611098726\n",
      "  time_since_restore: 2465.743956565857\n",
      "  time_this_iter_s: 5.129539489746094\n",
      "  time_total_s: 2465.743956565857\n",
      "  timestamp: 1595951110\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 695\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2465 s, 695 iter, 1390000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.420245979382807\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 695\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.645\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9069013595581055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00022502508363686502\n",
      "        policy_loss: -0.0006908783689141273\n",
      "        total_loss: 0.004369018599390984\n",
      "        vf_explained_var: 0.7789707183837891\n",
      "        vf_loss: 0.005059902090579271\n",
      "    load_time_ms: 1.852\n",
      "    num_steps_sampled: 1394000\n",
      "    num_steps_trained: 1394000\n",
      "    sample_time_ms: 4155.955\n",
      "    update_time_ms: 11.406\n",
      "  iterations_since_restore: 697\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.7\n",
      "    ram_util_percent: 70.75\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0897932495928275\n",
      "    mean_inference_ms: 1.407581591628265\n",
      "    mean_processing_ms: 0.9279164611098726\n",
      "  time_since_restore: 2473.922964811325\n",
      "  time_this_iter_s: 4.58155632019043\n",
      "  time_total_s: 2473.922964811325\n",
      "  timestamp: 1595951118\n",
      "  timesteps_since_restore: 1394000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1394000\n",
      "  training_iteration: 697\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2473 s, 697 iter, 1394000 ts, 11.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.420245979382807\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 695\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.908\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0213563442230225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.209264342440292e-05\n",
      "        policy_loss: -0.0013211669865995646\n",
      "        total_loss: 0.000104282378742937\n",
      "        vf_explained_var: 0.9214306473731995\n",
      "        vf_loss: 0.0014254519483074546\n",
      "    load_time_ms: 2.06\n",
      "    num_steps_sampled: 1398000\n",
      "    num_steps_trained: 1398000\n",
      "    sample_time_ms: 4050.658\n",
      "    update_time_ms: 9.685\n",
      "  iterations_since_restore: 699\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.94999999999999\n",
      "    ram_util_percent: 70.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0897932495928275\n",
      "    mean_inference_ms: 1.407581591628265\n",
      "    mean_processing_ms: 0.9279164611098726\n",
      "  time_since_restore: 2481.89754319191\n",
      "  time_this_iter_s: 3.8151988983154297\n",
      "  time_total_s: 2481.89754319191\n",
      "  timestamp: 1595951126\n",
      "  timesteps_since_restore: 1398000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1398000\n",
      "  training_iteration: 699\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2481 s, 699 iter, 1398000 ts, 11.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.419694967413443\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 700\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.025455355644226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.686842319439165e-05\n",
      "        policy_loss: -0.00011304474173812196\n",
      "        total_loss: 0.3410196304321289\n",
      "        vf_explained_var: -0.029970884323120117\n",
      "        vf_loss: 0.34113267064094543\n",
      "    load_time_ms: 2.084\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    sample_time_ms: 4217.728\n",
      "    update_time_ms: 10.239\n",
      "  iterations_since_restore: 700\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.34\n",
      "    ram_util_percent: 70.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.100367509664087\n",
      "    mean_inference_ms: 1.4098510340199975\n",
      "    mean_processing_ms: 0.9285379842952749\n",
      "  time_since_restore: 2489.1826088428497\n",
      "  time_this_iter_s: 7.285065650939941\n",
      "  time_total_s: 2489.1826088428497\n",
      "  timestamp: 1595951133\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 700\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2489 s, 700 iter, 1400000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.419694967413443\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 700\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.276\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9107085466384888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.625797373591922e-06\n",
      "        policy_loss: 7.466792885679752e-05\n",
      "        total_loss: 0.004729450214654207\n",
      "        vf_explained_var: 0.7874571681022644\n",
      "        vf_loss: 0.004654774442315102\n",
      "    load_time_ms: 2.216\n",
      "    num_steps_sampled: 1404000\n",
      "    num_steps_trained: 1404000\n",
      "    sample_time_ms: 4404.995\n",
      "    update_time_ms: 12.982\n",
      "  iterations_since_restore: 702\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.13333333333334\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.100367509664088\n",
      "    mean_inference_ms: 1.409851034019997\n",
      "    mean_processing_ms: 0.9285379842952749\n",
      "  time_since_restore: 2497.539999961853\n",
      "  time_this_iter_s: 4.268600702285767\n",
      "  time_total_s: 2497.539999961853\n",
      "  timestamp: 1595951142\n",
      "  timesteps_since_restore: 1404000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1404000\n",
      "  training_iteration: 702\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2497 s, 702 iter, 1404000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.419694967413443\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 700\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 54.028\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0145368576049805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00013117134221829474\n",
      "        policy_loss: -0.0014501332771033049\n",
      "        total_loss: 0.00014045524585526437\n",
      "        vf_explained_var: 0.9101394414901733\n",
      "        vf_loss: 0.0015905785840004683\n",
      "    load_time_ms: 2.155\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1408000\n",
      "    sample_time_ms: 4304.737\n",
      "    update_time_ms: 12.935\n",
      "  iterations_since_restore: 704\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.03999999999999\n",
      "    ram_util_percent: 71.88000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.100367509664088\n",
      "    mean_inference_ms: 1.409851034019997\n",
      "    mean_processing_ms: 0.9285379842952749\n",
      "  time_since_restore: 2504.5553212165833\n",
      "  time_this_iter_s: 3.4537158012390137\n",
      "  time_total_s: 2504.5553212165833\n",
      "  timestamp: 1595951149\n",
      "  timesteps_since_restore: 1408000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 704\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2504 s, 704 iter, 1408000 ts, 11.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.43907809148182\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 705\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.223\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7388342022895813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005710276309400797\n",
      "        policy_loss: -0.0028481960762292147\n",
      "        total_loss: 0.026337964460253716\n",
      "        vf_explained_var: 0.45700472593307495\n",
      "        vf_loss: 0.029186168685555458\n",
      "    load_time_ms: 2.079\n",
      "    num_steps_sampled: 1412000\n",
      "    num_steps_trained: 1412000\n",
      "    sample_time_ms: 4257.196\n",
      "    update_time_ms: 13.309\n",
      "  iterations_since_restore: 706\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.82\n",
      "    ram_util_percent: 70.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.110684357518339\n",
      "    mean_inference_ms: 1.4120688011884883\n",
      "    mean_processing_ms: 0.9291534687354193\n",
      "  time_since_restore: 2512.797026157379\n",
      "  time_this_iter_s: 3.596964120864868\n",
      "  time_total_s: 2512.797026157379\n",
      "  timestamp: 1595951157\n",
      "  timesteps_since_restore: 1412000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1412000\n",
      "  training_iteration: 706\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2512 s, 706 iter, 1412000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.43907809148182\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 705\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.155\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9826926589012146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1873185940203257e-05\n",
      "        policy_loss: 0.00015289497969206423\n",
      "        total_loss: 0.0021124009508639574\n",
      "        vf_explained_var: 0.8628689646720886\n",
      "        vf_loss: 0.001959524117410183\n",
      "    load_time_ms: 2.046\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    sample_time_ms: 4026.006\n",
      "    update_time_ms: 12.835\n",
      "  iterations_since_restore: 708\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.425\n",
      "    ram_util_percent: 70.675\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.110684357518339\n",
      "    mean_inference_ms: 1.4120688011884883\n",
      "    mean_processing_ms: 0.9291534687354193\n",
      "  time_since_restore: 2519.20996260643\n",
      "  time_this_iter_s: 3.1436262130737305\n",
      "  time_total_s: 2519.20996260643\n",
      "  timestamp: 1595951163\n",
      "  timesteps_since_restore: 1416000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 708\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2519 s, 708 iter, 1416000 ts, 11.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.449089593146628\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 710\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.698\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0101152658462524\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.427629755809903e-05\n",
      "        policy_loss: -0.00022220515529625118\n",
      "        total_loss: 0.3429417014122009\n",
      "        vf_explained_var: -0.0021401643753051758\n",
      "        vf_loss: 0.3431638777256012\n",
      "    load_time_ms: 1.776\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    sample_time_ms: 3731.171\n",
      "    update_time_ms: 11.522\n",
      "  iterations_since_restore: 710\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.74999999999999\n",
      "    ram_util_percent: 70.86666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.12102585213767\n",
      "    mean_inference_ms: 1.414295998637344\n",
      "    mean_processing_ms: 0.9297630230438482\n",
      "  time_since_restore: 2527.193834543228\n",
      "  time_this_iter_s: 4.417439699172974\n",
      "  time_total_s: 2527.193834543228\n",
      "  timestamp: 1595951171\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 710\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2527 s, 710 iter, 1420000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.44908959314663\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 710\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.526\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9068454504013062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010228967585135251\n",
      "        policy_loss: -0.00024792979820631444\n",
      "        total_loss: 0.004117610864341259\n",
      "        vf_explained_var: 0.7985249757766724\n",
      "        vf_loss: 0.004365535452961922\n",
      "    load_time_ms: 1.801\n",
      "    num_steps_sampled: 1424000\n",
      "    num_steps_trained: 1424000\n",
      "    sample_time_ms: 3613.309\n",
      "    update_time_ms: 8.724\n",
      "  iterations_since_restore: 712\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.18333333333334\n",
      "    ram_util_percent: 70.85000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.12102585213767\n",
      "    mean_inference_ms: 1.4142959986373438\n",
      "    mean_processing_ms: 0.9297630230438482\n",
      "  time_since_restore: 2534.382802248001\n",
      "  time_this_iter_s: 4.151457071304321\n",
      "  time_total_s: 2534.382802248001\n",
      "  timestamp: 1595951178\n",
      "  timesteps_since_restore: 1424000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1424000\n",
      "  training_iteration: 712\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2534 s, 712 iter, 1424000 ts, 11.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-24\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.44908959314663\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 710\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.464\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9809752702713013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2155115655332338e-05\n",
      "        policy_loss: 2.0783423678949475e-05\n",
      "        total_loss: 0.0018291816813871264\n",
      "        vf_explained_var: 0.9045804738998413\n",
      "        vf_loss: 0.0018083920003846288\n",
      "    load_time_ms: 1.823\n",
      "    num_steps_sampled: 1426000\n",
      "    num_steps_trained: 1426000\n",
      "    sample_time_ms: 3780.459\n",
      "    update_time_ms: 9.792\n",
      "  iterations_since_restore: 713\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.74285714285715\n",
      "    ram_util_percent: 67.9857142857143\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.12102585213767\n",
      "    mean_inference_ms: 1.4142959986373438\n",
      "    mean_processing_ms: 0.9297630230438482\n",
      "  time_since_restore: 2539.6644926071167\n",
      "  time_this_iter_s: 5.281690359115601\n",
      "  time_total_s: 2539.6644926071167\n",
      "  timestamp: 1595951184\n",
      "  timesteps_since_restore: 1426000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1426000\n",
      "  training_iteration: 713\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2539 s, 713 iter, 1426000 ts, 11.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.44908959314663\n",
      "  episode_reward_min: 10.467659099019484\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 710\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0001134872436523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.390447298239451e-05\n",
      "        policy_loss: -0.0007121696253307164\n",
      "        total_loss: 0.0007681522401981056\n",
      "        vf_explained_var: 0.859342098236084\n",
      "        vf_loss: 0.001480309059843421\n",
      "    load_time_ms: 1.88\n",
      "    num_steps_sampled: 1428000\n",
      "    num_steps_trained: 1428000\n",
      "    sample_time_ms: 3937.614\n",
      "    update_time_ms: 11.155\n",
      "  iterations_since_restore: 714\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.7375\n",
      "    ram_util_percent: 66.8125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.12102585213767\n",
      "    mean_inference_ms: 1.4142959986373438\n",
      "    mean_processing_ms: 0.9297630230438482\n",
      "  time_since_restore: 2544.709333181381\n",
      "  time_this_iter_s: 5.044840574264526\n",
      "  time_total_s: 2544.709333181381\n",
      "  timestamp: 1595951189\n",
      "  timesteps_since_restore: 1428000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1428000\n",
      "  training_iteration: 714\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2544 s, 714 iter, 1428000 ts, 11.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.469027609719992\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 715\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.824\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0098199844360352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.6324203392723575e-05\n",
      "        policy_loss: -0.00041210936615243554\n",
      "        total_loss: 0.3412576913833618\n",
      "        vf_explained_var: 0.014090418815612793\n",
      "        vf_loss: 0.34166979789733887\n",
      "    load_time_ms: 1.784\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    sample_time_ms: 4034.382\n",
      "    update_time_ms: 11.274\n",
      "  iterations_since_restore: 715\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.6\n",
      "    ram_util_percent: 67.63749999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1314353372518156\n",
      "    mean_inference_ms: 1.4165339611030754\n",
      "    mean_processing_ms: 0.9303920483685276\n",
      "  time_since_restore: 2550.3328745365143\n",
      "  time_this_iter_s: 5.623541355133057\n",
      "  time_total_s: 2550.3328745365143\n",
      "  timestamp: 1595951195\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 715\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2550 s, 715 iter, 1430000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.469027609719987\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 715\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.936\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8904079794883728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003021520678885281\n",
      "        policy_loss: -0.0007825002539902925\n",
      "        total_loss: 0.003789750626310706\n",
      "        vf_explained_var: 0.7733317017555237\n",
      "        vf_loss: 0.004572256468236446\n",
      "    load_time_ms: 1.876\n",
      "    num_steps_sampled: 1434000\n",
      "    num_steps_trained: 1434000\n",
      "    sample_time_ms: 4290.275\n",
      "    update_time_ms: 12.593\n",
      "  iterations_since_restore: 717\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.925\n",
      "    ram_util_percent: 66.63749999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.131435337251816\n",
      "    mean_inference_ms: 1.4165339611030756\n",
      "    mean_processing_ms: 0.9303920483685275\n",
      "  time_since_restore: 2559.805088996887\n",
      "  time_this_iter_s: 5.584776878356934\n",
      "  time_total_s: 2559.805088996887\n",
      "  timestamp: 1595951204\n",
      "  timesteps_since_restore: 1434000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1434000\n",
      "  training_iteration: 717\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2559 s, 717 iter, 1434000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.469027609719987\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 715\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 56.953\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9501033425331116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.737087015702855e-05\n",
      "        policy_loss: 0.0001471242867410183\n",
      "        total_loss: 0.002416536444798112\n",
      "        vf_explained_var: 0.8759782314300537\n",
      "        vf_loss: 0.0022694135550409555\n",
      "    load_time_ms: 2.012\n",
      "    num_steps_sampled: 1436000\n",
      "    num_steps_trained: 1436000\n",
      "    sample_time_ms: 4563.213\n",
      "    update_time_ms: 13.114\n",
      "  iterations_since_restore: 718\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.74444444444444\n",
      "    ram_util_percent: 66.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.131435337251816\n",
      "    mean_inference_ms: 1.4165339611030756\n",
      "    mean_processing_ms: 0.9303920483685275\n",
      "  time_since_restore: 2565.768693447113\n",
      "  time_this_iter_s: 5.96360445022583\n",
      "  time_total_s: 2565.768693447113\n",
      "  timestamp: 1595951210\n",
      "  timesteps_since_restore: 1436000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1436000\n",
      "  training_iteration: 718\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2565 s, 718 iter, 1436000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.469027609719987\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 715\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 57.193\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.97869473695755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.461148258385947e-05\n",
      "        policy_loss: -0.0003364050353411585\n",
      "        total_loss: 0.00121221540030092\n",
      "        vf_explained_var: 0.8970986008644104\n",
      "        vf_loss: 0.001548639265820384\n",
      "    load_time_ms: 2.022\n",
      "    num_steps_sampled: 1438000\n",
      "    num_steps_trained: 1438000\n",
      "    sample_time_ms: 4776.717\n",
      "    update_time_ms: 14.137\n",
      "  iterations_since_restore: 719\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.1\n",
      "    ram_util_percent: 66.65\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.131435337251816\n",
      "    mean_inference_ms: 1.4165339611030756\n",
      "    mean_processing_ms: 0.9303920483685275\n",
      "  time_since_restore: 2571.4863216876984\n",
      "  time_this_iter_s: 5.717628240585327\n",
      "  time_total_s: 2571.4863216876984\n",
      "  timestamp: 1595951216\n",
      "  timesteps_since_restore: 1438000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1438000\n",
      "  training_iteration: 719\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2571 s, 719 iter, 1438000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 18:47:07,459\tWARNING util.py:145 -- The `process_trial` operation took 0.13641119003295898 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.48127542256214\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 720\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 67.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9796096086502075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.359616989153437e-05\n",
      "        policy_loss: -0.00048567354679107666\n",
      "        total_loss: 0.35053306818008423\n",
      "        vf_explained_var: -0.0008293390274047852\n",
      "        vf_loss: 0.3510187566280365\n",
      "    load_time_ms: 2.651\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    sample_time_ms: 5418.356\n",
      "    update_time_ms: 13.517\n",
      "  iterations_since_restore: 720\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.73750000000001\n",
      "    ram_util_percent: 65.8125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.143240589410469\n",
      "    mean_inference_ms: 1.4190808130076902\n",
      "    mean_processing_ms: 0.9311502478544734\n",
      "  time_since_restore: 2582.459223508835\n",
      "  time_this_iter_s: 10.972901821136475\n",
      "  time_total_s: 2582.459223508835\n",
      "  timestamp: 1595951227\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 720\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2582 s, 720 iter, 1440000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.48127542256214\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 720\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 67.821\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7177650332450867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.314367884537205e-05\n",
      "        policy_loss: -0.00024682426010258496\n",
      "        total_loss: 0.029356421902775764\n",
      "        vf_explained_var: 0.523128867149353\n",
      "        vf_loss: 0.029603267088532448\n",
      "    load_time_ms: 2.705\n",
      "    num_steps_sampled: 1442000\n",
      "    num_steps_trained: 1442000\n",
      "    sample_time_ms: 5687.901\n",
      "    update_time_ms: 15.913\n",
      "  iterations_since_restore: 721\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.41250000000001\n",
      "    ram_util_percent: 64.7625\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.143240589410469\n",
      "    mean_inference_ms: 1.41908081300769\n",
      "    mean_processing_ms: 0.9311502478544736\n",
      "  time_since_restore: 2588.2194130420685\n",
      "  time_this_iter_s: 5.760189533233643\n",
      "  time_total_s: 2588.2194130420685\n",
      "  timestamp: 1595951233\n",
      "  timesteps_since_restore: 1442000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1442000\n",
      "  training_iteration: 721\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2588 s, 721 iter, 1442000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.48127542256214\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 720\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 60.342\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9698643684387207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.690803952049464e-06\n",
      "        policy_loss: -1.766514833434485e-05\n",
      "        total_loss: 0.0017171325162053108\n",
      "        vf_explained_var: 0.8841053247451782\n",
      "        vf_loss: 0.0017347988905385137\n",
      "    load_time_ms: 2.508\n",
      "    num_steps_sampled: 1446000\n",
      "    num_steps_trained: 1446000\n",
      "    sample_time_ms: 5721.572\n",
      "    update_time_ms: 14.581\n",
      "  iterations_since_restore: 723\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.60000000000001\n",
      "    ram_util_percent: 66.35\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.143240589410469\n",
      "    mean_inference_ms: 1.41908081300769\n",
      "    mean_processing_ms: 0.9311502478544736\n",
      "  time_since_restore: 2597.8583195209503\n",
      "  time_this_iter_s: 5.385109901428223\n",
      "  time_total_s: 2597.8583195209503\n",
      "  timestamp: 1595951242\n",
      "  timesteps_since_restore: 1446000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1446000\n",
      "  training_iteration: 723\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2597 s, 723 iter, 1446000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.494070526053143\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 725\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 63.937\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9918463230133057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.4630349293584004e-05\n",
      "        policy_loss: 9.047794446814805e-05\n",
      "        total_loss: 0.3524080812931061\n",
      "        vf_explained_var: -0.0035789012908935547\n",
      "        vf_loss: 0.3523176908493042\n",
      "    load_time_ms: 2.841\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    sample_time_ms: 5725.323\n",
      "    update_time_ms: 13.026\n",
      "  iterations_since_restore: 725\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.28888888888889\n",
      "    ram_util_percent: 66.92222222222222\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1553412803521885\n",
      "    mean_inference_ms: 1.4216934910879244\n",
      "    mean_processing_ms: 0.931940196283815\n",
      "  time_since_restore: 2608.613835811615\n",
      "  time_this_iter_s: 6.4381022453308105\n",
      "  time_total_s: 2608.613835811615\n",
      "  timestamp: 1595951253\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 725\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2608 s, 725 iter, 1450000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.494070526053143\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 725\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 72.725\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7110881209373474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002654731215443462\n",
      "        policy_loss: -0.0017384715611115098\n",
      "        total_loss: 0.02794583886861801\n",
      "        vf_explained_var: 0.48598527908325195\n",
      "        vf_loss: 0.029684288427233696\n",
      "    load_time_ms: 3.109\n",
      "    num_steps_sampled: 1452000\n",
      "    num_steps_trained: 1452000\n",
      "    sample_time_ms: 6294.796\n",
      "    update_time_ms: 12.952\n",
      "  iterations_since_restore: 726\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.34285714285714\n",
      "    ram_util_percent: 67.15714285714287\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.155341280352187\n",
      "    mean_inference_ms: 1.4216934910879246\n",
      "    mean_processing_ms: 0.9319401962838149\n",
      "  time_since_restore: 2618.3089492321014\n",
      "  time_this_iter_s: 9.69511342048645\n",
      "  time_total_s: 2618.3089492321014\n",
      "  timestamp: 1595951263\n",
      "  timesteps_since_restore: 1452000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1452000\n",
      "  training_iteration: 726\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2618 s, 726 iter, 1452000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.494070526053143\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 725\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 81.731\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8856322169303894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002074531657854095\n",
      "        policy_loss: -0.0009891565423458815\n",
      "        total_loss: 0.0036813088227063417\n",
      "        vf_explained_var: 0.7741978764533997\n",
      "        vf_loss: 0.004670468624681234\n",
      "    load_time_ms: 3.862\n",
      "    num_steps_sampled: 1454000\n",
      "    num_steps_trained: 1454000\n",
      "    sample_time_ms: 6790.42\n",
      "    update_time_ms: 15.064\n",
      "  iterations_since_restore: 727\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.89333333333335\n",
      "    ram_util_percent: 67.10000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.155341280352187\n",
      "    mean_inference_ms: 1.4216934910879246\n",
      "    mean_processing_ms: 0.9319401962838149\n",
      "  time_since_restore: 2628.9895379543304\n",
      "  time_this_iter_s: 10.680588722229004\n",
      "  time_total_s: 2628.9895379543304\n",
      "  timestamp: 1595951274\n",
      "  timesteps_since_restore: 1454000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1454000\n",
      "  training_iteration: 727\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2628 s, 727 iter, 1454000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.494070526053143\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 725\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 84.478\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9570196866989136\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3452967070625164e-05\n",
      "        policy_loss: 0.00011399483628338203\n",
      "        total_loss: 0.002280775923281908\n",
      "        vf_explained_var: 0.8734601140022278\n",
      "        vf_loss: 0.002166774356737733\n",
      "    load_time_ms: 3.997\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1456000\n",
      "    sample_time_ms: 7360.647\n",
      "    update_time_ms: 16.217\n",
      "  iterations_since_restore: 728\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.11176470588236\n",
      "    ram_util_percent: 67.11176470588236\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.155341280352187\n",
      "    mean_inference_ms: 1.4216934910879246\n",
      "    mean_processing_ms: 0.9319401962838149\n",
      "  time_since_restore: 2640.712880373001\n",
      "  time_this_iter_s: 11.723342418670654\n",
      "  time_total_s: 2640.712880373001\n",
      "  timestamp: 1595951285\n",
      "  timesteps_since_restore: 1456000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 728\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2640 s, 728 iter, 1456000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.494070526053143\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 725\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 90.415\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9950042963027954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1422276656958275e-05\n",
      "        policy_loss: -0.0004020223568659276\n",
      "        total_loss: 0.001195363001897931\n",
      "        vf_explained_var: 0.8482533693313599\n",
      "        vf_loss: 0.0015973947010934353\n",
      "    load_time_ms: 4.241\n",
      "    num_steps_sampled: 1458000\n",
      "    num_steps_trained: 1458000\n",
      "    sample_time_ms: 7888.817\n",
      "    update_time_ms: 18.221\n",
      "  iterations_since_restore: 729\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.73750000000001\n",
      "    ram_util_percent: 66.725\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.155341280352187\n",
      "    mean_inference_ms: 1.4216934910879246\n",
      "    mean_processing_ms: 0.9319401962838149\n",
      "  time_since_restore: 2651.805260658264\n",
      "  time_this_iter_s: 11.092380285263062\n",
      "  time_total_s: 2651.805260658264\n",
      "  timestamp: 1595951297\n",
      "  timesteps_since_restore: 1458000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1458000\n",
      "  training_iteration: 729\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2651 s, 729 iter, 1458000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.523187763359974\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 730\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 85.98\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.016942024230957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.08302446178277e-06\n",
      "        policy_loss: 9.782886627363041e-05\n",
      "        total_loss: 0.3474511206150055\n",
      "        vf_explained_var: 0.0051610469818115234\n",
      "        vf_loss: 0.3473533093929291\n",
      "    load_time_ms: 3.837\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    sample_time_ms: 7739.181\n",
      "    update_time_ms: 19.092\n",
      "  iterations_since_restore: 730\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.5923076923077\n",
      "    ram_util_percent: 64.19999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.171275623256804\n",
      "    mean_inference_ms: 1.425094357936061\n",
      "    mean_processing_ms: 0.932982724947952\n",
      "  time_since_restore: 2661.2451741695404\n",
      "  time_this_iter_s: 9.439913511276245\n",
      "  time_total_s: 2661.2451741695404\n",
      "  timestamp: 1595951306\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 730\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2661 s, 730 iter, 1460000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.523187763359974\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 730\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 92.515\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7251698970794678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00021560773893725127\n",
      "        policy_loss: -0.001047715195454657\n",
      "        total_loss: 0.028567921370267868\n",
      "        vf_explained_var: 0.5444198846817017\n",
      "        vf_loss: 0.02961564064025879\n",
      "    load_time_ms: 4.268\n",
      "    num_steps_sampled: 1462000\n",
      "    num_steps_trained: 1462000\n",
      "    sample_time_ms: 7814.522\n",
      "    update_time_ms: 19.262\n",
      "  iterations_since_restore: 731\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.19\n",
      "    ram_util_percent: 64.11\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.171275623256807\n",
      "    mean_inference_ms: 1.425094357936061\n",
      "    mean_processing_ms: 0.932982724947952\n",
      "  time_since_restore: 2667.8478150367737\n",
      "  time_this_iter_s: 6.602640867233276\n",
      "  time_total_s: 2667.8478150367737\n",
      "  timestamp: 1595951313\n",
      "  timesteps_since_restore: 1462000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1462000\n",
      "  training_iteration: 731\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2667 s, 731 iter, 1462000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.523187763359974\n",
      "  episode_reward_min: 10.484247135767813\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 730\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 95.375\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9777921438217163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.952590047148988e-05\n",
      "        policy_loss: -0.00038777446025051177\n",
      "        total_loss: 0.0011216695420444012\n",
      "        vf_explained_var: 0.8858311772346497\n",
      "        vf_loss: 0.001509450958110392\n",
      "    load_time_ms: 4.537\n",
      "    num_steps_sampled: 1466000\n",
      "    num_steps_trained: 1466000\n",
      "    sample_time_ms: 7702.873\n",
      "    update_time_ms: 19.621\n",
      "  iterations_since_restore: 733\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.43333333333334\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.171275623256807\n",
      "    mean_inference_ms: 1.425094357936061\n",
      "    mean_processing_ms: 0.932982724947952\n",
      "  time_since_restore: 2676.4100403785706\n",
      "  time_this_iter_s: 4.3741137981414795\n",
      "  time_total_s: 2676.4100403785706\n",
      "  timestamp: 1595951321\n",
      "  timesteps_since_restore: 1466000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1466000\n",
      "  training_iteration: 733\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2676 s, 733 iter, 1466000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.524594901324495\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 735\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 91.275\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9796435832977295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.830873902188614e-05\n",
      "        policy_loss: -0.0003308029263280332\n",
      "        total_loss: 0.34756165742874146\n",
      "        vf_explained_var: 0.02669471502304077\n",
      "        vf_loss: 0.3478924632072449\n",
      "    load_time_ms: 4.265\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    sample_time_ms: 7586.921\n",
      "    update_time_ms: 20.876\n",
      "  iterations_since_restore: 735\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.025000000000006\n",
      "    ram_util_percent: 61.875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.187729784877938\n",
      "    mean_inference_ms: 1.4285981345210144\n",
      "    mean_processing_ms: 0.9340492665310504\n",
      "  time_since_restore: 2685.9626574516296\n",
      "  time_this_iter_s: 5.319108486175537\n",
      "  time_total_s: 2685.9626574516296\n",
      "  timestamp: 1595951331\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 735\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2685 s, 735 iter, 1470000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.524594901324498\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 735\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 70.674\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.872529149055481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.63372115418315e-05\n",
      "        policy_loss: -0.00037224864354357123\n",
      "        total_loss: 0.003922241739928722\n",
      "        vf_explained_var: 0.8007694482803345\n",
      "        vf_loss: 0.004294487182050943\n",
      "    load_time_ms: 3.131\n",
      "    num_steps_sampled: 1474000\n",
      "    num_steps_trained: 1474000\n",
      "    sample_time_ms: 6366.047\n",
      "    update_time_ms: 18.574\n",
      "  iterations_since_restore: 737\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.46666666666665\n",
      "    ram_util_percent: 61.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.187729784877938\n",
      "    mean_inference_ms: 1.4285981345210146\n",
      "    mean_processing_ms: 0.9340492665310504\n",
      "  time_since_restore: 2693.8434598445892\n",
      "  time_this_iter_s: 4.009306192398071\n",
      "  time_total_s: 2693.8434598445892\n",
      "  timestamp: 1595951339\n",
      "  timesteps_since_restore: 1474000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1474000\n",
      "  training_iteration: 737\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2693 s, 737 iter, 1474000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.524594901324498\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 735\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.877\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9540463089942932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.8403264372609556e-05\n",
      "        policy_loss: -0.0008967366302385926\n",
      "        total_loss: 0.0007735738763585687\n",
      "        vf_explained_var: 0.8639152646064758\n",
      "        vf_loss: 0.001670310040935874\n",
      "    load_time_ms: 2.613\n",
      "    num_steps_sampled: 1478000\n",
      "    num_steps_trained: 1478000\n",
      "    sample_time_ms: 4940.431\n",
      "    update_time_ms: 14.65\n",
      "  iterations_since_restore: 739\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.53333333333333\n",
      "    ram_util_percent: 61.949999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.187729784877938\n",
      "    mean_inference_ms: 1.4285981345210146\n",
      "    mean_processing_ms: 0.9340492665310504\n",
      "  time_since_restore: 2702.1716482639313\n",
      "  time_this_iter_s: 3.9219579696655273\n",
      "  time_total_s: 2702.1716482639313\n",
      "  timestamp: 1595951347\n",
      "  timesteps_since_restore: 1478000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1478000\n",
      "  training_iteration: 739\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2702 s, 739 iter, 1478000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.537115754633124\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 740\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 52.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9592915177345276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2093246368749533e-05\n",
      "        policy_loss: 0.00022042369528207928\n",
      "        total_loss: 0.3595408499240875\n",
      "        vf_explained_var: -0.008330345153808594\n",
      "        vf_loss: 0.3593205213546753\n",
      "    load_time_ms: 2.584\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    sample_time_ms: 4564.471\n",
      "    update_time_ms: 14.791\n",
      "  iterations_since_restore: 740\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.9\n",
      "    ram_util_percent: 61.912499999999994\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.203779942784223\n",
      "    mean_inference_ms: 1.4320148155565635\n",
      "    mean_processing_ms: 0.9350820036477242\n",
      "  time_since_restore: 2707.7832174301147\n",
      "  time_this_iter_s: 5.611569166183472\n",
      "  time_total_s: 2707.7832174301147\n",
      "  timestamp: 1595951353\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 740\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2707 s, 740 iter, 1480000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.537115754633126\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 740\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.727\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.855129599571228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.000556759478058666\n",
      "        policy_loss: -0.00018484592146705836\n",
      "        total_loss: 0.004362098872661591\n",
      "        vf_explained_var: 0.8170560598373413\n",
      "        vf_loss: 0.004546936135739088\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 1484000\n",
      "    num_steps_trained: 1484000\n",
      "    sample_time_ms: 4319.393\n",
      "    update_time_ms: 12.846\n",
      "  iterations_since_restore: 742\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.91666666666667\n",
      "    ram_util_percent: 61.96666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.203779942784223\n",
      "    mean_inference_ms: 1.4320148155565635\n",
      "    mean_processing_ms: 0.9350820036477241\n",
      "  time_since_restore: 2716.0828535556793\n",
      "  time_this_iter_s: 4.314257621765137\n",
      "  time_total_s: 2716.0828535556793\n",
      "  timestamp: 1595951361\n",
      "  timesteps_since_restore: 1484000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1484000\n",
      "  training_iteration: 742\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2716 s, 742 iter, 1484000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.537115754633126\n",
      "  episode_reward_min: 10.796809491242296\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 740\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.265\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9745312929153442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001464660163037479\n",
      "        policy_loss: -0.0019881678745150566\n",
      "        total_loss: -0.000716398237273097\n",
      "        vf_explained_var: 0.9196460843086243\n",
      "        vf_loss: 0.0012717553181573749\n",
      "    load_time_ms: 2.048\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    sample_time_ms: 4270.433\n",
      "    update_time_ms: 12.731\n",
      "  iterations_since_restore: 744\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.68333333333334\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.203779942784223\n",
      "    mean_inference_ms: 1.4320148155565635\n",
      "    mean_processing_ms: 0.9350820036477241\n",
      "  time_since_restore: 2724.1461567878723\n",
      "  time_this_iter_s: 3.9074981212615967\n",
      "  time_total_s: 2724.1461567878723\n",
      "  timestamp: 1595951369\n",
      "  timesteps_since_restore: 1488000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 744\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2724 s, 744 iter, 1488000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.540398962776756\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 745\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0028831958770752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.548483022721484e-05\n",
      "        policy_loss: 0.0002466211444698274\n",
      "        total_loss: 0.3492508828639984\n",
      "        vf_explained_var: 0.00982522964477539\n",
      "        vf_loss: 0.34900426864624023\n",
      "    load_time_ms: 1.992\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    sample_time_ms: 4384.019\n",
      "    update_time_ms: 12.288\n",
      "  iterations_since_restore: 745\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.5\n",
      "    ram_util_percent: 61.82222222222222\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.219727890571949\n",
      "    mean_inference_ms: 1.4354104646196262\n",
      "    mean_processing_ms: 0.936106050838858\n",
      "  time_since_restore: 2730.617434024811\n",
      "  time_this_iter_s: 6.471277236938477\n",
      "  time_total_s: 2730.617434024811\n",
      "  timestamp: 1595951376\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 745\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2730 s, 745 iter, 1490000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.540398962776758\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 745\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.906\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8659539818763733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001881935604615137\n",
      "        policy_loss: -0.0007104940596036613\n",
      "        total_loss: 0.0029413376469165087\n",
      "        vf_explained_var: 0.8324273228645325\n",
      "        vf_loss: 0.003651821054518223\n",
      "    load_time_ms: 2.138\n",
      "    num_steps_sampled: 1494000\n",
      "    num_steps_trained: 1494000\n",
      "    sample_time_ms: 4411.188\n",
      "    update_time_ms: 12.945\n",
      "  iterations_since_restore: 747\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.1\n",
      "    ram_util_percent: 61.66666666666668\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.219727890571951\n",
      "    mean_inference_ms: 1.4354104646196262\n",
      "    mean_processing_ms: 0.936106050838858\n",
      "  time_since_restore: 2738.8127992153168\n",
      "  time_this_iter_s: 4.2500598430633545\n",
      "  time_total_s: 2738.8127992153168\n",
      "  timestamp: 1595951384\n",
      "  timesteps_since_restore: 1494000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1494000\n",
      "  training_iteration: 747\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2738 s, 747 iter, 1494000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.300825743297942\n",
      "  episode_reward_mean: 11.540398962776758\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 745\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.223\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9438302516937256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.862114731920883e-05\n",
      "        policy_loss: 5.7594775171310175e-06\n",
      "        total_loss: 0.0014816863695159554\n",
      "        vf_explained_var: 0.8344142436981201\n",
      "        vf_loss: 0.0014759218320250511\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 1496000\n",
      "    num_steps_trained: 1496000\n",
      "    sample_time_ms: 4471.361\n",
      "    update_time_ms: 12.903\n",
      "  iterations_since_restore: 748\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.92857142857143\n",
      "    ram_util_percent: 62.05714285714286\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.219727890571951\n",
      "    mean_inference_ms: 1.4354104646196262\n",
      "    mean_processing_ms: 0.936106050838858\n",
      "  time_since_restore: 2743.8709654808044\n",
      "  time_this_iter_s: 5.058166265487671\n",
      "  time_total_s: 2743.8709654808044\n",
      "  timestamp: 1595951389\n",
      "  timesteps_since_restore: 1496000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1496000\n",
      "  training_iteration: 748\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2743 s, 748 iter, 1496000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.53078899681112\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 750\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 56.538\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9676540493965149\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.9234160794876516e-05\n",
      "        policy_loss: 2.718305586313363e-05\n",
      "        total_loss: 0.3588446080684662\n",
      "        vf_explained_var: -0.02907693386077881\n",
      "        vf_loss: 0.358817458152771\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    sample_time_ms: 4536.274\n",
      "    update_time_ms: 12.606\n",
      "  iterations_since_restore: 750\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.3625\n",
      "    ram_util_percent: 62.724999999999994\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.235523787128012\n",
      "    mean_inference_ms: 1.4387511436064246\n",
      "    mean_processing_ms: 0.9371180885318883\n",
      "  time_since_restore: 2754.066594839096\n",
      "  time_this_iter_s: 5.869704723358154\n",
      "  time_total_s: 2754.066594839096\n",
      "  timestamp: 1595951399\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 750\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2754 s, 750 iter, 1500000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.53078899681112\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 750\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.56\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.837202787399292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00047926808474585414\n",
      "        policy_loss: -0.0012840924318879843\n",
      "        total_loss: 0.002744375728070736\n",
      "        vf_explained_var: 0.8047844171524048\n",
      "        vf_loss: 0.004028476309031248\n",
      "    load_time_ms: 2.055\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "    sample_time_ms: 4508.164\n",
      "    update_time_ms: 12.735\n",
      "  iterations_since_restore: 752\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.98333333333333\n",
      "    ram_util_percent: 62.16666666666668\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.235523787128011\n",
      "    mean_inference_ms: 1.4387511436064244\n",
      "    mean_processing_ms: 0.9371180885318883\n",
      "  time_since_restore: 2762.025068998337\n",
      "  time_this_iter_s: 3.8185997009277344\n",
      "  time_total_s: 2762.025068998337\n",
      "  timestamp: 1595951407\n",
      "  timesteps_since_restore: 1504000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 752\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2762 s, 752 iter, 1504000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.53078899681112\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 750\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 54.332\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9617037177085876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.731755663873628e-05\n",
      "        policy_loss: -0.0012548990780487657\n",
      "        total_loss: -0.00010049247794086114\n",
      "        vf_explained_var: 0.9285944700241089\n",
      "        vf_loss: 0.001154411002062261\n",
      "    load_time_ms: 2.273\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "    sample_time_ms: 4458.27\n",
      "    update_time_ms: 12.913\n",
      "  iterations_since_restore: 754\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.9\n",
      "    ram_util_percent: 62.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.235523787128011\n",
      "    mean_inference_ms: 1.4387511436064244\n",
      "    mean_processing_ms: 0.9371180885318883\n",
      "  time_since_restore: 2769.614554166794\n",
      "  time_this_iter_s: 3.352628469467163\n",
      "  time_total_s: 2769.614554166794\n",
      "  timestamp: 1595951415\n",
      "  timesteps_since_restore: 1508000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 754\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2769 s, 754 iter, 1508000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.534384824863153\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 755\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.357\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6416667103767395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.143355418927968e-05\n",
      "        policy_loss: -0.00020285797654651105\n",
      "        total_loss: 0.03160199522972107\n",
      "        vf_explained_var: 0.5460708141326904\n",
      "        vf_loss: 0.031804852187633514\n",
      "    load_time_ms: 2.117\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    sample_time_ms: 4167.637\n",
      "    update_time_ms: 12.035\n",
      "  iterations_since_restore: 756\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.6\n",
      "    ram_util_percent: 62.075\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.251175513009278\n",
      "    mean_inference_ms: 1.442057773611555\n",
      "    mean_processing_ms: 0.9381109062639144\n",
      "  time_since_restore: 2777.016886472702\n",
      "  time_this_iter_s: 2.8825719356536865\n",
      "  time_total_s: 2777.016886472702\n",
      "  timestamp: 1595951423\n",
      "  timesteps_since_restore: 1512000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 756\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2777 s, 756 iter, 1512000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.534384824863153\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 755\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8834840059280396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012229735148139298\n",
      "        policy_loss: -0.00018183994689024985\n",
      "        total_loss: 0.001808023895137012\n",
      "        vf_explained_var: 0.8994019031524658\n",
      "        vf_loss: 0.001989854034036398\n",
      "    load_time_ms: 1.885\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "    sample_time_ms: 3924.41\n",
      "    update_time_ms: 11.187\n",
      "  iterations_since_restore: 758\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.16\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.251175513009278\n",
      "    mean_inference_ms: 1.442057773611555\n",
      "    mean_processing_ms: 0.9381109062639144\n",
      "  time_since_restore: 2783.832219839096\n",
      "  time_this_iter_s: 3.7942135334014893\n",
      "  time_total_s: 2783.832219839096\n",
      "  timestamp: 1595951429\n",
      "  timesteps_since_restore: 1516000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 758\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2783 s, 758 iter, 1516000 ts, 11.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.549244292482665\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 760\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.854\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9249231219291687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012862759467680007\n",
      "        policy_loss: -0.0007279090932570398\n",
      "        total_loss: 0.35900598764419556\n",
      "        vf_explained_var: 0.001989424228668213\n",
      "        vf_loss: 0.3597338795661926\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    sample_time_ms: 3707.413\n",
      "    update_time_ms: 10.273\n",
      "  iterations_since_restore: 760\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.58333333333332\n",
      "    ram_util_percent: 61.949999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.266028413414409\n",
      "    mean_inference_ms: 1.445186298010038\n",
      "    mean_processing_ms: 0.9390485617557262\n",
      "  time_since_restore: 2791.793259859085\n",
      "  time_this_iter_s: 4.465952157974243\n",
      "  time_total_s: 2791.793259859085\n",
      "  timestamp: 1595951437\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 760\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2791 s, 760 iter, 1520000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.549244292482667\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 760\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7911866307258606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00024322175886482\n",
      "        policy_loss: -0.0007275292882695794\n",
      "        total_loss: 0.0043477751314640045\n",
      "        vf_explained_var: 0.7460047006607056\n",
      "        vf_loss: 0.0050753029063344\n",
      "    load_time_ms: 1.753\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "    sample_time_ms: 3611.949\n",
      "    update_time_ms: 9.044\n",
      "  iterations_since_restore: 762\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.075\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.266028413414409\n",
      "    mean_inference_ms: 1.4451862980100376\n",
      "    mean_processing_ms: 0.9390485617557265\n",
      "  time_since_restore: 2798.772321462631\n",
      "  time_this_iter_s: 3.4556620121002197\n",
      "  time_total_s: 2798.772321462631\n",
      "  timestamp: 1595951444\n",
      "  timesteps_since_restore: 1524000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 762\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2798 s, 762 iter, 1524000 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.289611994824325\n",
      "  episode_reward_mean: 11.549244292482667\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 760\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9016913771629333\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.067932579549961e-05\n",
      "        policy_loss: -0.0005207648500800133\n",
      "        total_loss: 0.001115167629905045\n",
      "        vf_explained_var: 0.8925562500953674\n",
      "        vf_loss: 0.0016359258443117142\n",
      "    load_time_ms: 1.538\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "    sample_time_ms: 3511.128\n",
      "    update_time_ms: 8.647\n",
      "  iterations_since_restore: 764\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.266028413414409\n",
      "    mean_inference_ms: 1.4451862980100376\n",
      "    mean_processing_ms: 0.9390485617557265\n",
      "  time_since_restore: 2805.293839931488\n",
      "  time_this_iter_s: 3.439046859741211\n",
      "  time_total_s: 2805.293839931488\n",
      "  timestamp: 1595951451\n",
      "  timesteps_since_restore: 1528000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 764\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2805 s, 764 iter, 1528000 ts, 11.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.564912824224987\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 765\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.283\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9258410930633545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.825945198826957e-06\n",
      "        policy_loss: 7.186603761510924e-05\n",
      "        total_loss: 0.3601662516593933\n",
      "        vf_explained_var: 0.014045119285583496\n",
      "        vf_loss: 0.3600943684577942\n",
      "    load_time_ms: 1.609\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    sample_time_ms: 3561.344\n",
      "    update_time_ms: 8.5\n",
      "  iterations_since_restore: 765\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.9\n",
      "    ram_util_percent: 62.24285714285713\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.280306524352096\n",
      "    mean_inference_ms: 1.4481832880171166\n",
      "    mean_processing_ms: 0.9399387299145401\n",
      "  time_since_restore: 2810.2905468940735\n",
      "  time_this_iter_s: 4.996706962585449\n",
      "  time_total_s: 2810.2905468940735\n",
      "  timestamp: 1595951456\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 765\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2810 s, 765 iter, 1530000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.564912824224987\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 765\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.84\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7962285280227661\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00014746117813047022\n",
      "        policy_loss: -0.00048349000280722976\n",
      "        total_loss: 0.0038451615255326033\n",
      "        vf_explained_var: 0.7840296626091003\n",
      "        vf_loss: 0.004328652750700712\n",
      "    load_time_ms: 1.588\n",
      "    num_steps_sampled: 1534000\n",
      "    num_steps_trained: 1534000\n",
      "    sample_time_ms: 3807.126\n",
      "    update_time_ms: 8.274\n",
      "  iterations_since_restore: 767\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.26666666666667\n",
      "    ram_util_percent: 62.51666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.280306524352096\n",
      "    mean_inference_ms: 1.4481832880171164\n",
      "    mean_processing_ms: 0.9399387299145401\n",
      "  time_since_restore: 2818.6688237190247\n",
      "  time_this_iter_s: 4.490233659744263\n",
      "  time_total_s: 2818.6688237190247\n",
      "  timestamp: 1595951464\n",
      "  timesteps_since_restore: 1534000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1534000\n",
      "  training_iteration: 767\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2818 s, 767 iter, 1534000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.564912824224987\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 765\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9027810096740723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.689703150186688e-05\n",
      "        policy_loss: -0.0011036639334633946\n",
      "        total_loss: 0.00025333641679026186\n",
      "        vf_explained_var: 0.879543662071228\n",
      "        vf_loss: 0.0013569948496297002\n",
      "    load_time_ms: 1.726\n",
      "    num_steps_sampled: 1538000\n",
      "    num_steps_trained: 1538000\n",
      "    sample_time_ms: 3936.002\n",
      "    update_time_ms: 8.443\n",
      "  iterations_since_restore: 769\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83333333333334\n",
      "    ram_util_percent: 63.01666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.280306524352096\n",
      "    mean_inference_ms: 1.4481832880171164\n",
      "    mean_processing_ms: 0.9399387299145401\n",
      "  time_since_restore: 2827.295240879059\n",
      "  time_this_iter_s: 4.615971565246582\n",
      "  time_total_s: 2827.295240879059\n",
      "  timestamp: 1595951473\n",
      "  timesteps_since_restore: 1538000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1538000\n",
      "  training_iteration: 769\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2827 s, 769 iter, 1538000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.584623095682781\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 770\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.253\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9105229377746582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.927900878712535e-05\n",
      "        policy_loss: -0.0005461464170366526\n",
      "        total_loss: 0.3640558123588562\n",
      "        vf_explained_var: -0.02422928810119629\n",
      "        vf_loss: 0.3646019697189331\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    sample_time_ms: 4122.03\n",
      "    update_time_ms: 8.782\n",
      "  iterations_since_restore: 770\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.23333333333333\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.294974941304107\n",
      "    mean_inference_ms: 1.4512565453784827\n",
      "    mean_processing_ms: 0.9408582348800404\n",
      "  time_since_restore: 2833.6363837718964\n",
      "  time_this_iter_s: 6.341142892837524\n",
      "  time_total_s: 2833.6363837718964\n",
      "  timestamp: 1595951479\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 770\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2833 s, 770 iter, 1540000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.584623095682781\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 770\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.883\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7865175008773804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00015837588580325246\n",
      "        policy_loss: -0.0005753154982812703\n",
      "        total_loss: 0.003707763273268938\n",
      "        vf_explained_var: 0.7724772691726685\n",
      "        vf_loss: 0.0042830840684473515\n",
      "    load_time_ms: 1.739\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "    sample_time_ms: 4173.512\n",
      "    update_time_ms: 9.447\n",
      "  iterations_since_restore: 772\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.05999999999999\n",
      "    ram_util_percent: 62.94\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.294974941304106\n",
      "    mean_inference_ms: 1.4512565453784827\n",
      "    mean_processing_ms: 0.9408582348800404\n",
      "  time_since_restore: 2841.170197248459\n",
      "  time_this_iter_s: 3.868025779724121\n",
      "  time_total_s: 2841.170197248459\n",
      "  timestamp: 1595951487\n",
      "  timesteps_since_restore: 1544000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 772\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2841 s, 772 iter, 1544000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.584623095682781\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 770\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.128\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8995873332023621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5016188374138437e-05\n",
      "        policy_loss: -0.00040089129470288754\n",
      "        total_loss: 0.0009821300627663732\n",
      "        vf_explained_var: 0.8442938923835754\n",
      "        vf_loss: 0.0013830313691869378\n",
      "    load_time_ms: 1.859\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "    sample_time_ms: 4221.365\n",
      "    update_time_ms: 10.355\n",
      "  iterations_since_restore: 774\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.525\n",
      "    ram_util_percent: 62.55\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.294974941304106\n",
      "    mean_inference_ms: 1.4512565453784827\n",
      "    mean_processing_ms: 0.9408582348800404\n",
      "  time_since_restore: 2848.218767642975\n",
      "  time_this_iter_s: 3.2564189434051514\n",
      "  time_total_s: 2848.218767642975\n",
      "  timestamp: 1595951494\n",
      "  timesteps_since_restore: 1548000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 774\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2848 s, 774 iter, 1548000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.582294080344182\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 775\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 42.512\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5848400592803955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0008904740097932518\n",
      "        policy_loss: -0.0023817396722733974\n",
      "        total_loss: 0.03126485273241997\n",
      "        vf_explained_var: 0.4985138177871704\n",
      "        vf_loss: 0.0336465947329998\n",
      "    load_time_ms: 1.806\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "    sample_time_ms: 4048.74\n",
      "    update_time_ms: 10.923\n",
      "  iterations_since_restore: 776\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.14000000000001\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.309181597720693\n",
      "    mean_inference_ms: 1.4542177955682645\n",
      "    mean_processing_ms: 0.941734595459834\n",
      "  time_since_restore: 2855.380609035492\n",
      "  time_this_iter_s: 3.001849412918091\n",
      "  time_total_s: 2855.380609035492\n",
      "  timestamp: 1595951501\n",
      "  timesteps_since_restore: 1552000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 776\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2855 s, 776 iter, 1552000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.582294080344182\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 775\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.742\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8962725400924683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00020766806846950203\n",
      "        policy_loss: -0.000708732579369098\n",
      "        total_loss: 0.0005077876849099994\n",
      "        vf_explained_var: 0.9490484595298767\n",
      "        vf_loss: 0.0012165126390755177\n",
      "    load_time_ms: 2.133\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "    sample_time_ms: 3989.182\n",
      "    update_time_ms: 10.753\n",
      "  iterations_since_restore: 778\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.95\n",
      "    ram_util_percent: 62.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.309181597720693\n",
      "    mean_inference_ms: 1.4542177955682645\n",
      "    mean_processing_ms: 0.941734595459834\n",
      "  time_since_restore: 2863.3801765441895\n",
      "  time_this_iter_s: 4.266136646270752\n",
      "  time_total_s: 2863.3801765441895\n",
      "  timestamp: 1595951509\n",
      "  timesteps_since_restore: 1556000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 778\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2863 s, 778 iter, 1556000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.566911483040737\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 780\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 47.299\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8710156679153442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.366880890913308e-05\n",
      "        policy_loss: -0.0002452587941661477\n",
      "        total_loss: 0.3684178292751312\n",
      "        vf_explained_var: -0.008362770080566406\n",
      "        vf_loss: 0.36866310238838196\n",
      "    load_time_ms: 2.037\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    sample_time_ms: 3849.846\n",
      "    update_time_ms: 10.443\n",
      "  iterations_since_restore: 780\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.6375\n",
      "    ram_util_percent: 62.525\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.323021285454294\n",
      "    mean_inference_ms: 1.457091716180932\n",
      "    mean_processing_ms: 0.9425940228369067\n",
      "  time_since_restore: 2872.9137551784515\n",
      "  time_this_iter_s: 5.557780504226685\n",
      "  time_total_s: 2872.9137551784515\n",
      "  timestamp: 1595951519\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 780\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2872 s, 780 iter, 1560000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.56691148304074\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 780\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.013\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7843368053436279\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00018807119340635836\n",
      "        policy_loss: -0.0007194423815235496\n",
      "        total_loss: 0.002938368357717991\n",
      "        vf_explained_var: 0.8643329739570618\n",
      "        vf_loss: 0.003657823195680976\n",
      "    load_time_ms: 2.178\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "    sample_time_ms: 3939.19\n",
      "    update_time_ms: 10.944\n",
      "  iterations_since_restore: 782\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.68333333333334\n",
      "    ram_util_percent: 62.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.323021285454294\n",
      "    mean_inference_ms: 1.4570917161809323\n",
      "    mean_processing_ms: 0.9425940228369066\n",
      "  time_since_restore: 2881.353513240814\n",
      "  time_this_iter_s: 3.9103305339813232\n",
      "  time_total_s: 2881.353513240814\n",
      "  timestamp: 1595951527\n",
      "  timesteps_since_restore: 1564000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 782\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2881 s, 782 iter, 1564000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.56691148304074\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 780\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.881\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8858460783958435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.184354424476624e-05\n",
      "        policy_loss: -0.001134540536440909\n",
      "        total_loss: 7.56883600843139e-05\n",
      "        vf_explained_var: 0.8650552034378052\n",
      "        vf_loss: 0.0012102313339710236\n",
      "    load_time_ms: 2.203\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "    sample_time_ms: 4130.494\n",
      "    update_time_ms: 10.236\n",
      "  iterations_since_restore: 784\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.42857142857143\n",
      "    ram_util_percent: 62.54285714285715\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.323021285454294\n",
      "    mean_inference_ms: 1.4570917161809323\n",
      "    mean_processing_ms: 0.9425940228369066\n",
      "  time_since_restore: 2890.3197989463806\n",
      "  time_this_iter_s: 4.760246753692627\n",
      "  time_total_s: 2890.3197989463806\n",
      "  timestamp: 1595951536\n",
      "  timesteps_since_restore: 1568000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 784\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2890 s, 784 iter, 1568000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.5651380036734\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 785\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 53.951\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8936986923217773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.282672544941306e-05\n",
      "        policy_loss: -0.0002434234629618004\n",
      "        total_loss: 0.364823579788208\n",
      "        vf_explained_var: -0.004736423492431641\n",
      "        vf_loss: 0.3650670051574707\n",
      "    load_time_ms: 2.542\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    sample_time_ms: 4314.122\n",
      "    update_time_ms: 10.877\n",
      "  iterations_since_restore: 785\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.875\n",
      "    ram_util_percent: 62.575\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.337179211109388\n",
      "    mean_inference_ms: 1.460026022741222\n",
      "    mean_processing_ms: 0.9434776884789712\n",
      "  time_since_restore: 2896.3780460357666\n",
      "  time_this_iter_s: 6.058247089385986\n",
      "  time_total_s: 2896.3780460357666\n",
      "  timestamp: 1595951542\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 785\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2896 s, 785 iter, 1570000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.565138003673404\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 785\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.29\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.757593035697937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4876872228342108e-05\n",
      "        policy_loss: -6.848478369647637e-05\n",
      "        total_loss: 0.004142005927860737\n",
      "        vf_explained_var: 0.818937361240387\n",
      "        vf_loss: 0.004210484679788351\n",
      "    load_time_ms: 2.341\n",
      "    num_steps_sampled: 1574000\n",
      "    num_steps_trained: 1574000\n",
      "    sample_time_ms: 4449.745\n",
      "    update_time_ms: 11.509\n",
      "  iterations_since_restore: 787\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.71666666666667\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3371792111093885\n",
      "    mean_inference_ms: 1.4600260227412218\n",
      "    mean_processing_ms: 0.9434776884789714\n",
      "  time_since_restore: 2904.443194627762\n",
      "  time_this_iter_s: 3.749267578125\n",
      "  time_total_s: 2904.443194627762\n",
      "  timestamp: 1595951550\n",
      "  timesteps_since_restore: 1574000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1574000\n",
      "  training_iteration: 787\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2904 s, 787 iter, 1574000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.565138003673404\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 785\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 60.553\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8638206720352173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.89392288425006e-05\n",
      "        policy_loss: -0.001069793477654457\n",
      "        total_loss: 0.0004281539877410978\n",
      "        vf_explained_var: 0.8456302881240845\n",
      "        vf_loss: 0.001497930265031755\n",
      "    load_time_ms: 2.633\n",
      "    num_steps_sampled: 1578000\n",
      "    num_steps_trained: 1578000\n",
      "    sample_time_ms: 4434.747\n",
      "    update_time_ms: 11.723\n",
      "  iterations_since_restore: 789\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.41666666666666\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3371792111093885\n",
      "    mean_inference_ms: 1.4600260227412218\n",
      "    mean_processing_ms: 0.9434776884789714\n",
      "  time_since_restore: 2912.6601791381836\n",
      "  time_this_iter_s: 4.072441816329956\n",
      "  time_total_s: 2912.6601791381836\n",
      "  timestamp: 1595951559\n",
      "  timesteps_since_restore: 1578000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1578000\n",
      "  training_iteration: 789\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2912 s, 789 iter, 1578000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.568058865578278\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 790\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 58.498\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8485588431358337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.709856850444339e-05\n",
      "        policy_loss: -0.00016671371122356504\n",
      "        total_loss: 0.3745770752429962\n",
      "        vf_explained_var: -0.006263852119445801\n",
      "        vf_loss: 0.3747437298297882\n",
      "    load_time_ms: 2.581\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    sample_time_ms: 4462.202\n",
      "    update_time_ms: 12.521\n",
      "  iterations_since_restore: 790\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.27499999999999\n",
      "    ram_util_percent: 62.5125\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351303531114739\n",
      "    mean_inference_ms: 1.4629428735978736\n",
      "    mean_processing_ms: 0.9443702481484821\n",
      "  time_since_restore: 2918.4682216644287\n",
      "  time_this_iter_s: 5.808042526245117\n",
      "  time_total_s: 2918.4682216644287\n",
      "  timestamp: 1595951565\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 790\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2918 s, 790 iter, 1580000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-52-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.568058865578273\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 790\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 60.704\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7355381846427917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005050142644904554\n",
      "        policy_loss: -0.002752973698079586\n",
      "        total_loss: 0.0014596352120861411\n",
      "        vf_explained_var: 0.7812657356262207\n",
      "        vf_loss: 0.004212615545839071\n",
      "    load_time_ms: 2.574\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    sample_time_ms: 4464.284\n",
      "    update_time_ms: 11.834\n",
      "  iterations_since_restore: 792\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.6\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.35130353111474\n",
      "    mean_inference_ms: 1.462942873597874\n",
      "    mean_processing_ms: 0.9443702481484821\n",
      "  time_since_restore: 2926.9575667381287\n",
      "  time_this_iter_s: 4.3872599601745605\n",
      "  time_total_s: 2926.9575667381287\n",
      "  timestamp: 1595951573\n",
      "  timesteps_since_restore: 1584000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 792\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2926 s, 792 iter, 1584000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.568058865578273\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 790\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 59.78\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8347997069358826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.475527915317798e-05\n",
      "        policy_loss: 2.9659271604032256e-05\n",
      "        total_loss: 0.0014853152679279447\n",
      "        vf_explained_var: 0.7587697505950928\n",
      "        vf_loss: 0.0014556607929989696\n",
      "    load_time_ms: 2.564\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "    sample_time_ms: 4391.505\n",
      "    update_time_ms: 12.703\n",
      "  iterations_since_restore: 794\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.61666666666667\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.35130353111474\n",
      "    mean_inference_ms: 1.462942873597874\n",
      "    mean_processing_ms: 0.9443702481484821\n",
      "  time_since_restore: 2935.1959443092346\n",
      "  time_this_iter_s: 4.27327561378479\n",
      "  time_total_s: 2935.1959443092346\n",
      "  timestamp: 1595951581\n",
      "  timesteps_since_restore: 1588000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 794\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2935 s, 794 iter, 1588000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.567443561639214\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 795\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 56.69\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.825444757938385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0464966180734336e-05\n",
      "        policy_loss: -0.0001082553862943314\n",
      "        total_loss: 0.3739953637123108\n",
      "        vf_explained_var: 0.010983407497406006\n",
      "        vf_loss: 0.3741036355495453\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    sample_time_ms: 4354.35\n",
      "    update_time_ms: 12.696\n",
      "  iterations_since_restore: 795\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.58749999999999\n",
      "    ram_util_percent: 62.537499999999994\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.365633838039775\n",
      "    mean_inference_ms: 1.4659064164122682\n",
      "    mean_processing_ms: 0.9452755254787897\n",
      "  time_since_restore: 2940.842385530472\n",
      "  time_this_iter_s: 5.646441221237183\n",
      "  time_total_s: 2940.842385530472\n",
      "  timestamp: 1595951587\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 795\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2940 s, 795 iter, 1590000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.567443561639216\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 795\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 60.136\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7278478741645813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.211061503970996e-05\n",
      "        policy_loss: -0.00025530625134706497\n",
      "        total_loss: 0.0037092608399689198\n",
      "        vf_explained_var: 0.8225615620613098\n",
      "        vf_loss: 0.00396459037438035\n",
      "    load_time_ms: 2.539\n",
      "    num_steps_sampled: 1594000\n",
      "    num_steps_trained: 1594000\n",
      "    sample_time_ms: 4422.966\n",
      "    update_time_ms: 12.773\n",
      "  iterations_since_restore: 797\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.21666666666667\n",
      "    ram_util_percent: 62.61666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.365633838039774\n",
      "    mean_inference_ms: 1.4659064164122682\n",
      "    mean_processing_ms: 0.9452755254787897\n",
      "  time_since_restore: 2949.6300115585327\n",
      "  time_this_iter_s: 4.553477764129639\n",
      "  time_total_s: 2949.6300115585327\n",
      "  timestamp: 1595951596\n",
      "  timesteps_since_restore: 1594000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1594000\n",
      "  training_iteration: 797\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2949 s, 797 iter, 1594000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.567443561639216\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 795\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.431\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8620278239250183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.464579488849267e-05\n",
      "        policy_loss: -0.0017383265076205134\n",
      "        total_loss: -0.0007084207609295845\n",
      "        vf_explained_var: 0.9042085409164429\n",
      "        vf_loss: 0.0010298991110175848\n",
      "    load_time_ms: 2.227\n",
      "    num_steps_sampled: 1598000\n",
      "    num_steps_trained: 1598000\n",
      "    sample_time_ms: 4361.635\n",
      "    update_time_ms: 11.813\n",
      "  iterations_since_restore: 799\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.58000000000001\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.365633838039774\n",
      "    mean_inference_ms: 1.4659064164122682\n",
      "    mean_processing_ms: 0.9452755254787897\n",
      "  time_since_restore: 2957.103091478348\n",
      "  time_this_iter_s: 3.5451529026031494\n",
      "  time_total_s: 2957.103091478348\n",
      "  timestamp: 1595951603\n",
      "  timesteps_since_restore: 1598000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1598000\n",
      "  training_iteration: 799\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2957 s, 799 iter, 1598000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.582616834113573\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 800\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 51.667\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.84502774477005\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.030734559521079e-05\n",
      "        policy_loss: 0.00012300968228373677\n",
      "        total_loss: 0.37033456563949585\n",
      "        vf_explained_var: 0.010179102420806885\n",
      "        vf_loss: 0.37021157145500183\n",
      "    load_time_ms: 2.265\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    sample_time_ms: 4344.498\n",
      "    update_time_ms: 12.201\n",
      "  iterations_since_restore: 800\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.9\n",
      "    ram_util_percent: 62.6875\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.379624388212389\n",
      "    mean_inference_ms: 1.4687865997817153\n",
      "    mean_processing_ms: 0.9461614892723995\n",
      "  time_since_restore: 2962.761245727539\n",
      "  time_this_iter_s: 5.658154249191284\n",
      "  time_total_s: 2962.761245727539\n",
      "  timestamp: 1595951609\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 800\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2962 s, 800 iter, 1600000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.582616834113573\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 800\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.702\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6909189224243164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004790943639818579\n",
      "        policy_loss: -0.0014241590397432446\n",
      "        total_loss: 0.0031255832873284817\n",
      "        vf_explained_var: 0.7511950731277466\n",
      "        vf_loss: 0.004549745470285416\n",
      "    load_time_ms: 2.241\n",
      "    num_steps_sampled: 1604000\n",
      "    num_steps_trained: 1604000\n",
      "    sample_time_ms: 4321.298\n",
      "    update_time_ms: 12.205\n",
      "  iterations_since_restore: 802\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.10000000000001\n",
      "    ram_util_percent: 62.73333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.379624388212388\n",
      "    mean_inference_ms: 1.4687865997817158\n",
      "    mean_processing_ms: 0.9461614892723996\n",
      "  time_since_restore: 2970.9724650382996\n",
      "  time_this_iter_s: 3.9403069019317627\n",
      "  time_total_s: 2970.9724650382996\n",
      "  timestamp: 1595951617\n",
      "  timesteps_since_restore: 1604000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1604000\n",
      "  training_iteration: 802\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2970 s, 802 iter, 1604000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.582616834113573\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 800\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 50.386\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8085882663726807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0544826838886365e-05\n",
      "        policy_loss: -0.0003623313969001174\n",
      "        total_loss: 0.0010959701612591743\n",
      "        vf_explained_var: 0.7545771598815918\n",
      "        vf_loss: 0.0014583037700504065\n",
      "    load_time_ms: 2.214\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "    sample_time_ms: 4359.751\n",
      "    update_time_ms: 11.146\n",
      "  iterations_since_restore: 804\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.66666666666667\n",
      "    ram_util_percent: 62.833333333333336\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.379624388212388\n",
      "    mean_inference_ms: 1.4687865997817158\n",
      "    mean_processing_ms: 0.9461614892723996\n",
      "  time_since_restore: 2979.6051774024963\n",
      "  time_this_iter_s: 4.813788890838623\n",
      "  time_total_s: 2979.6051774024963\n",
      "  timestamp: 1595951626\n",
      "  timesteps_since_restore: 1608000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 804\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2979 s, 804 iter, 1608000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.61542246652297\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 805\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 48.604\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47778332233428955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006033839308656752\n",
      "        policy_loss: -0.001347649609670043\n",
      "        total_loss: 0.03459220752120018\n",
      "        vf_explained_var: 0.5296287536621094\n",
      "        vf_loss: 0.03593984991312027\n",
      "    load_time_ms: 2.075\n",
      "    num_steps_sampled: 1612000\n",
      "    num_steps_trained: 1612000\n",
      "    sample_time_ms: 4192.178\n",
      "    update_time_ms: 10.271\n",
      "  iterations_since_restore: 806\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.67999999999999\n",
      "    ram_util_percent: 62.81999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.393727428696852\n",
      "    mean_inference_ms: 1.4716878742599158\n",
      "    mean_processing_ms: 0.9470449464796024\n",
      "  time_since_restore: 2987.7738234996796\n",
      "  time_this_iter_s: 3.2518866062164307\n",
      "  time_total_s: 2987.7738234996796\n",
      "  timestamp: 1595951634\n",
      "  timesteps_since_restore: 1612000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1612000\n",
      "  training_iteration: 806\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2987 s, 806 iter, 1612000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.61542246652297\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 805\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.787118673324585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.007193274446763e-05\n",
      "        policy_loss: -0.0003584318037610501\n",
      "        total_loss: 0.0011401643278077245\n",
      "        vf_explained_var: 0.9106580018997192\n",
      "        vf_loss: 0.001498601515777409\n",
      "    load_time_ms: 1.858\n",
      "    num_steps_sampled: 1616000\n",
      "    num_steps_trained: 1616000\n",
      "    sample_time_ms: 4103.235\n",
      "    update_time_ms: 9.363\n",
      "  iterations_since_restore: 808\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.05\n",
      "    ram_util_percent: 62.616666666666674\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.393727428696852\n",
      "    mean_inference_ms: 1.4716878742599158\n",
      "    mean_processing_ms: 0.9470449464796024\n",
      "  time_since_restore: 2995.2843656539917\n",
      "  time_this_iter_s: 3.815460443496704\n",
      "  time_total_s: 2995.2843656539917\n",
      "  timestamp: 1595951642\n",
      "  timesteps_since_restore: 1616000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1616000\n",
      "  training_iteration: 808\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 2995 s, 808 iter, 1616000 ts, 11.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.628910388933141\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 810\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 44.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8010601997375488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00018212568829767406\n",
      "        policy_loss: -0.00042252111597917974\n",
      "        total_loss: 0.3720570504665375\n",
      "        vf_explained_var: 0.01238483190536499\n",
      "        vf_loss: 0.37247949838638306\n",
      "    load_time_ms: 2.007\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    sample_time_ms: 4103.945\n",
      "    update_time_ms: 9.442\n",
      "  iterations_since_restore: 810\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.41428571428573\n",
      "    ram_util_percent: 62.95714285714285\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.40797363033784\n",
      "    mean_inference_ms: 1.4746167969880772\n",
      "    mean_processing_ms: 0.947949554196892\n",
      "  time_since_restore: 3004.510247707367\n",
      "  time_this_iter_s: 5.176299571990967\n",
      "  time_total_s: 3004.510247707367\n",
      "  timestamp: 1595951651\n",
      "  timesteps_since_restore: 1620000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 810\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3004 s, 810 iter, 1620000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.628910388933143\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 810\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 45.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6615148782730103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1686324796755798e-05\n",
      "        policy_loss: -3.399276829441078e-05\n",
      "        total_loss: 0.0045824795961380005\n",
      "        vf_explained_var: 0.782755434513092\n",
      "        vf_loss: 0.004616468213498592\n",
      "    load_time_ms: 2.001\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1624000\n",
      "    sample_time_ms: 4059.533\n",
      "    update_time_ms: 9.597\n",
      "  iterations_since_restore: 812\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.28333333333332\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.407973630337839\n",
      "    mean_inference_ms: 1.4746167969880772\n",
      "    mean_processing_ms: 0.947949554196892\n",
      "  time_since_restore: 3012.2941439151764\n",
      "  time_this_iter_s: 4.279787540435791\n",
      "  time_total_s: 3012.2941439151764\n",
      "  timestamp: 1595951659\n",
      "  timesteps_since_restore: 1624000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 812\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3012 s, 812 iter, 1624000 ts, 11.6 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.628910388933143\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 810\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 40.46\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7822807431221008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5999665012932383e-05\n",
      "        policy_loss: -0.00022414397972170264\n",
      "        total_loss: 0.0012488379143178463\n",
      "        vf_explained_var: 0.837023138999939\n",
      "        vf_loss: 0.0014729811809957027\n",
      "    load_time_ms: 1.89\n",
      "    num_steps_sampled: 1628000\n",
      "    num_steps_trained: 1628000\n",
      "    sample_time_ms: 3960.09\n",
      "    update_time_ms: 10.304\n",
      "  iterations_since_restore: 814\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8\n",
      "    ram_util_percent: 62.78333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.407973630337839\n",
      "    mean_inference_ms: 1.4746167969880772\n",
      "    mean_processing_ms: 0.947949554196892\n",
      "  time_since_restore: 3019.882274389267\n",
      "  time_this_iter_s: 4.112872123718262\n",
      "  time_total_s: 3019.882274389267\n",
      "  timestamp: 1595951666\n",
      "  timesteps_since_restore: 1628000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1628000\n",
      "  training_iteration: 814\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3019 s, 814 iter, 1628000 ts, 11.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.664631356444001\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 815\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.613\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7899147868156433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.9861340812640265e-05\n",
      "        policy_loss: -0.00020656442211475223\n",
      "        total_loss: 0.37879469990730286\n",
      "        vf_explained_var: -0.005814909934997559\n",
      "        vf_loss: 0.379001259803772\n",
      "    load_time_ms: 1.953\n",
      "    num_steps_sampled: 1630000\n",
      "    num_steps_trained: 1630000\n",
      "    sample_time_ms: 3988.526\n",
      "    update_time_ms: 10.249\n",
      "  iterations_since_restore: 815\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.3\n",
      "    ram_util_percent: 62.75\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.421801927308799\n",
      "    mean_inference_ms: 1.477442961498197\n",
      "    mean_processing_ms: 0.9488291142800724\n",
      "  time_since_restore: 3025.100650548935\n",
      "  time_this_iter_s: 5.218376159667969\n",
      "  time_total_s: 3025.100650548935\n",
      "  timestamp: 1595951672\n",
      "  timesteps_since_restore: 1630000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1630000\n",
      "  training_iteration: 815\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3025 s, 815 iter, 1630000 ts, 11.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.664631356444003\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 815\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.202\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6454612016677856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007124139228835702\n",
      "        policy_loss: -0.0020737105514854193\n",
      "        total_loss: 0.0027733026072382927\n",
      "        vf_explained_var: 0.7647473812103271\n",
      "        vf_loss: 0.004847025964409113\n",
      "    load_time_ms: 1.907\n",
      "    num_steps_sampled: 1634000\n",
      "    num_steps_trained: 1634000\n",
      "    sample_time_ms: 4097.735\n",
      "    update_time_ms: 10.807\n",
      "  iterations_since_restore: 817\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.32\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.421801927308799\n",
      "    mean_inference_ms: 1.4774429614981965\n",
      "    mean_processing_ms: 0.9488291142800727\n",
      "  time_since_restore: 3033.14870595932\n",
      "  time_this_iter_s: 3.6292359828948975\n",
      "  time_total_s: 3033.14870595932\n",
      "  timestamp: 1595951680\n",
      "  timesteps_since_restore: 1634000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1634000\n",
      "  training_iteration: 817\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3033 s, 817 iter, 1634000 ts, 11.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.331507497431044\n",
      "  episode_reward_mean: 11.664631356444003\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 815\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.132\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.755580484867096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.749173841555603e-06\n",
      "        policy_loss: -2.9854298190912232e-05\n",
      "        total_loss: 0.001552737201564014\n",
      "        vf_explained_var: 0.8576740026473999\n",
      "        vf_loss: 0.00158259691670537\n",
      "    load_time_ms: 1.969\n",
      "    num_steps_sampled: 1638000\n",
      "    num_steps_trained: 1638000\n",
      "    sample_time_ms: 4298.409\n",
      "    update_time_ms: 11.019\n",
      "  iterations_since_restore: 819\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85714285714286\n",
      "    ram_util_percent: 62.88571428571428\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.421801927308799\n",
      "    mean_inference_ms: 1.4774429614981965\n",
      "    mean_processing_ms: 0.9488291142800727\n",
      "  time_since_restore: 3042.997892141342\n",
      "  time_this_iter_s: 5.0654168128967285\n",
      "  time_total_s: 3042.997892141342\n",
      "  timestamp: 1595951690\n",
      "  timesteps_since_restore: 1638000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1638000\n",
      "  training_iteration: 819\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3042 s, 819 iter, 1638000 ts, 11.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-54-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.69540982231876\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 820\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.649\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43989962339401245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0017348212422803044\n",
      "        policy_loss: -0.002997379284352064\n",
      "        total_loss: 0.03420510143041611\n",
      "        vf_explained_var: 0.5118770599365234\n",
      "        vf_loss: 0.03720248490571976\n",
      "    load_time_ms: 1.712\n",
      "    num_steps_sampled: 1642000\n",
      "    num_steps_trained: 1642000\n",
      "    sample_time_ms: 4108.667\n",
      "    update_time_ms: 9.933\n",
      "  iterations_since_restore: 821\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.625\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4342577544854445\n",
      "    mean_inference_ms: 1.479963613562922\n",
      "    mean_processing_ms: 0.9495724770369321\n",
      "  time_since_restore: 3049.717670440674\n",
      "  time_this_iter_s: 2.433605432510376\n",
      "  time_total_s: 3049.717670440674\n",
      "  timestamp: 1595951696\n",
      "  timesteps_since_restore: 1642000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1642000\n",
      "  training_iteration: 821\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3049 s, 821 iter, 1642000 ts, 11.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.69540982231876\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 820\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.089\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7947023510932922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00019503245130181313\n",
      "        policy_loss: -0.00226816744543612\n",
      "        total_loss: -0.0012293376494199038\n",
      "        vf_explained_var: 0.90693598985672\n",
      "        vf_loss: 0.0010388264199718833\n",
      "    load_time_ms: 1.517\n",
      "    num_steps_sampled: 1648000\n",
      "    num_steps_trained: 1648000\n",
      "    sample_time_ms: 3607.699\n",
      "    update_time_ms: 7.656\n",
      "  iterations_since_restore: 824\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.03333333333335\n",
      "    ram_util_percent: 62.93333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4342577544854445\n",
      "    mean_inference_ms: 1.479963613562922\n",
      "    mean_processing_ms: 0.9495724770369321\n",
      "  time_since_restore: 3056.486284971237\n",
      "  time_this_iter_s: 2.3242621421813965\n",
      "  time_total_s: 3056.486284971237\n",
      "  timestamp: 1595951703\n",
      "  timesteps_since_restore: 1648000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1648000\n",
      "  training_iteration: 824\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3056 s, 824 iter, 1648000 ts, 11.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.718974762707292\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 825\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43885260820388794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003718733205460012\n",
      "        policy_loss: -0.0010465106461197138\n",
      "        total_loss: 0.03692163527011871\n",
      "        vf_explained_var: 0.5260372757911682\n",
      "        vf_loss: 0.03796814754605293\n",
      "    load_time_ms: 1.289\n",
      "    num_steps_sampled: 1652000\n",
      "    num_steps_trained: 1652000\n",
      "    sample_time_ms: 3231.064\n",
      "    update_time_ms: 6.48\n",
      "  iterations_since_restore: 826\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.3\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.444983023603552\n",
      "    mean_inference_ms: 1.4820974695119378\n",
      "    mean_processing_ms: 0.9502001134443154\n",
      "  time_since_restore: 3062.3116126060486\n",
      "  time_this_iter_s: 2.3633170127868652\n",
      "  time_total_s: 3062.3116126060486\n",
      "  timestamp: 1595951709\n",
      "  timesteps_since_restore: 1652000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1652000\n",
      "  training_iteration: 826\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3062 s, 826 iter, 1652000 ts, 11.7 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.718974762707292\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 825\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.397\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7932211756706238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002194201952079311\n",
      "        policy_loss: -0.0016451682895421982\n",
      "        total_loss: -0.00047697831178084016\n",
      "        vf_explained_var: 0.9136936664581299\n",
      "        vf_loss: 0.0011682033073157072\n",
      "    load_time_ms: 1.034\n",
      "    num_steps_sampled: 1658000\n",
      "    num_steps_trained: 1658000\n",
      "    sample_time_ms: 2552.065\n",
      "    update_time_ms: 5.783\n",
      "  iterations_since_restore: 829\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.2\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.444983023603552\n",
      "    mean_inference_ms: 1.4820974695119378\n",
      "    mean_processing_ms: 0.9502001134443154\n",
      "  time_since_restore: 3068.9431014060974\n",
      "  time_this_iter_s: 2.1264424324035645\n",
      "  time_total_s: 3068.9431014060974\n",
      "  timestamp: 1595951716\n",
      "  timesteps_since_restore: 1658000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1658000\n",
      "  training_iteration: 829\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3068 s, 829 iter, 1658000 ts, 11.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.726963797680076\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 830\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.455\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4385959804058075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00041444654925726354\n",
      "        policy_loss: 0.0003838019329123199\n",
      "        total_loss: 0.038017213344573975\n",
      "        vf_explained_var: 0.5189260840415955\n",
      "        vf_loss: 0.037633415311574936\n",
      "    load_time_ms: 1.028\n",
      "    num_steps_sampled: 1662000\n",
      "    num_steps_trained: 1662000\n",
      "    sample_time_ms: 2423.167\n",
      "    update_time_ms: 5.063\n",
      "  iterations_since_restore: 831\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.76666666666667\n",
      "    ram_util_percent: 62.86666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.45054943181992\n",
      "    mean_inference_ms: 1.4831526539161168\n",
      "    mean_processing_ms: 0.9505101152149171\n",
      "  time_since_restore: 3074.3570873737335\n",
      "  time_this_iter_s: 2.1671245098114014\n",
      "  time_total_s: 3074.3570873737335\n",
      "  timestamp: 1595951721\n",
      "  timesteps_since_restore: 1662000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1662000\n",
      "  training_iteration: 831\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3074 s, 831 iter, 1662000 ts, 11.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.726963797680076\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 830\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.325\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7660039067268372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.543684427626431e-05\n",
      "        policy_loss: -0.0009494552505202591\n",
      "        total_loss: 0.000337472913088277\n",
      "        vf_explained_var: 0.808148205280304\n",
      "        vf_loss: 0.0012869187630712986\n",
      "    load_time_ms: 1.645\n",
      "    num_steps_sampled: 1668000\n",
      "    num_steps_trained: 1668000\n",
      "    sample_time_ms: 2385.684\n",
      "    update_time_ms: 5.065\n",
      "  iterations_since_restore: 834\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.45054943181992\n",
      "    mean_inference_ms: 1.4831526539161168\n",
      "    mean_processing_ms: 0.9505101152149171\n",
      "  time_since_restore: 3080.767682313919\n",
      "  time_this_iter_s: 2.1061856746673584\n",
      "  time_total_s: 3080.767682313919\n",
      "  timestamp: 1595951728\n",
      "  timesteps_since_restore: 1668000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1668000\n",
      "  training_iteration: 834\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3080 s, 834 iter, 1668000 ts, 11.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.765212796483752\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 835\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.936\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4301244616508484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00114919594489038\n",
      "        policy_loss: -0.0025810012593865395\n",
      "        total_loss: 0.034610502421855927\n",
      "        vf_explained_var: 0.5175926685333252\n",
      "        vf_loss: 0.03719150647521019\n",
      "    load_time_ms: 1.662\n",
      "    num_steps_sampled: 1672000\n",
      "    num_steps_trained: 1672000\n",
      "    sample_time_ms: 2342.151\n",
      "    update_time_ms: 5.006\n",
      "  iterations_since_restore: 836\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.96666666666667\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.45453893395452\n",
      "    mean_inference_ms: 1.4838744712691516\n",
      "    mean_processing_ms: 0.950726757283655\n",
      "  time_since_restore: 3086.1163427829742\n",
      "  time_this_iter_s: 2.0768558979034424\n",
      "  time_total_s: 3086.1163427829742\n",
      "  timestamp: 1595951733\n",
      "  timesteps_since_restore: 1672000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1672000\n",
      "  training_iteration: 836\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3086 s, 836 iter, 1672000 ts, 11.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.765212796483752\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 835\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.783\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7371957898139954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010401609324617311\n",
      "        policy_loss: -2.0381927242851816e-05\n",
      "        total_loss: 0.0016733207739889622\n",
      "        vf_explained_var: 0.8387686014175415\n",
      "        vf_loss: 0.0016937132459133863\n",
      "    load_time_ms: 1.699\n",
      "    num_steps_sampled: 1676000\n",
      "    num_steps_trained: 1676000\n",
      "    sample_time_ms: 2440.769\n",
      "    update_time_ms: 4.007\n",
      "  iterations_since_restore: 838\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.575\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.45453893395452\n",
      "    mean_inference_ms: 1.4838744712691516\n",
      "    mean_processing_ms: 0.950726757283655\n",
      "  time_since_restore: 3091.5977528095245\n",
      "  time_this_iter_s: 2.3162245750427246\n",
      "  time_total_s: 3091.5977528095245\n",
      "  timestamp: 1595951738\n",
      "  timesteps_since_restore: 1676000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1676000\n",
      "  training_iteration: 838\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3091 s, 838 iter, 1676000 ts, 11.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.785649869058576\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 840\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.765042245388031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.122168709523976e-05\n",
      "        policy_loss: 3.829383786069229e-05\n",
      "        total_loss: 0.38578033447265625\n",
      "        vf_explained_var: 0.004981040954589844\n",
      "        vf_loss: 0.38574206829071045\n",
      "    load_time_ms: 1.697\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    sample_time_ms: 2486.574\n",
      "    update_time_ms: 3.98\n",
      "  iterations_since_restore: 840\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.58\n",
      "    ram_util_percent: 62.879999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4574977930779305\n",
      "    mean_inference_ms: 1.4843797542249642\n",
      "    mean_processing_ms: 0.9508796360774191\n",
      "  time_since_restore: 3097.4242606163025\n",
      "  time_this_iter_s: 3.4975287914276123\n",
      "  time_total_s: 3097.4242606163025\n",
      "  timestamp: 1595951744\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 840\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3097 s, 840 iter, 1680000 ts, 11.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.78564986905858\n",
      "  episode_reward_min: 10.821563491381733\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 840\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.762\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6589390635490417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1277537004825717e-07\n",
      "        policy_loss: -1.482677453168435e-05\n",
      "        total_loss: 0.0035374618601053953\n",
      "        vf_explained_var: 0.8388810157775879\n",
      "        vf_loss: 0.0035522817634046078\n",
      "    load_time_ms: 1.703\n",
      "    num_steps_sampled: 1684000\n",
      "    num_steps_trained: 1684000\n",
      "    sample_time_ms: 2554.57\n",
      "    update_time_ms: 4.019\n",
      "  iterations_since_restore: 842\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.93333333333334\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.457497793077931\n",
      "    mean_inference_ms: 1.4843797542249637\n",
      "    mean_processing_ms: 0.9508796360774191\n",
      "  time_since_restore: 3102.451593399048\n",
      "  time_this_iter_s: 2.382159948348999\n",
      "  time_total_s: 3102.451593399048\n",
      "  timestamp: 1595951749\n",
      "  timesteps_since_restore: 1684000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1684000\n",
      "  training_iteration: 842\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3102 s, 842 iter, 1684000 ts, 11.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-55-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.817897617285\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 845\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.336\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7411732077598572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012112906551919878\n",
      "        policy_loss: -0.00022042608179617673\n",
      "        total_loss: 0.38966649770736694\n",
      "        vf_explained_var: 0.018578946590423584\n",
      "        vf_loss: 0.38988691568374634\n",
      "    load_time_ms: 1.158\n",
      "    num_steps_sampled: 1690000\n",
      "    num_steps_trained: 1690000\n",
      "    sample_time_ms: 2578.183\n",
      "    update_time_ms: 4.276\n",
      "  iterations_since_restore: 845\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.79999999999999\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459266564707167\n",
      "    mean_inference_ms: 1.4846291381851608\n",
      "    mean_processing_ms: 0.9509437640837863\n",
      "  time_since_restore: 3110.215690135956\n",
      "  time_this_iter_s: 3.445665121078491\n",
      "  time_total_s: 3110.215690135956\n",
      "  timestamp: 1595951757\n",
      "  timesteps_since_restore: 1690000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1690000\n",
      "  training_iteration: 845\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3110 s, 845 iter, 1690000 ts, 11.8 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.817897617284999\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 845\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.544\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7060262560844421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.668538869940676e-05\n",
      "        policy_loss: 0.00015070724475663155\n",
      "        total_loss: 0.002102281665429473\n",
      "        vf_explained_var: 0.8324074149131775\n",
      "        vf_loss: 0.0019515688763931394\n",
      "    load_time_ms: 1.176\n",
      "    num_steps_sampled: 1696000\n",
      "    num_steps_trained: 1696000\n",
      "    sample_time_ms: 2471.787\n",
      "    update_time_ms: 4.504\n",
      "  iterations_since_restore: 848\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.95\n",
      "    ram_util_percent: 62.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459266564707168\n",
      "    mean_inference_ms: 1.484629138185161\n",
      "    mean_processing_ms: 0.9509437640837863\n",
      "  time_since_restore: 3116.7169859409332\n",
      "  time_this_iter_s: 2.2579281330108643\n",
      "  time_total_s: 3116.7169859409332\n",
      "  timestamp: 1595951764\n",
      "  timesteps_since_restore: 1696000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1696000\n",
      "  training_iteration: 848\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3116 s, 848 iter, 1696000 ts, 11.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.862397866740116\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 850\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.294\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7466121912002563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.791175499325618e-05\n",
      "        policy_loss: -0.0002480907423887402\n",
      "        total_loss: 0.391178160905838\n",
      "        vf_explained_var: -0.007870674133300781\n",
      "        vf_loss: 0.3914262056350708\n",
      "    load_time_ms: 1.158\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    sample_time_ms: 2475.501\n",
      "    update_time_ms: 4.94\n",
      "  iterations_since_restore: 850\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.040000000000006\n",
      "    ram_util_percent: 62.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459727758161196\n",
      "    mean_inference_ms: 1.484604459679266\n",
      "    mean_processing_ms: 0.9509247973162742\n",
      "  time_since_restore: 3122.606334924698\n",
      "  time_this_iter_s: 3.5448074340820312\n",
      "  time_total_s: 3122.606334924698\n",
      "  timestamp: 1595951770\n",
      "  timesteps_since_restore: 1700000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 850\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3122 s, 850 iter, 1700000 ts, 11.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.862397866740114\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 850\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.755\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7122856974601746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002790876606013626\n",
      "        policy_loss: -0.0007005395600572228\n",
      "        total_loss: 0.0010314054088667035\n",
      "        vf_explained_var: 0.8048441410064697\n",
      "        vf_loss: 0.0017319354228675365\n",
      "    load_time_ms: 1.153\n",
      "    num_steps_sampled: 1706000\n",
      "    num_steps_trained: 1706000\n",
      "    sample_time_ms: 2500.567\n",
      "    update_time_ms: 4.685\n",
      "  iterations_since_restore: 853\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.375\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459727758161196\n",
      "    mean_inference_ms: 1.4846044596792658\n",
      "    mean_processing_ms: 0.9509247973162741\n",
      "  time_since_restore: 3130.000096797943\n",
      "  time_this_iter_s: 2.4442102909088135\n",
      "  time_total_s: 3130.000096797943\n",
      "  timestamp: 1595951777\n",
      "  timesteps_since_restore: 1706000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1706000\n",
      "  training_iteration: 853\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3130 s, 853 iter, 1706000 ts, 11.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.904637740657614\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 855\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.774\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.736819326877594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010805690544657409\n",
      "        policy_loss: -0.0006734266062267125\n",
      "        total_loss: 0.3862108886241913\n",
      "        vf_explained_var: 0.015517055988311768\n",
      "        vf_loss: 0.3868843615055084\n",
      "    load_time_ms: 1.124\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "    sample_time_ms: 2520.334\n",
      "    update_time_ms: 4.9\n",
      "  iterations_since_restore: 855\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459398148760082\n",
      "    mean_inference_ms: 1.484425921808277\n",
      "    mean_processing_ms: 0.9508612292202784\n",
      "  time_since_restore: 3135.8176321983337\n",
      "  time_this_iter_s: 3.4614953994750977\n",
      "  time_total_s: 3135.8176321983337\n",
      "  timestamp: 1595951783\n",
      "  timesteps_since_restore: 1710000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 855\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3135 s, 855 iter, 1710000 ts, 11.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.904637740657614\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 855\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.714\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6949068307876587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006617454928345978\n",
      "        policy_loss: -0.0019349193898960948\n",
      "        total_loss: -0.0004601326072588563\n",
      "        vf_explained_var: 0.8065035343170166\n",
      "        vf_loss: 0.0014748178655281663\n",
      "    load_time_ms: 1.074\n",
      "    num_steps_sampled: 1716000\n",
      "    num_steps_trained: 1716000\n",
      "    sample_time_ms: 2590.716\n",
      "    update_time_ms: 4.757\n",
      "  iterations_since_restore: 858\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33333333333333\n",
      "    ram_util_percent: 62.79999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.459398148760081\n",
      "    mean_inference_ms: 1.4844259218082767\n",
      "    mean_processing_ms: 0.9508612292202783\n",
      "  time_since_restore: 3143.0214281082153\n",
      "  time_this_iter_s: 2.322171926498413\n",
      "  time_total_s: 3143.0214281082153\n",
      "  timestamp: 1595951790\n",
      "  timesteps_since_restore: 1716000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1716000\n",
      "  training_iteration: 858\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3143 s, 858 iter, 1716000 ts, 11.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.912306131211606\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 860\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.2\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.74643874168396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.758824019925669e-05\n",
      "        policy_loss: 0.0002060985571006313\n",
      "        total_loss: 0.383800745010376\n",
      "        vf_explained_var: 0.008154571056365967\n",
      "        vf_loss: 0.3835946321487427\n",
      "    load_time_ms: 1.09\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    sample_time_ms: 2572.036\n",
      "    update_time_ms: 5.008\n",
      "  iterations_since_restore: 860\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.425\n",
      "    ram_util_percent: 62.699999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4585531029185805\n",
      "    mean_inference_ms: 1.4841426933465716\n",
      "    mean_processing_ms: 0.9507670503630073\n",
      "  time_since_restore: 3148.7352323532104\n",
      "  time_this_iter_s: 3.398024559020996\n",
      "  time_total_s: 3148.7352323532104\n",
      "  timestamp: 1595951796\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 860\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3148 s, 860 iter, 1720000 ts, 11.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.912306131211606\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 860\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.22\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6738499999046326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3426422810880467e-05\n",
      "        policy_loss: -5.7386398111702874e-05\n",
      "        total_loss: 0.0015888996422290802\n",
      "        vf_explained_var: 0.7604209780693054\n",
      "        vf_loss: 0.0016462845960631967\n",
      "    load_time_ms: 1.077\n",
      "    num_steps_sampled: 1726000\n",
      "    num_steps_trained: 1726000\n",
      "    sample_time_ms: 2523.761\n",
      "    update_time_ms: 4.889\n",
      "  iterations_since_restore: 863\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.36666666666667\n",
      "    ram_util_percent: 62.70000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4585531029185805\n",
      "    mean_inference_ms: 1.4841426933465711\n",
      "    mean_processing_ms: 0.9507670503630073\n",
      "  time_since_restore: 3155.6463873386383\n",
      "  time_this_iter_s: 2.280407190322876\n",
      "  time_total_s: 3155.6463873386383\n",
      "  timestamp: 1595951803\n",
      "  timesteps_since_restore: 1726000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1726000\n",
      "  training_iteration: 863\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3155 s, 863 iter, 1726000 ts, 11.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.91616877364417\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 865\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7039953470230103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.229722490999848e-05\n",
      "        policy_loss: -2.415370909147896e-05\n",
      "        total_loss: 0.3954022526741028\n",
      "        vf_explained_var: -0.011422157287597656\n",
      "        vf_loss: 0.3954264521598816\n",
      "    load_time_ms: 1.056\n",
      "    num_steps_sampled: 1730000\n",
      "    num_steps_trained: 1730000\n",
      "    sample_time_ms: 2520.489\n",
      "    update_time_ms: 5.106\n",
      "  iterations_since_restore: 865\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.1\n",
      "    ram_util_percent: 62.779999999999994\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4570641186408775\n",
      "    mean_inference_ms: 1.4837282412189894\n",
      "    mean_processing_ms: 0.9506310004635818\n",
      "  time_since_restore: 3161.421589612961\n",
      "  time_this_iter_s: 3.535381555557251\n",
      "  time_total_s: 3161.421589612961\n",
      "  timestamp: 1595951809\n",
      "  timesteps_since_restore: 1730000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1730000\n",
      "  training_iteration: 865\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3161 s, 865 iter, 1730000 ts, 11.9 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.916168773644166\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 865\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.844\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6636591553688049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010488763655303046\n",
      "        policy_loss: -0.0003060731978621334\n",
      "        total_loss: 0.0013811469543725252\n",
      "        vf_explained_var: 0.8044946789741516\n",
      "        vf_loss: 0.0016872319392859936\n",
      "    load_time_ms: 1.139\n",
      "    num_steps_sampled: 1736000\n",
      "    num_steps_trained: 1736000\n",
      "    sample_time_ms: 2489.114\n",
      "    update_time_ms: 5.0\n",
      "  iterations_since_restore: 868\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.625\n",
      "    ram_util_percent: 62.824999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.457064118640878\n",
      "    mean_inference_ms: 1.4837282412189898\n",
      "    mean_processing_ms: 0.9506310004635818\n",
      "  time_since_restore: 3168.33198928833\n",
      "  time_this_iter_s: 2.3564062118530273\n",
      "  time_total_s: 3168.33198928833\n",
      "  timestamp: 1595951815\n",
      "  timesteps_since_restore: 1736000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1736000\n",
      "  training_iteration: 868\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3168 s, 868 iter, 1736000 ts, 11.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.946877132990334\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 870\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.566\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6849035620689392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.0468654080759734e-05\n",
      "        policy_loss: -0.00012116718426113948\n",
      "        total_loss: 0.39777880907058716\n",
      "        vf_explained_var: 0.005616426467895508\n",
      "        vf_loss: 0.3979000151157379\n",
      "    load_time_ms: 1.125\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "    sample_time_ms: 2511.381\n",
      "    update_time_ms: 4.76\n",
      "  iterations_since_restore: 870\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.31999999999999\n",
      "    ram_util_percent: 62.879999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.454361488602945\n",
      "    mean_inference_ms: 1.4830603283028845\n",
      "    mean_processing_ms: 0.9504175042247271\n",
      "  time_since_restore: 3174.255080461502\n",
      "  time_this_iter_s: 3.5222296714782715\n",
      "  time_total_s: 3174.255080461502\n",
      "  timestamp: 1595951821\n",
      "  timesteps_since_restore: 1740000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 870\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3174 s, 870 iter, 1740000 ts, 11.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.946877132990334\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 870\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.661\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6902309060096741\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.940616119914921e-06\n",
      "        policy_loss: -0.00014409732830245048\n",
      "        total_loss: 0.0011915421346202493\n",
      "        vf_explained_var: 0.81326824426651\n",
      "        vf_loss: 0.0013356332201510668\n",
      "    load_time_ms: 1.101\n",
      "    num_steps_sampled: 1746000\n",
      "    num_steps_trained: 1746000\n",
      "    sample_time_ms: 2492.549\n",
      "    update_time_ms: 5.759\n",
      "  iterations_since_restore: 873\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.575\n",
      "    ram_util_percent: 62.95\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.454361488602945\n",
      "    mean_inference_ms: 1.4830603283028845\n",
      "    mean_processing_ms: 0.9504175042247273\n",
      "  time_since_restore: 3180.988580942154\n",
      "  time_this_iter_s: 2.427097797393799\n",
      "  time_total_s: 3180.988580942154\n",
      "  timestamp: 1595951828\n",
      "  timesteps_since_restore: 1746000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1746000\n",
      "  training_iteration: 873\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3180 s, 873 iter, 1746000 ts, 11.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.964430232241439\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 875\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.461\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7196294665336609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.489434675429948e-05\n",
      "        policy_loss: 0.00039714432205073535\n",
      "        total_loss: 0.3959137499332428\n",
      "        vf_explained_var: -0.008060336112976074\n",
      "        vf_loss: 0.3955165445804596\n",
      "    load_time_ms: 1.286\n",
      "    num_steps_sampled: 1750000\n",
      "    num_steps_trained: 1750000\n",
      "    sample_time_ms: 2637.463\n",
      "    update_time_ms: 5.75\n",
      "  iterations_since_restore: 875\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.771428571428565\n",
      "    ram_util_percent: 62.89999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.451145513823261\n",
      "    mean_inference_ms: 1.4822832921324904\n",
      "    mean_processing_ms: 0.9501837122203739\n",
      "  time_since_restore: 3188.2589206695557\n",
      "  time_this_iter_s: 4.675624847412109\n",
      "  time_total_s: 3188.2589206695557\n",
      "  timestamp: 1595951835\n",
      "  timesteps_since_restore: 1750000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1750000\n",
      "  training_iteration: 875\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3188 s, 875 iter, 1750000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.96443023224144\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 875\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.363\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6021986603736877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007320369477383792\n",
      "        policy_loss: -0.0010405792854726315\n",
      "        total_loss: 0.0019272556528449059\n",
      "        vf_explained_var: 0.8424869179725647\n",
      "        vf_loss: 0.0029678340069949627\n",
      "    load_time_ms: 1.355\n",
      "    num_steps_sampled: 1754000\n",
      "    num_steps_trained: 1754000\n",
      "    sample_time_ms: 2756.72\n",
      "    update_time_ms: 6.883\n",
      "  iterations_since_restore: 877\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.925\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.451145513823263\n",
      "    mean_inference_ms: 1.4822832921324909\n",
      "    mean_processing_ms: 0.9501837122203739\n",
      "  time_since_restore: 3194.1056163311005\n",
      "  time_this_iter_s: 2.966285228729248\n",
      "  time_total_s: 3194.1056163311005\n",
      "  timestamp: 1595951841\n",
      "  timesteps_since_restore: 1754000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1754000\n",
      "  training_iteration: 877\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3194 s, 877 iter, 1754000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.96443023224144\n",
      "  episode_reward_min: 11.12026813692992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 875\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.049\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.721123456954956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.249815047951415e-05\n",
      "        policy_loss: -0.0011420889059081674\n",
      "        total_loss: -0.0001462717045797035\n",
      "        vf_explained_var: 0.7266570329666138\n",
      "        vf_loss: 0.0009958221344277263\n",
      "    load_time_ms: 1.313\n",
      "    num_steps_sampled: 1758000\n",
      "    num_steps_trained: 1758000\n",
      "    sample_time_ms: 2856.165\n",
      "    update_time_ms: 7.416\n",
      "  iterations_since_restore: 879\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.2\n",
      "    ram_util_percent: 62.85\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.451145513823263\n",
      "    mean_inference_ms: 1.4822832921324909\n",
      "    mean_processing_ms: 0.9501837122203739\n",
      "  time_since_restore: 3199.832933664322\n",
      "  time_this_iter_s: 2.682766914367676\n",
      "  time_total_s: 3199.832933664322\n",
      "  timestamp: 1595951847\n",
      "  timesteps_since_restore: 1758000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1758000\n",
      "  training_iteration: 879\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3199 s, 879 iter, 1758000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.97636791462066\n",
      "  episode_reward_min: 11.359761510946118\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 880\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.517\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3518749475479126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005938094691373408\n",
      "        policy_loss: -0.0011459350353106856\n",
      "        total_loss: 0.03821077570319176\n",
      "        vf_explained_var: 0.5656646490097046\n",
      "        vf_loss: 0.03935670852661133\n",
      "    load_time_ms: 1.36\n",
      "    num_steps_sampled: 1762000\n",
      "    num_steps_trained: 1762000\n",
      "    sample_time_ms: 2873.451\n",
      "    update_time_ms: 7.154\n",
      "  iterations_since_restore: 881\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.83333333333333\n",
      "    ram_util_percent: 62.833333333333336\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.447328101756958\n",
      "    mean_inference_ms: 1.4813832264147557\n",
      "    mean_processing_ms: 0.9499089850461465\n",
      "  time_since_restore: 3205.627876520157\n",
      "  time_this_iter_s: 2.3302011489868164\n",
      "  time_total_s: 3205.627876520157\n",
      "  timestamp: 1595951853\n",
      "  timesteps_since_restore: 1762000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1762000\n",
      "  training_iteration: 881\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3205 s, 881 iter, 1762000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.97636791462066\n",
      "  episode_reward_min: 11.359761510946118\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 880\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.37\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7340893745422363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004075146862305701\n",
      "        policy_loss: -0.0019649621099233627\n",
      "        total_loss: -0.0008425369160249829\n",
      "        vf_explained_var: 0.7971814274787903\n",
      "        vf_loss: 0.0011224269401282072\n",
      "    load_time_ms: 1.188\n",
      "    num_steps_sampled: 1768000\n",
      "    num_steps_trained: 1768000\n",
      "    sample_time_ms: 2826.714\n",
      "    update_time_ms: 6.547\n",
      "  iterations_since_restore: 884\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.10000000000001\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.447328101756958\n",
      "    mean_inference_ms: 1.4813832264147557\n",
      "    mean_processing_ms: 0.9499089850461465\n",
      "  time_since_restore: 3212.329439163208\n",
      "  time_this_iter_s: 2.1145918369293213\n",
      "  time_total_s: 3212.329439163208\n",
      "  timestamp: 1595951860\n",
      "  timesteps_since_restore: 1768000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1768000\n",
      "  training_iteration: 884\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3212 s, 884 iter, 1768000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.988118822010971\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 885\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.276\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3565672039985657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001214123098179698\n",
      "        policy_loss: -0.0028303752187639475\n",
      "        total_loss: 0.03633483871817589\n",
      "        vf_explained_var: 0.5399805307388306\n",
      "        vf_loss: 0.03916522487998009\n",
      "    load_time_ms: 1.3\n",
      "    num_steps_sampled: 1772000\n",
      "    num_steps_trained: 1772000\n",
      "    sample_time_ms: 2682.826\n",
      "    update_time_ms: 8.132\n",
      "  iterations_since_restore: 886\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9\n",
      "    ram_util_percent: 62.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4422710151600935\n",
      "    mean_inference_ms: 1.4802232773196204\n",
      "    mean_processing_ms: 0.9495750926880523\n",
      "  time_since_restore: 3218.488350868225\n",
      "  time_this_iter_s: 2.713087320327759\n",
      "  time_total_s: 3218.488350868225\n",
      "  timestamp: 1595951866\n",
      "  timesteps_since_restore: 1772000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1772000\n",
      "  training_iteration: 886\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3218 s, 886 iter, 1772000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.988118822010971\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 885\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.89\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6847860217094421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00030393386259675026\n",
      "        policy_loss: -4.852485653827898e-05\n",
      "        total_loss: 0.001271465327590704\n",
      "        vf_explained_var: 0.884882390499115\n",
      "        vf_loss: 0.0013199825771152973\n",
      "    load_time_ms: 1.291\n",
      "    num_steps_sampled: 1776000\n",
      "    num_steps_trained: 1776000\n",
      "    sample_time_ms: 2612.923\n",
      "    update_time_ms: 6.247\n",
      "  iterations_since_restore: 888\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.525\n",
      "    ram_util_percent: 63.025\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4422710151600935\n",
      "    mean_inference_ms: 1.4802232773196204\n",
      "    mean_processing_ms: 0.9495750926880523\n",
      "  time_since_restore: 3223.7488856315613\n",
      "  time_this_iter_s: 3.042587995529175\n",
      "  time_total_s: 3223.7488856315613\n",
      "  timestamp: 1595951871\n",
      "  timesteps_since_restore: 1776000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1776000\n",
      "  training_iteration: 888\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3223 s, 888 iter, 1776000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.989615815586124\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 890\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.944\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7139879465103149\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.083179348730482e-05\n",
      "        policy_loss: -6.210994615685195e-05\n",
      "        total_loss: 0.3971062898635864\n",
      "        vf_explained_var: -0.01870405673980713\n",
      "        vf_loss: 0.39716845750808716\n",
      "    load_time_ms: 1.295\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "    sample_time_ms: 2586.1\n",
      "    update_time_ms: 6.086\n",
      "  iterations_since_restore: 890\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.02\n",
      "    ram_util_percent: 62.919999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.436332971425678\n",
      "    mean_inference_ms: 1.4788782410749042\n",
      "    mean_processing_ms: 0.9491783026974927\n",
      "  time_since_restore: 3229.6252596378326\n",
      "  time_this_iter_s: 3.590045213699341\n",
      "  time_total_s: 3229.6252596378326\n",
      "  timestamp: 1595951877\n",
      "  timesteps_since_restore: 1780000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 890\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3229 s, 890 iter, 1780000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.989615815586124\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 890\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.189\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6898673176765442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001175325523945503\n",
      "        policy_loss: -0.0012610259000211954\n",
      "        total_loss: -0.0001571035390952602\n",
      "        vf_explained_var: 0.8190730810165405\n",
      "        vf_loss: 0.001103932154364884\n",
      "    load_time_ms: 1.339\n",
      "    num_steps_sampled: 1786000\n",
      "    num_steps_trained: 1786000\n",
      "    sample_time_ms: 2617.09\n",
      "    update_time_ms: 6.168\n",
      "  iterations_since_restore: 893\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.27499999999999\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.436332971425677\n",
      "    mean_inference_ms: 1.4788782410749044\n",
      "    mean_processing_ms: 0.9491783026974926\n",
      "  time_since_restore: 3236.855456352234\n",
      "  time_this_iter_s: 2.3507165908813477\n",
      "  time_total_s: 3236.855456352234\n",
      "  timestamp: 1595951884\n",
      "  timesteps_since_restore: 1786000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1786000\n",
      "  training_iteration: 893\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3236 s, 893 iter, 1786000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.981192664475648\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 895\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.521\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6880874633789062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002174447727156803\n",
      "        policy_loss: -0.0008551941136829555\n",
      "        total_loss: 0.39468011260032654\n",
      "        vf_explained_var: 0.006873071193695068\n",
      "        vf_loss: 0.39553534984588623\n",
      "    load_time_ms: 1.214\n",
      "    num_steps_sampled: 1790000\n",
      "    num_steps_trained: 1790000\n",
      "    sample_time_ms: 2636.01\n",
      "    update_time_ms: 6.192\n",
      "  iterations_since_restore: 895\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.44000000000001\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.429391238223904\n",
      "    mean_inference_ms: 1.4773247014588815\n",
      "    mean_processing_ms: 0.9487098041938711\n",
      "  time_since_restore: 3242.5238692760468\n",
      "  time_this_iter_s: 3.4030535221099854\n",
      "  time_total_s: 3242.5238692760468\n",
      "  timestamp: 1595951890\n",
      "  timesteps_since_restore: 1790000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1790000\n",
      "  training_iteration: 895\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3242 s, 895 iter, 1790000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.981192664475648\n",
      "  episode_reward_min: 11.489600350433566\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 895\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.915\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6275717616081238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.977567030233331e-05\n",
      "        policy_loss: 2.7234314984525554e-05\n",
      "        total_loss: 0.001525983796454966\n",
      "        vf_explained_var: 0.7875499725341797\n",
      "        vf_loss: 0.0014987419126555324\n",
      "    load_time_ms: 1.189\n",
      "    num_steps_sampled: 1796000\n",
      "    num_steps_trained: 1796000\n",
      "    sample_time_ms: 2548.263\n",
      "    update_time_ms: 4.224\n",
      "  iterations_since_restore: 898\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.9\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.4293912382239045\n",
      "    mean_inference_ms: 1.4773247014588813\n",
      "    mean_processing_ms: 0.9487098041938711\n",
      "  time_since_restore: 3249.6009798049927\n",
      "  time_this_iter_s: 2.442655563354492\n",
      "  time_total_s: 3249.6009798049927\n",
      "  timestamp: 1595951897\n",
      "  timesteps_since_restore: 1796000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1796000\n",
      "  training_iteration: 898\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3249 s, 898 iter, 1796000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.999088206990278\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 900\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.034\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6610851883888245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011991632345598191\n",
      "        policy_loss: -0.0004586176946759224\n",
      "        total_loss: 0.3959358334541321\n",
      "        vf_explained_var: 0.0212249755859375\n",
      "        vf_loss: 0.3963943421840668\n",
      "    load_time_ms: 1.169\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    sample_time_ms: 2543.198\n",
      "    update_time_ms: 4.259\n",
      "  iterations_since_restore: 900\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.28000000000001\n",
      "    ram_util_percent: 62.879999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.421512566101281\n",
      "    mean_inference_ms: 1.4755781930137837\n",
      "    mean_processing_ms: 0.948164894469522\n",
      "  time_since_restore: 3255.4278626441956\n",
      "  time_this_iter_s: 3.413439989089966\n",
      "  time_total_s: 3255.4278626441956\n",
      "  timestamp: 1595951903\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 900\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3255 s, 900 iter, 1800000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.999088206990278\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 900\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.6\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6350646018981934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002607625210657716\n",
      "        policy_loss: -2.6496887585381046e-05\n",
      "        total_loss: 0.0014036035863682628\n",
      "        vf_explained_var: 0.8270727396011353\n",
      "        vf_loss: 0.0014300990151241422\n",
      "    load_time_ms: 1.153\n",
      "    num_steps_sampled: 1806000\n",
      "    num_steps_trained: 1806000\n",
      "    sample_time_ms: 2542.041\n",
      "    update_time_ms: 4.38\n",
      "  iterations_since_restore: 903\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.53333333333332\n",
      "    ram_util_percent: 63.06666666666666\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.421512566101281\n",
      "    mean_inference_ms: 1.4755781930137841\n",
      "    mean_processing_ms: 0.9481648944695222\n",
      "  time_since_restore: 3262.643835067749\n",
      "  time_this_iter_s: 2.4223737716674805\n",
      "  time_total_s: 3262.643835067749\n",
      "  timestamp: 1595951910\n",
      "  timesteps_since_restore: 1806000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1806000\n",
      "  training_iteration: 903\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3262 s, 903 iter, 1806000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.997717967529729\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 905\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.761\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6877488493919373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.319108797470108e-05\n",
      "        policy_loss: 2.8260707040317357e-05\n",
      "        total_loss: 0.3967036008834839\n",
      "        vf_explained_var: -0.009940028190612793\n",
      "        vf_loss: 0.39667537808418274\n",
      "    load_time_ms: 1.149\n",
      "    num_steps_sampled: 1810000\n",
      "    num_steps_trained: 1810000\n",
      "    sample_time_ms: 2558.488\n",
      "    update_time_ms: 4.18\n",
      "  iterations_since_restore: 905\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.980000000000004\n",
      "    ram_util_percent: 62.96\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.412709480634512\n",
      "    mean_inference_ms: 1.473640477534904\n",
      "    mean_processing_ms: 0.9475738275572863\n",
      "  time_since_restore: 3268.475391626358\n",
      "  time_this_iter_s: 3.5925703048706055\n",
      "  time_total_s: 3268.475391626358\n",
      "  timestamp: 1595951916\n",
      "  timesteps_since_restore: 1810000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1810000\n",
      "  training_iteration: 905\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3268 s, 905 iter, 1810000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 11.99771796752973\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 905\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.345\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6365971565246582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012918144057039171\n",
      "        policy_loss: -0.0008597316918894649\n",
      "        total_loss: 0.00040377522236667573\n",
      "        vf_explained_var: 0.8251832127571106\n",
      "        vf_loss: 0.0012635133462026715\n",
      "    load_time_ms: 1.177\n",
      "    num_steps_sampled: 1816000\n",
      "    num_steps_trained: 1816000\n",
      "    sample_time_ms: 2571.269\n",
      "    update_time_ms: 4.3\n",
      "  iterations_since_restore: 908\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.06666666666668\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.412709480634511\n",
      "    mean_inference_ms: 1.4736404775349032\n",
      "    mean_processing_ms: 0.947573827557286\n",
      "  time_since_restore: 3275.6869733333588\n",
      "  time_this_iter_s: 2.3593389987945557\n",
      "  time_total_s: 3275.6869733333588\n",
      "  timestamp: 1595951923\n",
      "  timesteps_since_restore: 1816000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1816000\n",
      "  training_iteration: 908\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3275 s, 908 iter, 1816000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 12.006014049881758\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 910\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.665\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6495389342308044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.787596016190946e-05\n",
      "        policy_loss: -1.1961937161686365e-05\n",
      "        total_loss: 0.4029844403266907\n",
      "        vf_explained_var: -0.015181422233581543\n",
      "        vf_loss: 0.4029964804649353\n",
      "    load_time_ms: 1.194\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "    sample_time_ms: 2563.716\n",
      "    update_time_ms: 4.301\n",
      "  iterations_since_restore: 910\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.04\n",
      "    ram_util_percent: 62.879999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.403207292220377\n",
      "    mean_inference_ms: 1.4715577765681416\n",
      "    mean_processing_ms: 0.9469369973054039\n",
      "  time_since_restore: 3281.4423356056213\n",
      "  time_this_iter_s: 3.45902419090271\n",
      "  time_total_s: 3281.4423356056213\n",
      "  timestamp: 1595951929\n",
      "  timesteps_since_restore: 1820000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 910\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3281 s, 910 iter, 1820000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 12.006014049881763\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 910\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.973\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5937249064445496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.962761396309361e-05\n",
      "        policy_loss: -0.0005085468292236328\n",
      "        total_loss: 0.0010266713798046112\n",
      "        vf_explained_var: 0.8194358348846436\n",
      "        vf_loss: 0.0015352070331573486\n",
      "    load_time_ms: 1.12\n",
      "    num_steps_sampled: 1826000\n",
      "    num_steps_trained: 1826000\n",
      "    sample_time_ms: 2595.162\n",
      "    update_time_ms: 4.079\n",
      "  iterations_since_restore: 913\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.25\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.403207292220377\n",
      "    mean_inference_ms: 1.4715577765681414\n",
      "    mean_processing_ms: 0.946936997305404\n",
      "  time_since_restore: 3288.973900794983\n",
      "  time_this_iter_s: 2.7756574153900146\n",
      "  time_total_s: 3288.973900794983\n",
      "  timestamp: 1595951937\n",
      "  timesteps_since_restore: 1826000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1826000\n",
      "  training_iteration: 913\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3288 s, 913 iter, 1826000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 12.007296053012077\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 915\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.639\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6345471143722534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011986577737843618\n",
      "        policy_loss: 0.0002125911705661565\n",
      "        total_loss: 0.4036966562271118\n",
      "        vf_explained_var: -0.0025281906127929688\n",
      "        vf_loss: 0.4034840166568756\n",
      "    load_time_ms: 1.246\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "    sample_time_ms: 2653.657\n",
      "    update_time_ms: 4.245\n",
      "  iterations_since_restore: 915\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.5\n",
      "    ram_util_percent: 62.92\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.393034474135063\n",
      "    mean_inference_ms: 1.4693417496469126\n",
      "    mean_processing_ms: 0.9462576430467299\n",
      "  time_since_restore: 3295.4539756774902\n",
      "  time_this_iter_s: 3.8504998683929443\n",
      "  time_total_s: 3295.4539756774902\n",
      "  timestamp: 1595951943\n",
      "  timesteps_since_restore: 1830000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 915\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3295 s, 915 iter, 1830000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 12.007296053012078\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 915\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.928\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4777700901031494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001154170895460993\n",
      "        policy_loss: -0.0007043714285828173\n",
      "        total_loss: 0.0035837998148053885\n",
      "        vf_explained_var: 0.7891417145729065\n",
      "        vf_loss: 0.0042881774716079235\n",
      "    load_time_ms: 1.293\n",
      "    num_steps_sampled: 1834000\n",
      "    num_steps_trained: 1834000\n",
      "    sample_time_ms: 2786.892\n",
      "    update_time_ms: 5.882\n",
      "  iterations_since_restore: 917\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.41999999999999\n",
      "    ram_util_percent: 63.08\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.393034474135061\n",
      "    mean_inference_ms: 1.469341749646913\n",
      "    mean_processing_ms: 0.9462576430467299\n",
      "  time_since_restore: 3301.6967170238495\n",
      "  time_this_iter_s: 3.7344741821289062\n",
      "  time_total_s: 3301.6967170238495\n",
      "  timestamp: 1595951949\n",
      "  timesteps_since_restore: 1834000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1834000\n",
      "  training_iteration: 917\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3301 s, 917 iter, 1834000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.48621766096005\n",
      "  episode_reward_mean: 12.007296053012078\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 915\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.231\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5851531028747559\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3793696527718566e-05\n",
      "        policy_loss: -3.0522824090439826e-05\n",
      "        total_loss: 0.0014120254199951887\n",
      "        vf_explained_var: 0.7712337970733643\n",
      "        vf_loss: 0.0014425499830394983\n",
      "    load_time_ms: 1.303\n",
      "    num_steps_sampled: 1838000\n",
      "    num_steps_trained: 1838000\n",
      "    sample_time_ms: 2859.264\n",
      "    update_time_ms: 6.376\n",
      "  iterations_since_restore: 919\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.45\n",
      "    ram_util_percent: 62.925000000000004\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.393034474135061\n",
      "    mean_inference_ms: 1.469341749646913\n",
      "    mean_processing_ms: 0.9462576430467299\n",
      "  time_since_restore: 3307.0834777355194\n",
      "  time_this_iter_s: 2.551335573196411\n",
      "  time_total_s: 3307.0834777355194\n",
      "  timestamp: 1595951955\n",
      "  timesteps_since_restore: 1838000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1838000\n",
      "  training_iteration: 919\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3307 s, 919 iter, 1838000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.021495813145739\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 920\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.429\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.23622648417949677\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1172085578436963e-05\n",
      "        policy_loss: -0.0001107473362935707\n",
      "        total_loss: 0.04281562194228172\n",
      "        vf_explained_var: 0.5449806451797485\n",
      "        vf_loss: 0.042926374822854996\n",
      "    load_time_ms: 1.338\n",
      "    num_steps_sampled: 1842000\n",
      "    num_steps_trained: 1842000\n",
      "    sample_time_ms: 2929.036\n",
      "    update_time_ms: 6.443\n",
      "  iterations_since_restore: 921\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.86666666666666\n",
      "    ram_util_percent: 62.93333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.382172525087702\n",
      "    mean_inference_ms: 1.4669833303109632\n",
      "    mean_processing_ms: 0.9455534804297094\n",
      "  time_since_restore: 3313.5287232398987\n",
      "  time_this_iter_s: 2.4156651496887207\n",
      "  time_total_s: 3313.5287232398987\n",
      "  timestamp: 1595951961\n",
      "  timesteps_since_restore: 1842000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1842000\n",
      "  training_iteration: 921\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3313 s, 921 iter, 1842000 ts, 12 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-29\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.021495813145739\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 920\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5878670811653137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.749800766352564e-05\n",
      "        policy_loss: -0.0002276897430419922\n",
      "        total_loss: 0.0010227051097899675\n",
      "        vf_explained_var: 0.7338936924934387\n",
      "        vf_loss: 0.0012504145270213485\n",
      "    load_time_ms: 1.341\n",
      "    num_steps_sampled: 1848000\n",
      "    num_steps_trained: 1848000\n",
      "    sample_time_ms: 2874.22\n",
      "    update_time_ms: 6.466\n",
      "  iterations_since_restore: 924\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.55000000000001\n",
      "    ram_util_percent: 63.025\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.382172525087702\n",
      "    mean_inference_ms: 1.4669833303109632\n",
      "    mean_processing_ms: 0.9455534804297094\n",
      "  time_since_restore: 3320.8671264648438\n",
      "  time_this_iter_s: 2.506995677947998\n",
      "  time_total_s: 3320.8671264648438\n",
      "  timestamp: 1595951969\n",
      "  timesteps_since_restore: 1848000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1848000\n",
      "  training_iteration: 924\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3320 s, 924 iter, 1848000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.0474317994106\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 925\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.942\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.21995270252227783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010008073411881924\n",
      "        policy_loss: -0.0066068717278540134\n",
      "        total_loss: 0.03619121387600899\n",
      "        vf_explained_var: 0.5398982763290405\n",
      "        vf_loss: 0.04279806837439537\n",
      "    load_time_ms: 1.22\n",
      "    num_steps_sampled: 1852000\n",
      "    num_steps_trained: 1852000\n",
      "    sample_time_ms: 2869.923\n",
      "    update_time_ms: 6.518\n",
      "  iterations_since_restore: 926\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.375\n",
      "    ram_util_percent: 63.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.371521906318215\n",
      "    mean_inference_ms: 1.4646743315599506\n",
      "    mean_processing_ms: 0.9448609907001891\n",
      "  time_since_restore: 3327.0926446914673\n",
      "  time_this_iter_s: 2.4518401622772217\n",
      "  time_total_s: 3327.0926446914673\n",
      "  timestamp: 1595951975\n",
      "  timesteps_since_restore: 1852000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1852000\n",
      "  training_iteration: 926\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3327 s, 926 iter, 1852000 ts, 12 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-42\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.0474317994106\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 925\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.14\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6200413703918457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003924739430658519\n",
      "        policy_loss: -0.0031070217955857515\n",
      "        total_loss: -0.0023075228091329336\n",
      "        vf_explained_var: 0.8977749943733215\n",
      "        vf_loss: 0.0007995054475031793\n",
      "    load_time_ms: 1.155\n",
      "    num_steps_sampled: 1858000\n",
      "    num_steps_trained: 1858000\n",
      "    sample_time_ms: 2658.636\n",
      "    update_time_ms: 4.945\n",
      "  iterations_since_restore: 929\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.025\n",
      "    ram_util_percent: 63.074999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.371521906318215\n",
      "    mean_inference_ms: 1.4646743315599506\n",
      "    mean_processing_ms: 0.9448609907001891\n",
      "  time_since_restore: 3334.0506851673126\n",
      "  time_this_iter_s: 2.4354231357574463\n",
      "  time_total_s: 3334.0506851673126\n",
      "  timestamp: 1595951982\n",
      "  timesteps_since_restore: 1858000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1858000\n",
      "  training_iteration: 929\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3334 s, 929 iter, 1858000 ts, 12 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.06236070290176\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 930\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.537\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2074827253818512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035069843288511038\n",
      "        policy_loss: -0.0030724171083420515\n",
      "        total_loss: 0.03921904042363167\n",
      "        vf_explained_var: 0.5780091881752014\n",
      "        vf_loss: 0.04229145869612694\n",
      "    load_time_ms: 1.117\n",
      "    num_steps_sampled: 1862000\n",
      "    num_steps_trained: 1862000\n",
      "    sample_time_ms: 2637.307\n",
      "    update_time_ms: 5.437\n",
      "  iterations_since_restore: 931\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.775\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.36107233122485\n",
      "    mean_inference_ms: 1.4624101224518569\n",
      "    mean_processing_ms: 0.9441793086160085\n",
      "  time_since_restore: 3340.2925395965576\n",
      "  time_this_iter_s: 2.756606101989746\n",
      "  time_total_s: 3340.2925395965576\n",
      "  timestamp: 1595951988\n",
      "  timesteps_since_restore: 1862000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1862000\n",
      "  training_iteration: 931\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3340 s, 931 iter, 1862000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_18-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.06236070290176\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 930\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.43\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6025879979133606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003128226671833545\n",
      "        policy_loss: -0.0018677921034395695\n",
      "        total_loss: -0.0010977325728163123\n",
      "        vf_explained_var: 0.8628805875778198\n",
      "        vf_loss: 0.0007700671558268368\n",
      "    load_time_ms: 1.271\n",
      "    num_steps_sampled: 1868000\n",
      "    num_steps_trained: 1868000\n",
      "    sample_time_ms: 2770.408\n",
      "    update_time_ms: 5.695\n",
      "  iterations_since_restore: 934\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.16666666666667\n",
      "    ram_util_percent: 65.73333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.36107233122485\n",
      "    mean_inference_ms: 1.4624101224518569\n",
      "    mean_processing_ms: 0.9441793086160085\n",
      "  time_since_restore: 3348.990196943283\n",
      "  time_this_iter_s: 4.066545009613037\n",
      "  time_total_s: 3348.990196943283\n",
      "  timestamp: 1595951997\n",
      "  timesteps_since_restore: 1868000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1868000\n",
      "  training_iteration: 934\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3348 s, 934 iter, 1868000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.052701077741013\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 935\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.224\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5967766046524048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00017238661530427635\n",
      "        policy_loss: 7.488823030143976e-05\n",
      "        total_loss: 0.39638426899909973\n",
      "        vf_explained_var: 0.014850735664367676\n",
      "        vf_loss: 0.3963093161582947\n",
      "    load_time_ms: 1.672\n",
      "    num_steps_sampled: 1870000\n",
      "    num_steps_trained: 1870000\n",
      "    sample_time_ms: 3043.672\n",
      "    update_time_ms: 5.236\n",
      "  iterations_since_restore: 935\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.3111111111111\n",
      "    ram_util_percent: 65.75555555555556\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351341010584282\n",
      "    mean_inference_ms: 1.4603195656459556\n",
      "    mean_processing_ms: 0.9435666591897585\n",
      "  time_since_restore: 3355.601301431656\n",
      "  time_this_iter_s: 6.611104488372803\n",
      "  time_total_s: 3355.601301431656\n",
      "  timestamp: 1595952003\n",
      "  timesteps_since_restore: 1870000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1870000\n",
      "  training_iteration: 935\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3355 s, 935 iter, 1870000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.052701077741016\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 935\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 41.034\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.18900282680988312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011531932279467583\n",
      "        policy_loss: -0.004829156678169966\n",
      "        total_loss: 0.038986608386039734\n",
      "        vf_explained_var: 0.5993626117706299\n",
      "        vf_loss: 0.04381575807929039\n",
      "    load_time_ms: 1.837\n",
      "    num_steps_sampled: 1872000\n",
      "    num_steps_trained: 1872000\n",
      "    sample_time_ms: 3331.021\n",
      "    update_time_ms: 6.475\n",
      "  iterations_since_restore: 936\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.9375\n",
      "    ram_util_percent: 65.7\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351341010584284\n",
      "    mean_inference_ms: 1.4603195656459553\n",
      "    mean_processing_ms: 0.9435666591897587\n",
      "  time_since_restore: 3361.02268075943\n",
      "  time_this_iter_s: 5.421379327774048\n",
      "  time_total_s: 3361.02268075943\n",
      "  timestamp: 1595952009\n",
      "  timesteps_since_restore: 1872000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1872000\n",
      "  training_iteration: 936\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3361 s, 936 iter, 1872000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.052701077741016\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 935\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 49.208\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44717156887054443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.195626727072522e-05\n",
      "        policy_loss: 4.986178828403354e-05\n",
      "        total_loss: 0.002837186912074685\n",
      "        vf_explained_var: 0.8375981450080872\n",
      "        vf_loss: 0.002787324832752347\n",
      "    load_time_ms: 2.046\n",
      "    num_steps_sampled: 1874000\n",
      "    num_steps_trained: 1874000\n",
      "    sample_time_ms: 3661.494\n",
      "    update_time_ms: 7.593\n",
      "  iterations_since_restore: 937\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.36250000000001\n",
      "    ram_util_percent: 65.9625\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351341010584284\n",
      "    mean_inference_ms: 1.4603195656459553\n",
      "    mean_processing_ms: 0.9435666591897587\n",
      "  time_since_restore: 3366.6898698806763\n",
      "  time_this_iter_s: 5.667189121246338\n",
      "  time_total_s: 3366.6898698806763\n",
      "  timestamp: 1595952014\n",
      "  timesteps_since_restore: 1874000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1874000\n",
      "  training_iteration: 937\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3366 s, 937 iter, 1874000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.052701077741016\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 935\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 53.353\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5184381008148193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4462640794808976e-05\n",
      "        policy_loss: -0.0001112480167648755\n",
      "        total_loss: 0.0013662941055372357\n",
      "        vf_explained_var: 0.8405165076255798\n",
      "        vf_loss: 0.0014775388408452272\n",
      "    load_time_ms: 2.092\n",
      "    num_steps_sampled: 1876000\n",
      "    num_steps_trained: 1876000\n",
      "    sample_time_ms: 3981.879\n",
      "    update_time_ms: 9.11\n",
      "  iterations_since_restore: 938\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.625\n",
      "    ram_util_percent: 66.2375\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351341010584284\n",
      "    mean_inference_ms: 1.4603195656459553\n",
      "    mean_processing_ms: 0.9435666591897587\n",
      "  time_since_restore: 3372.2559773921967\n",
      "  time_this_iter_s: 5.566107511520386\n",
      "  time_total_s: 3372.2559773921967\n",
      "  timestamp: 1595952020\n",
      "  timesteps_since_restore: 1876000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1876000\n",
      "  training_iteration: 938\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3372 s, 938 iter, 1876000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.052701077741016\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 935\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 58.876\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.530417263507843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00017626419139560312\n",
      "        policy_loss: -0.0007369675440713763\n",
      "        total_loss: 0.0005518255056813359\n",
      "        vf_explained_var: 0.6923211812973022\n",
      "        vf_loss: 0.0012887849006801844\n",
      "    load_time_ms: 2.148\n",
      "    num_steps_sampled: 1878000\n",
      "    num_steps_trained: 1878000\n",
      "    sample_time_ms: 4292.779\n",
      "    update_time_ms: 9.9\n",
      "  iterations_since_restore: 939\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.44999999999999\n",
      "    ram_util_percent: 66.275\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.351341010584284\n",
      "    mean_inference_ms: 1.4603195656459553\n",
      "    mean_processing_ms: 0.9435666591897587\n",
      "  time_since_restore: 3377.885820865631\n",
      "  time_this_iter_s: 5.629843473434448\n",
      "  time_total_s: 3377.885820865631\n",
      "  timestamp: 1595952026\n",
      "  timesteps_since_restore: 1878000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1878000\n",
      "  training_iteration: 939\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3377 s, 939 iter, 1878000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.062407249009858\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 940\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 57.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.15907132625579834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.229341230820864e-05\n",
      "        policy_loss: -0.00021300697699189186\n",
      "        total_loss: 0.04463575407862663\n",
      "        vf_explained_var: 0.556377649307251\n",
      "        vf_loss: 0.04484875500202179\n",
      "    load_time_ms: 2.172\n",
      "    num_steps_sampled: 1882000\n",
      "    num_steps_trained: 1882000\n",
      "    sample_time_ms: 4419.763\n",
      "    update_time_ms: 9.982\n",
      "  iterations_since_restore: 941\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.2\n",
      "    ram_util_percent: 63.666666666666664\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.343090672442809\n",
      "    mean_inference_ms: 1.4585640781759601\n",
      "    mean_processing_ms: 0.9430402338519186\n",
      "  time_since_restore: 3385.375006914139\n",
      "  time_this_iter_s: 2.554028272628784\n",
      "  time_total_s: 3385.375006914139\n",
      "  timestamp: 1595952033\n",
      "  timesteps_since_restore: 1882000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1882000\n",
      "  training_iteration: 941\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3385 s, 941 iter, 1882000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.062407249009858\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 940\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 55.282\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5102177858352661\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004885875387117267\n",
      "        policy_loss: 0.00025584697141312063\n",
      "        total_loss: 0.0016653756611049175\n",
      "        vf_explained_var: 0.8051317930221558\n",
      "        vf_loss: 0.001409545075148344\n",
      "    load_time_ms: 2.051\n",
      "    num_steps_sampled: 1886000\n",
      "    num_steps_trained: 1886000\n",
      "    sample_time_ms: 4476.185\n",
      "    update_time_ms: 9.764\n",
      "  iterations_since_restore: 943\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.225\n",
      "    ram_util_percent: 63.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.343090672442809\n",
      "    mean_inference_ms: 1.4585640781759601\n",
      "    mean_processing_ms: 0.9430402338519186\n",
      "  time_since_restore: 3390.5447912216187\n",
      "  time_this_iter_s: 2.4297454357147217\n",
      "  time_total_s: 3390.5447912216187\n",
      "  timestamp: 1595952039\n",
      "  timesteps_since_restore: 1886000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1886000\n",
      "  training_iteration: 943\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3390 s, 943 iter, 1886000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.08007783157366\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 945\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.6\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5375997424125671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011771428398787975\n",
      "        policy_loss: -2.5924682631739415e-05\n",
      "        total_loss: 0.40857842564582825\n",
      "        vf_explained_var: 0.0073473453521728516\n",
      "        vf_loss: 0.40860438346862793\n",
      "    load_time_ms: 1.575\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "    sample_time_ms: 4043.664\n",
      "    update_time_ms: 9.486\n",
      "  iterations_since_restore: 945\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.92\n",
      "    ram_util_percent: 63.78000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.335066582701654\n",
      "    mean_inference_ms: 1.456863830136121\n",
      "    mean_processing_ms: 0.9425321842839145\n",
      "  time_since_restore: 3396.7729563713074\n",
      "  time_this_iter_s: 3.9721744060516357\n",
      "  time_total_s: 3396.7729563713074\n",
      "  timestamp: 1595952045\n",
      "  timesteps_since_restore: 1890000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 945\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3396 s, 945 iter, 1890000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.08007783157366\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 945\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.714\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5313687920570374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00028448732336983085\n",
      "        policy_loss: 6.482696335297078e-05\n",
      "        total_loss: 0.0011785316746681929\n",
      "        vf_explained_var: 0.8629975318908691\n",
      "        vf_loss: 0.001113707316108048\n",
      "    load_time_ms: 1.179\n",
      "    num_steps_sampled: 1896000\n",
      "    num_steps_trained: 1896000\n",
      "    sample_time_ms: 3065.909\n",
      "    update_time_ms: 5.776\n",
      "  iterations_since_restore: 948\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0\n",
      "    ram_util_percent: 63.79999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.335066582701654\n",
      "    mean_inference_ms: 1.456863830136121\n",
      "    mean_processing_ms: 0.9425321842839145\n",
      "  time_since_restore: 3403.371571779251\n",
      "  time_this_iter_s: 2.1595168113708496\n",
      "  time_total_s: 3403.371571779251\n",
      "  timestamp: 1595952051\n",
      "  timesteps_since_restore: 1896000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1896000\n",
      "  training_iteration: 948\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3403 s, 948 iter, 1896000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.0743682214771\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 950\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.587\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5602474212646484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7238071197643876e-05\n",
      "        policy_loss: 7.881975034251809e-05\n",
      "        total_loss: 0.40425175428390503\n",
      "        vf_explained_var: 0.009844779968261719\n",
      "        vf_loss: 0.4041729271411896\n",
      "    load_time_ms: 1.197\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1900000\n",
      "    sample_time_ms: 2609.624\n",
      "    update_time_ms: 4.471\n",
      "  iterations_since_restore: 950\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.93999999999998\n",
      "    ram_util_percent: 63.919999999999995\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3271670612015205\n",
      "    mean_inference_ms: 1.4551917723398518\n",
      "    mean_processing_ms: 0.9420307524968703\n",
      "  time_since_restore: 3409.331199169159\n",
      "  time_this_iter_s: 3.6552937030792236\n",
      "  time_total_s: 3409.331199169159\n",
      "  timestamp: 1595952057\n",
      "  timesteps_since_restore: 1900000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 950\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3409 s, 950 iter, 1900000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.074368221477103\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 950\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.866\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4174393117427826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002322604414075613\n",
      "        policy_loss: -0.0010078963823616505\n",
      "        total_loss: 0.001971282996237278\n",
      "        vf_explained_var: 0.8537911772727966\n",
      "        vf_loss: 0.002979191252961755\n",
      "    load_time_ms: 1.311\n",
      "    num_steps_sampled: 1904000\n",
      "    num_steps_trained: 1904000\n",
      "    sample_time_ms: 2618.991\n",
      "    update_time_ms: 6.233\n",
      "  iterations_since_restore: 952\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.03333333333335\n",
      "    ram_util_percent: 63.333333333333336\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3271670612015205\n",
      "    mean_inference_ms: 1.4551917723398518\n",
      "    mean_processing_ms: 0.9420307524968702\n",
      "  time_since_restore: 3414.8227343559265\n",
      "  time_this_iter_s: 2.2933249473571777\n",
      "  time_total_s: 3414.8227343559265\n",
      "  timestamp: 1595952063\n",
      "  timesteps_since_restore: 1904000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1904000\n",
      "  training_iteration: 952\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3414 s, 952 iter, 1904000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.074368221477103\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 950\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.148\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5085053443908691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012001213326584548\n",
      "        policy_loss: -0.0006569080287590623\n",
      "        total_loss: 0.0005499477265402675\n",
      "        vf_explained_var: 0.6780502796173096\n",
      "        vf_loss: 0.0012068605283275247\n",
      "    load_time_ms: 1.579\n",
      "    num_steps_sampled: 1908000\n",
      "    num_steps_trained: 1908000\n",
      "    sample_time_ms: 2704.126\n",
      "    update_time_ms: 7.033\n",
      "  iterations_since_restore: 954\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.5\n",
      "    ram_util_percent: 63.25\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.3271670612015205\n",
      "    mean_inference_ms: 1.4551917723398518\n",
      "    mean_processing_ms: 0.9420307524968702\n",
      "  time_since_restore: 3420.4409806728363\n",
      "  time_this_iter_s: 2.7505195140838623\n",
      "  time_total_s: 3420.4409806728363\n",
      "  timestamp: 1595952069\n",
      "  timesteps_since_restore: 1908000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1908000\n",
      "  training_iteration: 954\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3420 s, 954 iter, 1908000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.0763061910769\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 955\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.588\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.13893549144268036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0008603821042925119\n",
      "        policy_loss: -0.00023152161156758666\n",
      "        total_loss: 0.04413020983338356\n",
      "        vf_explained_var: 0.5868090391159058\n",
      "        vf_loss: 0.04436173290014267\n",
      "    load_time_ms: 1.563\n",
      "    num_steps_sampled: 1912000\n",
      "    num_steps_trained: 1912000\n",
      "    sample_time_ms: 2722.008\n",
      "    update_time_ms: 7.618\n",
      "  iterations_since_restore: 956\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.6\n",
      "    ram_util_percent: 63.29999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.319559450728206\n",
      "    mean_inference_ms: 1.4535798854139776\n",
      "    mean_processing_ms: 0.9415413661595177\n",
      "  time_since_restore: 3426.7212932109833\n",
      "  time_this_iter_s: 2.106156349182129\n",
      "  time_total_s: 3426.7212932109833\n",
      "  timestamp: 1595952075\n",
      "  timesteps_since_restore: 1912000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1912000\n",
      "  training_iteration: 956\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3426 s, 956 iter, 1912000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.0763061910769\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 955\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 36.828\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5260812640190125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002730650012381375\n",
      "        policy_loss: -0.0012349624885246158\n",
      "        total_loss: -0.0002650032110977918\n",
      "        vf_explained_var: 0.8390589356422424\n",
      "        vf_loss: 0.000969946850091219\n",
      "    load_time_ms: 1.567\n",
      "    num_steps_sampled: 1918000\n",
      "    num_steps_trained: 1918000\n",
      "    sample_time_ms: 2711.636\n",
      "    update_time_ms: 7.397\n",
      "  iterations_since_restore: 959\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.16666666666667\n",
      "    ram_util_percent: 63.20000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.319559450728206\n",
      "    mean_inference_ms: 1.4535798854139776\n",
      "    mean_processing_ms: 0.9415413661595177\n",
      "  time_since_restore: 3433.389658689499\n",
      "  time_this_iter_s: 2.303741693496704\n",
      "  time_total_s: 3433.389658689499\n",
      "  timestamp: 1595952082\n",
      "  timesteps_since_restore: 1918000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1918000\n",
      "  training_iteration: 959\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3433 s, 959 iter, 1918000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.091097238907357\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 960\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.631\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.11451448500156403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009773441590368748\n",
      "        policy_loss: -0.005880569573491812\n",
      "        total_loss: 0.03927022963762283\n",
      "        vf_explained_var: 0.5718435049057007\n",
      "        vf_loss: 0.04515080153942108\n",
      "    load_time_ms: 1.401\n",
      "    num_steps_sampled: 1922000\n",
      "    num_steps_trained: 1922000\n",
      "    sample_time_ms: 2619.283\n",
      "    update_time_ms: 6.824\n",
      "  iterations_since_restore: 961\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.36666666666667\n",
      "    ram_util_percent: 63.29999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.311998134205444\n",
      "    mean_inference_ms: 1.4519707602364036\n",
      "    mean_processing_ms: 0.9410532075747031\n",
      "  time_since_restore: 3439.1867785453796\n",
      "  time_this_iter_s: 2.3069655895233154\n",
      "  time_total_s: 3439.1867785453796\n",
      "  timestamp: 1595952087\n",
      "  timesteps_since_restore: 1922000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1922000\n",
      "  training_iteration: 961\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3439 s, 961 iter, 1922000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.091097238907357\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 960\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5233200192451477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007380792521871626\n",
      "        policy_loss: -0.0036238341126590967\n",
      "        total_loss: -0.0028010113164782524\n",
      "        vf_explained_var: 0.9039016366004944\n",
      "        vf_loss: 0.0008228224469348788\n",
      "    load_time_ms: 1.115\n",
      "    num_steps_sampled: 1928000\n",
      "    num_steps_trained: 1928000\n",
      "    sample_time_ms: 2533.401\n",
      "    update_time_ms: 4.979\n",
      "  iterations_since_restore: 964\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.03333333333335\n",
      "    ram_util_percent: 63.29999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.311998134205444\n",
      "    mean_inference_ms: 1.4519707602364036\n",
      "    mean_processing_ms: 0.9410532075747031\n",
      "  time_since_restore: 3446.145712852478\n",
      "  time_this_iter_s: 2.365316152572632\n",
      "  time_total_s: 3446.145712852478\n",
      "  timestamp: 1595952094\n",
      "  timesteps_since_restore: 1928000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1928000\n",
      "  training_iteration: 964\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3446 s, 964 iter, 1928000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.099914384335657\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 965\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.76\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.09819259494543076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0013643751153722405\n",
      "        policy_loss: -0.0010409584501758218\n",
      "        total_loss: 0.04441828280687332\n",
      "        vf_explained_var: 0.5754004716873169\n",
      "        vf_loss: 0.045459240674972534\n",
      "    load_time_ms: 1.176\n",
      "    num_steps_sampled: 1932000\n",
      "    num_steps_trained: 1932000\n",
      "    sample_time_ms: 2529.146\n",
      "    update_time_ms: 4.229\n",
      "  iterations_since_restore: 966\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.175\n",
      "    ram_util_percent: 63.425\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.304530264856607\n",
      "    mean_inference_ms: 1.4503813644920935\n",
      "    mean_processing_ms: 0.9405670776072347\n",
      "  time_since_restore: 3452.3875646591187\n",
      "  time_this_iter_s: 2.881040334701538\n",
      "  time_total_s: 3452.3875646591187\n",
      "  timestamp: 1595952101\n",
      "  timesteps_since_restore: 1932000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1932000\n",
      "  training_iteration: 966\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3452 s, 966 iter, 1932000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.099914384335657\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 965\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.998\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46354952454566956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4927982761037129e-07\n",
      "        policy_loss: -3.440380169195123e-06\n",
      "        total_loss: 0.0012650227872654796\n",
      "        vf_explained_var: 0.7885802388191223\n",
      "        vf_loss: 0.0012684569228440523\n",
      "    load_time_ms: 1.176\n",
      "    num_steps_sampled: 1936000\n",
      "    num_steps_trained: 1936000\n",
      "    sample_time_ms: 2590.0\n",
      "    update_time_ms: 4.512\n",
      "  iterations_since_restore: 968\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.26666666666667\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.304530264856607\n",
      "    mean_inference_ms: 1.4503813644920935\n",
      "    mean_processing_ms: 0.9405670776072347\n",
      "  time_since_restore: 3457.3775506019592\n",
      "  time_this_iter_s: 2.244091033935547\n",
      "  time_total_s: 3457.3775506019592\n",
      "  timestamp: 1595952106\n",
      "  timesteps_since_restore: 1936000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1936000\n",
      "  training_iteration: 968\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3457 s, 968 iter, 1936000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.097868661534589\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 970\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.093\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49343574047088623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00022079456539358944\n",
      "        policy_loss: -0.00014244532212615013\n",
      "        total_loss: 0.41227027773857117\n",
      "        vf_explained_var: 0.0012631416320800781\n",
      "        vf_loss: 0.41241273283958435\n",
      "    load_time_ms: 1.127\n",
      "    num_steps_sampled: 1940000\n",
      "    num_steps_trained: 1940000\n",
      "    sample_time_ms: 2561.959\n",
      "    update_time_ms: 4.561\n",
      "  iterations_since_restore: 970\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.519999999999996\n",
      "    ram_util_percent: 63.279999999999994\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.297213969088857\n",
      "    mean_inference_ms: 1.4488318133029212\n",
      "    mean_processing_ms: 0.9400887198407102\n",
      "  time_since_restore: 3462.8789167404175\n",
      "  time_this_iter_s: 3.2514097690582275\n",
      "  time_total_s: 3462.8789167404175\n",
      "  timestamp: 1595952111\n",
      "  timesteps_since_restore: 1940000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1940000\n",
      "  training_iteration: 970\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3462 s, 970 iter, 1940000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-01-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.097868661534587\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 970\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.711\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42640188336372375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0008233878179453313\n",
      "        policy_loss: -0.0023679714649915695\n",
      "        total_loss: -0.000856950762681663\n",
      "        vf_explained_var: 0.75764399766922\n",
      "        vf_loss: 0.001511029084213078\n",
      "    load_time_ms: 1.129\n",
      "    num_steps_sampled: 1946000\n",
      "    num_steps_trained: 1946000\n",
      "    sample_time_ms: 2516.714\n",
      "    update_time_ms: 4.724\n",
      "  iterations_since_restore: 973\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.53333333333333\n",
      "    ram_util_percent: 63.29999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.297213969088857\n",
      "    mean_inference_ms: 1.4488318133029212\n",
      "    mean_processing_ms: 0.9400887198407101\n",
      "  time_since_restore: 3469.3373708724976\n",
      "  time_this_iter_s: 2.1268422603607178\n",
      "  time_total_s: 3469.3373708724976\n",
      "  timestamp: 1595952118\n",
      "  timesteps_since_restore: 1946000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1946000\n",
      "  training_iteration: 973\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3469 s, 973 iter, 1946000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.11973966685251\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 975\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.387\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4649229049682617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.3210438636597246e-05\n",
      "        policy_loss: -0.00024603557540103793\n",
      "        total_loss: 0.4118273854255676\n",
      "        vf_explained_var: 0.01025933027267456\n",
      "        vf_loss: 0.4120734632015228\n",
      "    load_time_ms: 1.096\n",
      "    num_steps_sampled: 1950000\n",
      "    num_steps_trained: 1950000\n",
      "    sample_time_ms: 2479.716\n",
      "    update_time_ms: 4.75\n",
      "  iterations_since_restore: 975\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.86\n",
      "    ram_util_percent: 63.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.28977910067476\n",
      "    mean_inference_ms: 1.447256581538917\n",
      "    mean_processing_ms: 0.9395961481177395\n",
      "  time_since_restore: 3474.691485643387\n",
      "  time_this_iter_s: 3.2419631481170654\n",
      "  time_total_s: 3474.691485643387\n",
      "  timestamp: 1595952123\n",
      "  timesteps_since_restore: 1950000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1950000\n",
      "  training_iteration: 975\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3474 s, 975 iter, 1950000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.119739666852514\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 975\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.316\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4602167308330536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003184604283887893\n",
      "        policy_loss: 0.0008915166836231947\n",
      "        total_loss: 0.001883010845631361\n",
      "        vf_explained_var: 0.8984900116920471\n",
      "        vf_loss: 0.000991489039734006\n",
      "    load_time_ms: 1.058\n",
      "    num_steps_sampled: 1956000\n",
      "    num_steps_trained: 1956000\n",
      "    sample_time_ms: 2345.594\n",
      "    update_time_ms: 4.461\n",
      "  iterations_since_restore: 978\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.46666666666665\n",
      "    ram_util_percent: 63.199999999999996\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.289779100674761\n",
      "    mean_inference_ms: 1.4472565815389171\n",
      "    mean_processing_ms: 0.9395961481177395\n",
      "  time_since_restore: 3481.20210647583\n",
      "  time_this_iter_s: 2.3291378021240234\n",
      "  time_total_s: 3481.20210647583\n",
      "  timestamp: 1595952129\n",
      "  timesteps_since_restore: 1956000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1956000\n",
      "  training_iteration: 978\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3481 s, 978 iter, 1956000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.134422090360575\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 980\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.423\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5072261691093445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0009115382563322783\n",
      "        policy_loss: -0.0011294966097921133\n",
      "        total_loss: 0.4055469334125519\n",
      "        vf_explained_var: -0.0041550397872924805\n",
      "        vf_loss: 0.4066764712333679\n",
      "    load_time_ms: 1.132\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1960000\n",
      "    sample_time_ms: 2370.298\n",
      "    update_time_ms: 4.291\n",
      "  iterations_since_restore: 980\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.019999999999996\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.282152069241729\n",
      "    mean_inference_ms: 1.4456382943280577\n",
      "    mean_processing_ms: 0.9390964228346654\n",
      "  time_since_restore: 3486.991248369217\n",
      "  time_this_iter_s: 3.5290017127990723\n",
      "  time_total_s: 3486.991248369217\n",
      "  timestamp: 1595952135\n",
      "  timesteps_since_restore: 1960000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 980\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3486 s, 980 iter, 1960000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.134422090360575\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 980\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.528\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3667745292186737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006607749382965267\n",
      "        policy_loss: -0.0003469772345852107\n",
      "        total_loss: 0.0024783744011074305\n",
      "        vf_explained_var: 0.8810955286026001\n",
      "        vf_loss: 0.0028253556229174137\n",
      "    load_time_ms: 1.296\n",
      "    num_steps_sampled: 1964000\n",
      "    num_steps_trained: 1964000\n",
      "    sample_time_ms: 2472.252\n",
      "    update_time_ms: 5.259\n",
      "  iterations_since_restore: 982\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.53333333333335\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.282152069241729\n",
      "    mean_inference_ms: 1.445638294328057\n",
      "    mean_processing_ms: 0.9390964228346657\n",
      "  time_since_restore: 3492.384348630905\n",
      "  time_this_iter_s: 2.403839111328125\n",
      "  time_total_s: 3492.384348630905\n",
      "  timestamp: 1595952141\n",
      "  timesteps_since_restore: 1964000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1964000\n",
      "  training_iteration: 982\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3492 s, 982 iter, 1964000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.134422090360575\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 980\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.438\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5041666030883789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001389248063787818\n",
      "        policy_loss: -0.0008948516915552318\n",
      "        total_loss: -7.255506352521479e-05\n",
      "        vf_explained_var: 0.7572695016860962\n",
      "        vf_loss: 0.0008222932810895145\n",
      "    load_time_ms: 1.306\n",
      "    num_steps_sampled: 1968000\n",
      "    num_steps_trained: 1968000\n",
      "    sample_time_ms: 2560.3\n",
      "    update_time_ms: 5.424\n",
      "  iterations_since_restore: 984\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.97500000000001\n",
      "    ram_util_percent: 63.575\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.282152069241729\n",
      "    mean_inference_ms: 1.445638294328057\n",
      "    mean_processing_ms: 0.9390964228346657\n",
      "  time_since_restore: 3497.5291752815247\n",
      "  time_this_iter_s: 2.8636279106140137\n",
      "  time_total_s: 3497.5291752815247\n",
      "  timestamp: 1595952146\n",
      "  timesteps_since_restore: 1968000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1968000\n",
      "  training_iteration: 984\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3497 s, 984 iter, 1968000 ts, 12.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.147916932183573\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 985\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.648\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.08102354407310486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005565192084759474\n",
      "        policy_loss: -0.0028718719258904457\n",
      "        total_loss: 0.04177727550268173\n",
      "        vf_explained_var: 0.6141334176063538\n",
      "        vf_loss: 0.044649139046669006\n",
      "    load_time_ms: 1.334\n",
      "    num_steps_sampled: 1972000\n",
      "    num_steps_trained: 1972000\n",
      "    sample_time_ms: 2674.007\n",
      "    update_time_ms: 5.427\n",
      "  iterations_since_restore: 986\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.666666666666664\n",
      "    ram_util_percent: 64.36666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.27486140893317\n",
      "    mean_inference_ms: 1.4440925372071343\n",
      "    mean_processing_ms: 0.9386068049837754\n",
      "  time_since_restore: 3504.0006725788116\n",
      "  time_this_iter_s: 2.1744961738586426\n",
      "  time_total_s: 3504.0006725788116\n",
      "  timestamp: 1595952152\n",
      "  timesteps_since_restore: 1972000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1972000\n",
      "  training_iteration: 986\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3504 s, 986 iter, 1972000 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.147916932183573\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 985\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.017\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4643223285675049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00014019623631611466\n",
      "        policy_loss: -0.0009135780273936689\n",
      "        total_loss: 0.0001066436743712984\n",
      "        vf_explained_var: 0.7939542531967163\n",
      "        vf_loss: 0.0010202247649431229\n",
      "    load_time_ms: 1.415\n",
      "    num_steps_sampled: 1978000\n",
      "    num_steps_trained: 1978000\n",
      "    sample_time_ms: 2683.001\n",
      "    update_time_ms: 5.429\n",
      "  iterations_since_restore: 989\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.375\n",
      "    ram_util_percent: 64.25\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.27486140893317\n",
      "    mean_inference_ms: 1.4440925372071343\n",
      "    mean_processing_ms: 0.9386068049837754\n",
      "  time_since_restore: 3510.827832698822\n",
      "  time_this_iter_s: 2.3745973110198975\n",
      "  time_total_s: 3510.827832698822\n",
      "  timestamp: 1595952159\n",
      "  timesteps_since_restore: 1978000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1978000\n",
      "  training_iteration: 989\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3510 s, 989 iter, 1978000 ts, 12.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.161019700406031\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 990\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.857\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.049137264490127563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009252618998289108\n",
      "        policy_loss: -0.003011707216501236\n",
      "        total_loss: 0.04277145490050316\n",
      "        vf_explained_var: 0.5865193605422974\n",
      "        vf_loss: 0.04578317329287529\n",
      "    load_time_ms: 1.197\n",
      "    num_steps_sampled: 1982000\n",
      "    num_steps_trained: 1982000\n",
      "    sample_time_ms: 2616.024\n",
      "    update_time_ms: 5.901\n",
      "  iterations_since_restore: 991\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.93333333333333\n",
      "    ram_util_percent: 64.1\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.267518228427216\n",
      "    mean_inference_ms: 1.442532871152352\n",
      "    mean_processing_ms: 0.9381175057942905\n",
      "  time_since_restore: 3516.6062886714935\n",
      "  time_this_iter_s: 2.2108662128448486\n",
      "  time_total_s: 3516.6062886714935\n",
      "  timestamp: 1595952165\n",
      "  timesteps_since_restore: 1982000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1982000\n",
      "  training_iteration: 991\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3516 s, 991 iter, 1982000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.161019700406031\n",
      "  episode_reward_min: 11.582827318007082\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 990\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.859\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42478540539741516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.813317668274976e-06\n",
      "        policy_loss: 9.812927601160482e-05\n",
      "        total_loss: 0.0012799968244507909\n",
      "        vf_explained_var: 0.8232789635658264\n",
      "        vf_loss: 0.0011818729108199477\n",
      "    load_time_ms: 1.385\n",
      "    num_steps_sampled: 1986000\n",
      "    num_steps_trained: 1986000\n",
      "    sample_time_ms: 2753.969\n",
      "    update_time_ms: 5.166\n",
      "  iterations_since_restore: 993\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.48\n",
      "    ram_util_percent: 64.36\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.267518228427216\n",
      "    mean_inference_ms: 1.442532871152352\n",
      "    mean_processing_ms: 0.9381175057942905\n",
      "  time_since_restore: 3522.6775546073914\n",
      "  time_this_iter_s: 3.0536813735961914\n",
      "  time_total_s: 3522.6775546073914\n",
      "  timestamp: 1595952171\n",
      "  timesteps_since_restore: 1986000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1986000\n",
      "  training_iteration: 993\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3522 s, 993 iter, 1986000 ts, 12.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.186544534653267\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 995\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.515\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45732080936431885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012759694072883576\n",
      "        policy_loss: -0.0007634926005266607\n",
      "        total_loss: 0.4148380160331726\n",
      "        vf_explained_var: -0.011525630950927734\n",
      "        vf_loss: 0.4156014919281006\n",
      "    load_time_ms: 1.383\n",
      "    num_steps_sampled: 1990000\n",
      "    num_steps_trained: 1990000\n",
      "    sample_time_ms: 2657.678\n",
      "    update_time_ms: 5.285\n",
      "  iterations_since_restore: 995\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.339999999999996\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.260410459139106\n",
      "    mean_inference_ms: 1.4410263552441185\n",
      "    mean_processing_ms: 0.9376439072316449\n",
      "  time_since_restore: 3528.8718957901\n",
      "  time_this_iter_s: 3.391010046005249\n",
      "  time_total_s: 3528.8718957901\n",
      "  timestamp: 1595952177\n",
      "  timesteps_since_restore: 1990000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1990000\n",
      "  training_iteration: 995\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3528 s, 995 iter, 1990000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.186544534653267\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 995\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.958\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4192790389060974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001505494728917256\n",
      "        policy_loss: -0.000462146766949445\n",
      "        total_loss: 0.0006176948663778603\n",
      "        vf_explained_var: 0.8011970520019531\n",
      "        vf_loss: 0.0010798504808917642\n",
      "    load_time_ms: 1.36\n",
      "    num_steps_sampled: 1996000\n",
      "    num_steps_trained: 1996000\n",
      "    sample_time_ms: 2627.587\n",
      "    update_time_ms: 5.403\n",
      "  iterations_since_restore: 998\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.13333333333333\n",
      "    ram_util_percent: 64.23333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.260410459139106\n",
      "    mean_inference_ms: 1.4410263552441187\n",
      "    mean_processing_ms: 0.9376439072316451\n",
      "  time_since_restore: 3535.1917674541473\n",
      "  time_this_iter_s: 2.092085838317871\n",
      "  time_total_s: 3535.1917674541473\n",
      "  timestamp: 1595952184\n",
      "  timesteps_since_restore: 1996000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1996000\n",
      "  training_iteration: 998\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3535 s, 998 iter, 1996000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.19356816417323\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.934\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43534669280052185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007605269202031195\n",
      "        policy_loss: -0.00018470191571395844\n",
      "        total_loss: 0.4184100925922394\n",
      "        vf_explained_var: -0.006638288497924805\n",
      "        vf_loss: 0.4185948669910431\n",
      "    load_time_ms: 1.28\n",
      "    num_steps_sampled: 2000000\n",
      "    num_steps_trained: 2000000\n",
      "    sample_time_ms: 2617.13\n",
      "    update_time_ms: 4.615\n",
      "  iterations_since_restore: 1000\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.279999999999994\n",
      "    ram_util_percent: 64.34\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.253319891600902\n",
      "    mean_inference_ms: 1.4395177102210055\n",
      "    mean_processing_ms: 0.9371736403382899\n",
      "  time_since_restore: 3540.9673776626587\n",
      "  time_this_iter_s: 3.6260762214660645\n",
      "  time_total_s: 3540.9673776626587\n",
      "  timestamp: 1595952189\n",
      "  timesteps_since_restore: 2000000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2000000\n",
      "  training_iteration: 1000\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3540 s, 1000 iter, 2000000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.193568164173232\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.4\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2858445346355438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001550712389871478\n",
      "        policy_loss: -0.0006809302722103894\n",
      "        total_loss: 0.0030812013428658247\n",
      "        vf_explained_var: 0.8557937145233154\n",
      "        vf_loss: 0.0037621320225298405\n",
      "    load_time_ms: 1.128\n",
      "    num_steps_sampled: 2004000\n",
      "    num_steps_trained: 2004000\n",
      "    sample_time_ms: 2650.764\n",
      "    update_time_ms: 5.666\n",
      "  iterations_since_restore: 1002\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.1\n",
      "    ram_util_percent: 64.34\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.253319891600903\n",
      "    mean_inference_ms: 1.4395177102210055\n",
      "    mean_processing_ms: 0.9371736403382895\n",
      "  time_since_restore: 3546.56901884079\n",
      "  time_this_iter_s: 3.1747729778289795\n",
      "  time_total_s: 3546.56901884079\n",
      "  timestamp: 1595952195\n",
      "  timesteps_since_restore: 2004000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2004000\n",
      "  training_iteration: 1002\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3546 s, 1002 iter, 2004000 ts, 12.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.193568164173232\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.849\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4198755919933319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00022808140784036368\n",
      "        policy_loss: -0.0010047964751720428\n",
      "        total_loss: -2.371311211391003e-06\n",
      "        vf_explained_var: 0.8558553457260132\n",
      "        vf_loss: 0.0010024361545220017\n",
      "    load_time_ms: 1.185\n",
      "    num_steps_sampled: 2008000\n",
      "    num_steps_trained: 2008000\n",
      "    sample_time_ms: 2680.745\n",
      "    update_time_ms: 6.104\n",
      "  iterations_since_restore: 1004\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.725\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.253319891600903\n",
      "    mean_inference_ms: 1.4395177102210055\n",
      "    mean_processing_ms: 0.9371736403382895\n",
      "  time_since_restore: 3552.7715775966644\n",
      "  time_this_iter_s: 2.763946056365967\n",
      "  time_total_s: 3552.7715775966644\n",
      "  timestamp: 1595952201\n",
      "  timesteps_since_restore: 2008000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2008000\n",
      "  training_iteration: 1004\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3552 s, 1004 iter, 2008000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.215453745304679\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1005\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.11\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.020427614450454712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005150984041392803\n",
      "        policy_loss: -0.0011562748113647103\n",
      "        total_loss: 0.044989362359046936\n",
      "        vf_explained_var: 0.5970230102539062\n",
      "        vf_loss: 0.04614563658833504\n",
      "    load_time_ms: 1.172\n",
      "    num_steps_sampled: 2012000\n",
      "    num_steps_trained: 2012000\n",
      "    sample_time_ms: 2697.499\n",
      "    update_time_ms: 6.042\n",
      "  iterations_since_restore: 1006\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.9\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.246538364601451\n",
      "    mean_inference_ms: 1.4380661981940346\n",
      "    mean_processing_ms: 0.9367177845925257\n",
      "  time_since_restore: 3558.427769422531\n",
      "  time_this_iter_s: 2.1350491046905518\n",
      "  time_total_s: 3558.427769422531\n",
      "  timestamp: 1595952207\n",
      "  timesteps_since_restore: 2012000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2012000\n",
      "  training_iteration: 1006\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3558 s, 1006 iter, 2012000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.215453745304679\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1005\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.105\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4264393150806427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00013796165876556188\n",
      "        policy_loss: -0.0009017953998409212\n",
      "        total_loss: 4.0912627810030244e-06\n",
      "        vf_explained_var: 0.7920249104499817\n",
      "        vf_loss: 0.000905886641703546\n",
      "    load_time_ms: 1.169\n",
      "    num_steps_sampled: 2018000\n",
      "    num_steps_trained: 2018000\n",
      "    sample_time_ms: 2692.151\n",
      "    update_time_ms: 5.905\n",
      "  iterations_since_restore: 1009\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.7\n",
      "    ram_util_percent: 64.53333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.246538364601451\n",
      "    mean_inference_ms: 1.4380661981940346\n",
      "    mean_processing_ms: 0.9367177845925257\n",
      "  time_since_restore: 3564.748910188675\n",
      "  time_this_iter_s: 2.1275179386138916\n",
      "  time_total_s: 3564.748910188675\n",
      "  timestamp: 1595952213\n",
      "  timesteps_since_restore: 2018000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2018000\n",
      "  training_iteration: 1009\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3564 s, 1009 iter, 2018000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.231338292683272\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.018453126773238182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01058993674814701\n",
      "        policy_loss: -0.003577891271561384\n",
      "        total_loss: 0.042336173355579376\n",
      "        vf_explained_var: 0.6091337203979492\n",
      "        vf_loss: 0.04591405391693115\n",
      "    load_time_ms: 1.183\n",
      "    num_steps_sampled: 2022000\n",
      "    num_steps_trained: 2022000\n",
      "    sample_time_ms: 2625.162\n",
      "    update_time_ms: 5.941\n",
      "  iterations_since_restore: 1011\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.5\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.239729480555728\n",
      "    mean_inference_ms: 1.4366047481210475\n",
      "    mean_processing_ms: 0.936258581905772\n",
      "  time_since_restore: 3570.089767932892\n",
      "  time_this_iter_s: 2.076157569885254\n",
      "  time_total_s: 3570.089767932892\n",
      "  timestamp: 1595952219\n",
      "  timesteps_since_restore: 2022000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2022000\n",
      "  training_iteration: 1011\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3570 s, 1011 iter, 2022000 ts, 12.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.231338292683272\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.268\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4022929072380066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.832484278769698e-05\n",
      "        policy_loss: -2.5658964659669437e-05\n",
      "        total_loss: 0.0010808194056153297\n",
      "        vf_explained_var: 0.7182184457778931\n",
      "        vf_loss: 0.0011064793216064572\n",
      "    load_time_ms: 1.057\n",
      "    num_steps_sampled: 2028000\n",
      "    num_steps_trained: 2028000\n",
      "    sample_time_ms: 2340.575\n",
      "    update_time_ms: 3.962\n",
      "  iterations_since_restore: 1014\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.8\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.239729480555728\n",
      "    mean_inference_ms: 1.4366047481210475\n",
      "    mean_processing_ms: 0.936258581905772\n",
      "  time_since_restore: 3576.536318063736\n",
      "  time_this_iter_s: 2.1078290939331055\n",
      "  time_total_s: 3576.536318063736\n",
      "  timestamp: 1595952225\n",
      "  timesteps_since_restore: 2028000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2028000\n",
      "  training_iteration: 1014\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3576 s, 1014 iter, 2028000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.24039479930163\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1015\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.261\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.013671155087649822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006038257852196693\n",
      "        policy_loss: -0.0030410918407142162\n",
      "        total_loss: 0.04299860820174217\n",
      "        vf_explained_var: 0.6100587248802185\n",
      "        vf_loss: 0.04603969678282738\n",
      "    load_time_ms: 1.049\n",
      "    num_steps_sampled: 2032000\n",
      "    num_steps_trained: 2032000\n",
      "    sample_time_ms: 2307.201\n",
      "    update_time_ms: 3.971\n",
      "  iterations_since_restore: 1016\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.0\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2327950740978135\n",
      "    mean_inference_ms: 1.4351055978073866\n",
      "    mean_processing_ms: 0.9357895361828358\n",
      "  time_since_restore: 3581.8592870235443\n",
      "  time_this_iter_s: 2.0648767948150635\n",
      "  time_total_s: 3581.8592870235443\n",
      "  timestamp: 1595952231\n",
      "  timesteps_since_restore: 2032000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2032000\n",
      "  training_iteration: 1016\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3581 s, 1016 iter, 2032000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.70688362756533\n",
      "  episode_reward_mean: 12.24039479930163\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1015\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.567\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38936203718185425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3686886834184406e-06\n",
      "        policy_loss: -2.0647048586397432e-05\n",
      "        total_loss: 0.0011412010062485933\n",
      "        vf_explained_var: 0.6263555288314819\n",
      "        vf_loss: 0.001161852735094726\n",
      "    load_time_ms: 1.056\n",
      "    num_steps_sampled: 2038000\n",
      "    num_steps_trained: 2038000\n",
      "    sample_time_ms: 2317.401\n",
      "    update_time_ms: 4.052\n",
      "  iterations_since_restore: 1019\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.925\n",
      "    ram_util_percent: 64.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2327950740978135\n",
      "    mean_inference_ms: 1.4351055978073866\n",
      "    mean_processing_ms: 0.9357895361828358\n",
      "  time_since_restore: 3588.2868180274963\n",
      "  time_this_iter_s: 2.200134754180908\n",
      "  time_total_s: 3588.2868180274963\n",
      "  timestamp: 1595952237\n",
      "  timesteps_since_restore: 2038000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2038000\n",
      "  training_iteration: 1019\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3588 s, 1019 iter, 2038000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.656212212773392\n",
      "  episode_reward_mean: 12.235620389423946\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.703\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.0038682103622704744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001223649742314592\n",
      "        policy_loss: -0.00011769485718104988\n",
      "        total_loss: 0.046131160110235214\n",
      "        vf_explained_var: 0.5925347805023193\n",
      "        vf_loss: 0.046248842030763626\n",
      "    load_time_ms: 1.035\n",
      "    num_steps_sampled: 2042000\n",
      "    num_steps_trained: 2042000\n",
      "    sample_time_ms: 2322.367\n",
      "    update_time_ms: 4.072\n",
      "  iterations_since_restore: 1021\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.666666666666664\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.225558056929884\n",
      "    mean_inference_ms: 1.433535971348026\n",
      "    mean_processing_ms: 0.9353062805690558\n",
      "  time_since_restore: 3593.679780960083\n",
      "  time_this_iter_s: 2.1275837421417236\n",
      "  time_total_s: 3593.679780960083\n",
      "  timestamp: 1595952242\n",
      "  timesteps_since_restore: 2042000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2042000\n",
      "  training_iteration: 1021\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3593 s, 1021 iter, 2042000 ts, 12.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.656212212773392\n",
      "  episode_reward_mean: 12.235620389423946\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.002\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3625120222568512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005893782945349813\n",
      "        policy_loss: -0.0003905754128936678\n",
      "        total_loss: 0.000905220047570765\n",
      "        vf_explained_var: 0.7204900979995728\n",
      "        vf_loss: 0.001295790309086442\n",
      "    load_time_ms: 1.049\n",
      "    num_steps_sampled: 2048000\n",
      "    num_steps_trained: 2048000\n",
      "    sample_time_ms: 2313.514\n",
      "    update_time_ms: 4.192\n",
      "  iterations_since_restore: 1024\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.63333333333333\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.225558056929884\n",
      "    mean_inference_ms: 1.433535971348026\n",
      "    mean_processing_ms: 0.9353062805690558\n",
      "  time_since_restore: 3600.0312242507935\n",
      "  time_this_iter_s: 2.1184897422790527\n",
      "  time_total_s: 3600.0312242507935\n",
      "  timestamp: 1595952249\n",
      "  timesteps_since_restore: 2048000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2048000\n",
      "  training_iteration: 1024\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3600 s, 1024 iter, 2048000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.718048693253886\n",
      "  episode_reward_mean: 12.247094271786354\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1025\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.979\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.03500417247414589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003124767972622067\n",
      "        policy_loss: -0.0004863238427788019\n",
      "        total_loss: 0.047073397785425186\n",
      "        vf_explained_var: 0.5310268402099609\n",
      "        vf_loss: 0.047559719532728195\n",
      "    load_time_ms: 1.059\n",
      "    num_steps_sampled: 2052000\n",
      "    num_steps_trained: 2052000\n",
      "    sample_time_ms: 2332.43\n",
      "    update_time_ms: 4.231\n",
      "  iterations_since_restore: 1026\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.13333333333333\n",
      "    ram_util_percent: 64.2\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.218257901416843\n",
      "    mean_inference_ms: 1.4319425684671259\n",
      "    mean_processing_ms: 0.9348212182978247\n",
      "  time_since_restore: 3605.5446128845215\n",
      "  time_this_iter_s: 2.1083786487579346\n",
      "  time_total_s: 3605.5446128845215\n",
      "  timestamp: 1595952254\n",
      "  timesteps_since_restore: 2052000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2052000\n",
      "  training_iteration: 1026\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3605 s, 1026 iter, 2052000 ts, 12.2 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.718048693253886\n",
      "  episode_reward_mean: 12.247094271786354\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1025\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.03\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.32202184200286865\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003906603436917067\n",
      "        policy_loss: -0.0037858772557228804\n",
      "        total_loss: -0.0023018685169517994\n",
      "        vf_explained_var: 0.7497837543487549\n",
      "        vf_loss: 0.001483995234593749\n",
      "    load_time_ms: 1.082\n",
      "    num_steps_sampled: 2058000\n",
      "    num_steps_trained: 2058000\n",
      "    sample_time_ms: 2347.847\n",
      "    update_time_ms: 4.206\n",
      "  iterations_since_restore: 1029\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.26666666666667\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.218257901416843\n",
      "    mean_inference_ms: 1.4319425684671259\n",
      "    mean_processing_ms: 0.9348212182978247\n",
      "  time_since_restore: 3612.125806570053\n",
      "  time_this_iter_s: 2.1380157470703125\n",
      "  time_total_s: 3612.125806570053\n",
      "  timestamp: 1595952261\n",
      "  timesteps_since_restore: 2058000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2058000\n",
      "  training_iteration: 1029\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3612 s, 1029 iter, 2058000 ts, 12.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.300161875597466\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.02024429850280285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01060409750789404\n",
      "        policy_loss: -0.0022192259784787893\n",
      "        total_loss: 0.04288824647665024\n",
      "        vf_explained_var: 0.6286404132843018\n",
      "        vf_loss: 0.04510747641324997\n",
      "    load_time_ms: 1.082\n",
      "    num_steps_sampled: 2062000\n",
      "    num_steps_trained: 2062000\n",
      "    sample_time_ms: 2351.527\n",
      "    update_time_ms: 4.234\n",
      "  iterations_since_restore: 1031\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.699999999999996\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2109550424255415\n",
      "    mean_inference_ms: 1.430345619304961\n",
      "    mean_processing_ms: 0.9343398200022034\n",
      "  time_since_restore: 3617.5609652996063\n",
      "  time_this_iter_s: 2.182857036590576\n",
      "  time_total_s: 3617.5609652996063\n",
      "  timestamp: 1595952266\n",
      "  timesteps_since_restore: 2062000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2062000\n",
      "  training_iteration: 1031\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3617 s, 1031 iter, 2062000 ts, 12.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.300161875597466\n",
      "  episode_reward_min: 11.802681132572525\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.765\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3863304555416107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006703626131638885\n",
      "        policy_loss: -0.0015458643902093172\n",
      "        total_loss: -0.0007134447223506868\n",
      "        vf_explained_var: 0.8850376605987549\n",
      "        vf_loss: 0.0008324201917275786\n",
      "    load_time_ms: 1.097\n",
      "    num_steps_sampled: 2068000\n",
      "    num_steps_trained: 2068000\n",
      "    sample_time_ms: 2356.756\n",
      "    update_time_ms: 4.505\n",
      "  iterations_since_restore: 1034\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.03333333333333\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2109550424255415\n",
      "    mean_inference_ms: 1.430345619304961\n",
      "    mean_processing_ms: 0.9343398200022034\n",
      "  time_since_restore: 3623.9706563949585\n",
      "  time_this_iter_s: 2.1539876461029053\n",
      "  time_total_s: 3623.9706563949585\n",
      "  timestamp: 1595952273\n",
      "  timesteps_since_restore: 2068000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2068000\n",
      "  training_iteration: 1034\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3623 s, 1034 iter, 2068000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.314750278347594\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1035\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.0539206862449646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.775263995630667e-05\n",
      "        policy_loss: 0.0001002206772682257\n",
      "        total_loss: 0.047228287905454636\n",
      "        vf_explained_var: 0.602036714553833\n",
      "        vf_loss: 0.04712805896997452\n",
      "    load_time_ms: 1.096\n",
      "    num_steps_sampled: 2072000\n",
      "    num_steps_trained: 2072000\n",
      "    sample_time_ms: 2338.412\n",
      "    update_time_ms: 4.435\n",
      "  iterations_since_restore: 1036\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.833333333333336\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2031566296092375\n",
      "    mean_inference_ms: 1.4286263527166534\n",
      "    mean_processing_ms: 0.9338053904924927\n",
      "  time_since_restore: 3629.29847574234\n",
      "  time_this_iter_s: 2.093294382095337\n",
      "  time_total_s: 3629.29847574234\n",
      "  timestamp: 1595952278\n",
      "  timesteps_since_restore: 2072000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2072000\n",
      "  training_iteration: 1036\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3629 s, 1036 iter, 2072000 ts, 12.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.314750278347594\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1035\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.143\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.35605618357658386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012805912410840392\n",
      "        policy_loss: -0.0007573699695058167\n",
      "        total_loss: 7.580661622341722e-05\n",
      "        vf_explained_var: 0.8438408374786377\n",
      "        vf_loss: 0.0008331844583153725\n",
      "    load_time_ms: 1.073\n",
      "    num_steps_sampled: 2078000\n",
      "    num_steps_trained: 2078000\n",
      "    sample_time_ms: 2319.963\n",
      "    update_time_ms: 4.336\n",
      "  iterations_since_restore: 1039\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.60000000000001\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.2031566296092375\n",
      "    mean_inference_ms: 1.4286263527166534\n",
      "    mean_processing_ms: 0.9338053904924927\n",
      "  time_since_restore: 3635.6880345344543\n",
      "  time_this_iter_s: 2.114436149597168\n",
      "  time_total_s: 3635.6880345344543\n",
      "  timestamp: 1595952285\n",
      "  timesteps_since_restore: 2078000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2078000\n",
      "  training_iteration: 1039\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3635 s, 1039 iter, 2078000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.322244785994293\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.638\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.06404539197683334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002349922899156809\n",
      "        policy_loss: -0.00027187252999283373\n",
      "        total_loss: 0.04560033977031708\n",
      "        vf_explained_var: 0.6318893432617188\n",
      "        vf_loss: 0.04587221145629883\n",
      "    load_time_ms: 1.071\n",
      "    num_steps_sampled: 2082000\n",
      "    num_steps_trained: 2082000\n",
      "    sample_time_ms: 2309.15\n",
      "    update_time_ms: 4.368\n",
      "  iterations_since_restore: 1041\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53333333333334\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.193918517388227\n",
      "    mean_inference_ms: 1.4265722459946093\n",
      "    mean_processing_ms: 0.9331902549624017\n",
      "  time_since_restore: 3641.0092923641205\n",
      "  time_this_iter_s: 2.097944974899292\n",
      "  time_total_s: 3641.0092923641205\n",
      "  timestamp: 1595952290\n",
      "  timesteps_since_restore: 2082000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2082000\n",
      "  training_iteration: 1041\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3641 s, 1041 iter, 2082000 ts, 12.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-04-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.322244785994293\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.104\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36212098598480225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00044381406041793525\n",
      "        policy_loss: -0.0016202469123527408\n",
      "        total_loss: -0.0008667869842611253\n",
      "        vf_explained_var: 0.8004154562950134\n",
      "        vf_loss: 0.0007534671458415687\n",
      "    load_time_ms: 1.047\n",
      "    num_steps_sampled: 2088000\n",
      "    num_steps_trained: 2088000\n",
      "    sample_time_ms: 2307.036\n",
      "    update_time_ms: 4.049\n",
      "  iterations_since_restore: 1044\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.63333333333334\n",
      "    ram_util_percent: 64.33333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.193918517388227\n",
      "    mean_inference_ms: 1.4265722459946093\n",
      "    mean_processing_ms: 0.9331902549624017\n",
      "  time_since_restore: 3647.387576341629\n",
      "  time_this_iter_s: 2.1936135292053223\n",
      "  time_total_s: 3647.387576341629\n",
      "  timestamp: 1595952296\n",
      "  timesteps_since_restore: 2088000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2088000\n",
      "  training_iteration: 1044\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3647 s, 1044 iter, 2088000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.310934697966957\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1045\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.54\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.08351270109415054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009559686295688152\n",
      "        policy_loss: -0.0026072596665471792\n",
      "        total_loss: 0.044170163571834564\n",
      "        vf_explained_var: 0.6200321912765503\n",
      "        vf_loss: 0.046777430921792984\n",
      "    load_time_ms: 1.046\n",
      "    num_steps_sampled: 2092000\n",
      "    num_steps_trained: 2092000\n",
      "    sample_time_ms: 2306.333\n",
      "    update_time_ms: 4.153\n",
      "  iterations_since_restore: 1046\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.800000000000004\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.184563248476092\n",
      "    mean_inference_ms: 1.4244836618935697\n",
      "    mean_processing_ms: 0.9325700518045175\n",
      "  time_since_restore: 3652.718684196472\n",
      "  time_this_iter_s: 2.0764029026031494\n",
      "  time_total_s: 3652.718684196472\n",
      "  timestamp: 1595952302\n",
      "  timesteps_since_restore: 2092000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2092000\n",
      "  training_iteration: 1046\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3652 s, 1046 iter, 2092000 ts, 12.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.310934697966957\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1045\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.7\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3536534309387207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012060365406796336\n",
      "        policy_loss: -0.0011999454582110047\n",
      "        total_loss: -0.0005768914124928415\n",
      "        vf_explained_var: 0.8545697331428528\n",
      "        vf_loss: 0.0006230534636415541\n",
      "    load_time_ms: 1.066\n",
      "    num_steps_sampled: 2098000\n",
      "    num_steps_trained: 2098000\n",
      "    sample_time_ms: 2304.45\n",
      "    update_time_ms: 4.177\n",
      "  iterations_since_restore: 1049\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.33333333333333\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.184563248476092\n",
      "    mean_inference_ms: 1.4244836618935697\n",
      "    mean_processing_ms: 0.9325700518045175\n",
      "  time_since_restore: 3659.0911157131195\n",
      "  time_this_iter_s: 2.1036105155944824\n",
      "  time_total_s: 3659.0911157131195\n",
      "  timestamp: 1595952308\n",
      "  timesteps_since_restore: 2098000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2098000\n",
      "  training_iteration: 1049\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3659 s, 1049 iter, 2098000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.311679942405187\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1050\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.595\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.1000455766916275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0015727534191682935\n",
      "        policy_loss: -0.0014103762805461884\n",
      "        total_loss: 0.043940216302871704\n",
      "        vf_explained_var: 0.6465206146240234\n",
      "        vf_loss: 0.045350585132837296\n",
      "    load_time_ms: 1.065\n",
      "    num_steps_sampled: 2102000\n",
      "    num_steps_trained: 2102000\n",
      "    sample_time_ms: 2338.5\n",
      "    update_time_ms: 4.122\n",
      "  iterations_since_restore: 1051\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.23333333333333\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.175250271545602\n",
      "    mean_inference_ms: 1.422402410775965\n",
      "    mean_processing_ms: 0.9319528249748958\n",
      "  time_since_restore: 3664.789311647415\n",
      "  time_this_iter_s: 2.242856025695801\n",
      "  time_total_s: 3664.789311647415\n",
      "  timestamp: 1595952314\n",
      "  timesteps_since_restore: 2102000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2102000\n",
      "  training_iteration: 1051\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3664 s, 1051 iter, 2102000 ts, 12.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.311679942405187\n",
      "  episode_reward_min: 11.948573781850294\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1050\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 29.258\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.35266777873039246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035834484733641148\n",
      "        policy_loss: -0.006728469859808683\n",
      "        total_loss: -0.006096876226365566\n",
      "        vf_explained_var: 0.8111460208892822\n",
      "        vf_loss: 0.0006316041690297425\n",
      "    load_time_ms: 1.186\n",
      "    num_steps_sampled: 2108000\n",
      "    num_steps_trained: 2108000\n",
      "    sample_time_ms: 2469.44\n",
      "    update_time_ms: 6.047\n",
      "  iterations_since_restore: 1054\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.775\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.175250271545602\n",
      "    mean_inference_ms: 1.422402410775965\n",
      "    mean_processing_ms: 0.9319528249748958\n",
      "  time_since_restore: 3672.5539679527283\n",
      "  time_this_iter_s: 3.0334787368774414\n",
      "  time_total_s: 3672.5539679527283\n",
      "  timestamp: 1595952322\n",
      "  timesteps_since_restore: 2108000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2108000\n",
      "  training_iteration: 1054\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3672 s, 1054 iter, 2108000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.298417456363332\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1055\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.014\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.10834717005491257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0012874004896730185\n",
      "        policy_loss: -0.0008775811293162405\n",
      "        total_loss: 0.04545699432492256\n",
      "        vf_explained_var: 0.6387875080108643\n",
      "        vf_loss: 0.04633456468582153\n",
      "    load_time_ms: 1.278\n",
      "    num_steps_sampled: 2112000\n",
      "    num_steps_trained: 2112000\n",
      "    sample_time_ms: 2522.284\n",
      "    update_time_ms: 6.743\n",
      "  iterations_since_restore: 1056\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.925\n",
      "    ram_util_percent: 64.475\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.165839867486178\n",
      "    mean_inference_ms: 1.420299465719751\n",
      "    mean_processing_ms: 0.9313374612853083\n",
      "  time_since_restore: 3678.4670009613037\n",
      "  time_this_iter_s: 2.4535610675811768\n",
      "  time_total_s: 3678.4670009613037\n",
      "  timestamp: 1595952327\n",
      "  timesteps_since_restore: 2112000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2112000\n",
      "  training_iteration: 1056\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3678 s, 1056 iter, 2112000 ts, 12.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.298417456363332\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1055\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.203\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2778174877166748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00013122326345182955\n",
      "        policy_loss: -0.0005211958778090775\n",
      "        total_loss: 0.0004502992669586092\n",
      "        vf_explained_var: 0.49794530868530273\n",
      "        vf_loss: 0.0009714933694340289\n",
      "    load_time_ms: 1.271\n",
      "    num_steps_sampled: 2118000\n",
      "    num_steps_trained: 2118000\n",
      "    sample_time_ms: 2535.299\n",
      "    update_time_ms: 6.689\n",
      "  iterations_since_restore: 1059\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.26666666666667\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.165839867486178\n",
      "    mean_inference_ms: 1.420299465719751\n",
      "    mean_processing_ms: 0.9313374612853083\n",
      "  time_since_restore: 3684.971568107605\n",
      "  time_this_iter_s: 2.1578726768493652\n",
      "  time_total_s: 3684.971568107605\n",
      "  timestamp: 1595952334\n",
      "  timesteps_since_restore: 2118000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2118000\n",
      "  training_iteration: 1059\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3684 s, 1059 iter, 2118000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.30872489442151\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 37.181\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.1429593563079834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009664895944297314\n",
      "        policy_loss: -0.003080686554312706\n",
      "        total_loss: 0.04475635662674904\n",
      "        vf_explained_var: 0.6142561435699463\n",
      "        vf_loss: 0.047837015241384506\n",
      "    load_time_ms: 1.445\n",
      "    num_steps_sampled: 2122000\n",
      "    num_steps_trained: 2122000\n",
      "    sample_time_ms: 2601.315\n",
      "    update_time_ms: 7.515\n",
      "  iterations_since_restore: 1061\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.75\n",
      "    ram_util_percent: 64.55\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.15653235618186\n",
      "    mean_inference_ms: 1.4182202223093032\n",
      "    mean_processing_ms: 0.9307359864691327\n",
      "  time_since_restore: 3691.393320083618\n",
      "  time_this_iter_s: 2.9276814460754395\n",
      "  time_total_s: 3691.393320083618\n",
      "  timestamp: 1595952340\n",
      "  timesteps_since_restore: 2122000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2122000\n",
      "  training_iteration: 1061\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3691 s, 1061 iter, 2122000 ts, 12.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.30872489442151\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.625\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2555045485496521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.071391544537619e-05\n",
      "        policy_loss: -0.00016200923710130155\n",
      "        total_loss: 0.00076046132016927\n",
      "        vf_explained_var: 0.7118685245513916\n",
      "        vf_loss: 0.0009224663954228163\n",
      "    load_time_ms: 1.322\n",
      "    num_steps_sampled: 2128000\n",
      "    num_steps_trained: 2128000\n",
      "    sample_time_ms: 2489.907\n",
      "    update_time_ms: 6.286\n",
      "  iterations_since_restore: 1064\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.76666666666667\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.15653235618186\n",
      "    mean_inference_ms: 1.4182202223093032\n",
      "    mean_processing_ms: 0.9307359864691327\n",
      "  time_since_restore: 3697.988237142563\n",
      "  time_this_iter_s: 2.0974626541137695\n",
      "  time_total_s: 3697.988237142563\n",
      "  timestamp: 1595952347\n",
      "  timesteps_since_restore: 2128000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2128000\n",
      "  training_iteration: 1064\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3697 s, 1064 iter, 2128000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.32474186148895\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1065\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.555\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.1600562483072281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.184266905824188e-06\n",
      "        policy_loss: -8.403205720242113e-05\n",
      "        total_loss: 0.04863806068897247\n",
      "        vf_explained_var: 0.6082496643066406\n",
      "        vf_loss: 0.048722099512815475\n",
      "    load_time_ms: 1.235\n",
      "    num_steps_sampled: 2132000\n",
      "    num_steps_trained: 2132000\n",
      "    sample_time_ms: 2461.074\n",
      "    update_time_ms: 6.335\n",
      "  iterations_since_restore: 1066\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.53333333333332\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.147344306118651\n",
      "    mean_inference_ms: 1.4161633820101631\n",
      "    mean_processing_ms: 0.930143643159728\n",
      "  time_since_restore: 3703.560537815094\n",
      "  time_this_iter_s: 2.1110007762908936\n",
      "  time_total_s: 3703.560537815094\n",
      "  timestamp: 1595952353\n",
      "  timesteps_since_restore: 2132000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2132000\n",
      "  training_iteration: 1066\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3703 s, 1066 iter, 2132000 ts, 12.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.32474186148895\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1065\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.547\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.25356414914131165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00023158240946941078\n",
      "        policy_loss: -0.0003111390979029238\n",
      "        total_loss: 0.0006062097381800413\n",
      "        vf_explained_var: 0.6636900901794434\n",
      "        vf_loss: 0.000917361001484096\n",
      "    load_time_ms: 1.232\n",
      "    num_steps_sampled: 2138000\n",
      "    num_steps_trained: 2138000\n",
      "    sample_time_ms: 2465.205\n",
      "    update_time_ms: 6.81\n",
      "  iterations_since_restore: 1069\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.86666666666666\n",
      "    ram_util_percent: 64.43333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.147344306118651\n",
      "    mean_inference_ms: 1.4161633820101631\n",
      "    mean_processing_ms: 0.930143643159728\n",
      "  time_since_restore: 3710.1144552230835\n",
      "  time_this_iter_s: 2.181880235671997\n",
      "  time_total_s: 3710.1144552230835\n",
      "  timestamp: 1595952359\n",
      "  timesteps_since_restore: 2138000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2138000\n",
      "  training_iteration: 1069\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3710 s, 1069 iter, 2138000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.339275137420946\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1070\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.739\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.16774390637874603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00952819176018238\n",
      "        policy_loss: -0.0019251556368544698\n",
      "        total_loss: 0.04724428430199623\n",
      "        vf_explained_var: 0.6056028604507446\n",
      "        vf_loss: 0.04916946589946747\n",
      "    load_time_ms: 1.3\n",
      "    num_steps_sampled: 2142000\n",
      "    num_steps_trained: 2142000\n",
      "    sample_time_ms: 2396.527\n",
      "    update_time_ms: 5.95\n",
      "  iterations_since_restore: 1071\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60000000000001\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1381256897916865\n",
      "    mean_inference_ms: 1.4140882251227274\n",
      "    mean_processing_ms: 0.9295501575592034\n",
      "  time_since_restore: 3715.8227643966675\n",
      "  time_this_iter_s: 2.389677047729492\n",
      "  time_total_s: 3715.8227643966675\n",
      "  timestamp: 1595952365\n",
      "  timesteps_since_restore: 2142000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2142000\n",
      "  training_iteration: 1071\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3715 s, 1071 iter, 2142000 ts, 12.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.339275137420946\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1070\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.017\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.24361567199230194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.101412625663215e-06\n",
      "        policy_loss: -1.814937604649458e-05\n",
      "        total_loss: 0.0008973274379968643\n",
      "        vf_explained_var: 0.667052149772644\n",
      "        vf_loss: 0.0009154768195003271\n",
      "    load_time_ms: 1.3\n",
      "    num_steps_sampled: 2148000\n",
      "    num_steps_trained: 2148000\n",
      "    sample_time_ms: 2431.663\n",
      "    update_time_ms: 6.808\n",
      "  iterations_since_restore: 1074\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.06666666666666\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1381256897916865\n",
      "    mean_inference_ms: 1.4140882251227274\n",
      "    mean_processing_ms: 0.9295501575592034\n",
      "  time_since_restore: 3722.768723964691\n",
      "  time_this_iter_s: 2.4089598655700684\n",
      "  time_total_s: 3722.768723964691\n",
      "  timestamp: 1595952372\n",
      "  timesteps_since_restore: 2148000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2148000\n",
      "  training_iteration: 1074\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3722 s, 1074 iter, 2148000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.348776638032284\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.987\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.15998296439647675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006855764426290989\n",
      "        policy_loss: -0.0026086722500622272\n",
      "        total_loss: 0.04421549662947655\n",
      "        vf_explained_var: 0.6393896341323853\n",
      "        vf_loss: 0.046824172139167786\n",
      "    load_time_ms: 1.305\n",
      "    num_steps_sampled: 2152000\n",
      "    num_steps_trained: 2152000\n",
      "    sample_time_ms: 2461.664\n",
      "    update_time_ms: 5.985\n",
      "  iterations_since_restore: 1076\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.074999999999996\n",
      "    ram_util_percent: 64.55\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1290839785802245\n",
      "    mean_inference_ms: 1.4120536072084402\n",
      "    mean_processing_ms: 0.9289682813881744\n",
      "  time_since_restore: 3728.631242752075\n",
      "  time_this_iter_s: 2.331737756729126\n",
      "  time_total_s: 3728.631242752075\n",
      "  timestamp: 1595952378\n",
      "  timesteps_since_restore: 2152000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2152000\n",
      "  training_iteration: 1076\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3728 s, 1076 iter, 2152000 ts, 12.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-25\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.348776638032284\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.842\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.26434335112571716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.2133738790871575e-05\n",
      "        policy_loss: 5.2058221626793966e-05\n",
      "        total_loss: 0.0008489990141242743\n",
      "        vf_explained_var: 0.7907741069793701\n",
      "        vf_loss: 0.0007969385478645563\n",
      "    load_time_ms: 1.305\n",
      "    num_steps_sampled: 2158000\n",
      "    num_steps_trained: 2158000\n",
      "    sample_time_ms: 2497.145\n",
      "    update_time_ms: 5.633\n",
      "  iterations_since_restore: 1079\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.2\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.1290839785802245\n",
      "    mean_inference_ms: 1.4120536072084402\n",
      "    mean_processing_ms: 0.9289682813881744\n",
      "  time_since_restore: 3735.5323855876923\n",
      "  time_this_iter_s: 2.2817440032958984\n",
      "  time_total_s: 3735.5323855876923\n",
      "  timestamp: 1595952385\n",
      "  timesteps_since_restore: 2158000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2158000\n",
      "  training_iteration: 1079\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3735 s, 1079 iter, 2158000 ts, 12.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.358827404663343\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.376\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.15798862278461456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.379412712296471e-05\n",
      "        policy_loss: 3.034591600226122e-06\n",
      "        total_loss: 0.046746302396059036\n",
      "        vf_explained_var: 0.6465085744857788\n",
      "        vf_loss: 0.046743255108594894\n",
      "    load_time_ms: 1.041\n",
      "    num_steps_sampled: 2162000\n",
      "    num_steps_trained: 2162000\n",
      "    sample_time_ms: 2549.707\n",
      "    update_time_ms: 5.866\n",
      "  iterations_since_restore: 1081\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.26666666666667\n",
      "    ram_util_percent: 64.56666666666666\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.120206927283824\n",
      "    mean_inference_ms: 1.4100560987195394\n",
      "    mean_processing_ms: 0.928396849383503\n",
      "  time_since_restore: 3741.7229402065277\n",
      "  time_this_iter_s: 2.340390205383301\n",
      "  time_total_s: 3741.7229402065277\n",
      "  timestamp: 1595952391\n",
      "  timesteps_since_restore: 2162000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2162000\n",
      "  training_iteration: 1081\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3741 s, 1081 iter, 2162000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.358827404663343\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.27564239501953125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00015632495342288166\n",
      "        policy_loss: -0.0009680175571702421\n",
      "        total_loss: -0.0002771291765384376\n",
      "        vf_explained_var: 0.816662073135376\n",
      "        vf_loss: 0.0006908811046741903\n",
      "    load_time_ms: 1.06\n",
      "    num_steps_sampled: 2168000\n",
      "    num_steps_trained: 2168000\n",
      "    sample_time_ms: 2560.997\n",
      "    update_time_ms: 6.078\n",
      "  iterations_since_restore: 1084\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.3\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.120206927283824\n",
      "    mean_inference_ms: 1.4100560987195394\n",
      "    mean_processing_ms: 0.928396849383503\n",
      "  time_since_restore: 3748.780546426773\n",
      "  time_this_iter_s: 2.3832452297210693\n",
      "  time_total_s: 3748.780546426773\n",
      "  timestamp: 1595952398\n",
      "  timesteps_since_restore: 2168000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2168000\n",
      "  training_iteration: 1084\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3748 s, 1084 iter, 2168000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.369287563006242\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1085\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.018\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.1619308590888977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004424898885190487\n",
      "        policy_loss: -0.0011221247259527445\n",
      "        total_loss: 0.044604603201150894\n",
      "        vf_explained_var: 0.6596852540969849\n",
      "        vf_loss: 0.04572674259543419\n",
      "    load_time_ms: 1.083\n",
      "    num_steps_sampled: 2172000\n",
      "    num_steps_trained: 2172000\n",
      "    sample_time_ms: 2546.119\n",
      "    update_time_ms: 6.011\n",
      "  iterations_since_restore: 1086\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.6\n",
      "    ram_util_percent: 64.43333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.111238318383592\n",
      "    mean_inference_ms: 1.408043142962819\n",
      "    mean_processing_ms: 0.927821243762273\n",
      "  time_since_restore: 3754.4922659397125\n",
      "  time_this_iter_s: 2.086500644683838\n",
      "  time_total_s: 3754.4922659397125\n",
      "  timestamp: 1595952404\n",
      "  timesteps_since_restore: 2172000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2172000\n",
      "  training_iteration: 1086\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3754 s, 1086 iter, 2172000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.369287563006242\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1085\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.444\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.28833338618278503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001928982324898243\n",
      "        policy_loss: -0.003043468575924635\n",
      "        total_loss: -0.0023589134216308594\n",
      "        vf_explained_var: 0.7421095371246338\n",
      "        vf_loss: 0.0006845460156910121\n",
      "    load_time_ms: 1.151\n",
      "    num_steps_sampled: 2178000\n",
      "    num_steps_trained: 2178000\n",
      "    sample_time_ms: 2525.878\n",
      "    update_time_ms: 6.054\n",
      "  iterations_since_restore: 1089\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.93333333333332\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.111238318383592\n",
      "    mean_inference_ms: 1.408043142962819\n",
      "    mean_processing_ms: 0.927821243762273\n",
      "  time_since_restore: 3761.1948368549347\n",
      "  time_this_iter_s: 2.3075361251831055\n",
      "  time_total_s: 3761.1948368549347\n",
      "  timestamp: 1595952411\n",
      "  timesteps_since_restore: 2178000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2178000\n",
      "  training_iteration: 1089\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3761 s, 1089 iter, 2178000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-06-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.367371466233077\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1090\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.85\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.17653940618038177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00633964641019702\n",
      "        policy_loss: -0.0026080035604536533\n",
      "        total_loss: 0.045765481889247894\n",
      "        vf_explained_var: 0.6261807680130005\n",
      "        vf_loss: 0.048373498022556305\n",
      "    load_time_ms: 1.439\n",
      "    num_steps_sampled: 2182000\n",
      "    num_steps_trained: 2182000\n",
      "    sample_time_ms: 2519.679\n",
      "    update_time_ms: 5.937\n",
      "  iterations_since_restore: 1091\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.050000000000004\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.102353800274005\n",
      "    mean_inference_ms: 1.4060527445836737\n",
      "    mean_processing_ms: 0.9272542142866866\n",
      "  time_since_restore: 3767.3252720832825\n",
      "  time_this_iter_s: 2.430652379989624\n",
      "  time_total_s: 3767.3252720832825\n",
      "  timestamp: 1595952417\n",
      "  timesteps_since_restore: 2182000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2182000\n",
      "  training_iteration: 1091\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3767 s, 1091 iter, 2182000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.367371466233077\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1090\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.919\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.23978041112422943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006606184178963304\n",
      "        policy_loss: -0.000588800641708076\n",
      "        total_loss: 0.0002138693380402401\n",
      "        vf_explained_var: 0.7841747999191284\n",
      "        vf_loss: 0.0008026500581763685\n",
      "    load_time_ms: 1.42\n",
      "    num_steps_sampled: 2188000\n",
      "    num_steps_trained: 2188000\n",
      "    sample_time_ms: 2482.7\n",
      "    update_time_ms: 4.232\n",
      "  iterations_since_restore: 1094\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43333333333334\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.102353800274005\n",
      "    mean_inference_ms: 1.4060527445836737\n",
      "    mean_processing_ms: 0.9272542142866866\n",
      "  time_since_restore: 3773.9966061115265\n",
      "  time_this_iter_s: 2.1461730003356934\n",
      "  time_total_s: 3773.9966061115265\n",
      "  timestamp: 1595952423\n",
      "  timesteps_since_restore: 2188000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2188000\n",
      "  training_iteration: 1094\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3773 s, 1094 iter, 2188000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.380234323791495\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1095\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.237\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.17854198813438416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005220267921686172\n",
      "        policy_loss: -0.00039589786319993436\n",
      "        total_loss: 0.04518543928861618\n",
      "        vf_explained_var: 0.6701016426086426\n",
      "        vf_loss: 0.04558134078979492\n",
      "    load_time_ms: 1.402\n",
      "    num_steps_sampled: 2192000\n",
      "    num_steps_trained: 2192000\n",
      "    sample_time_ms: 2453.8\n",
      "    update_time_ms: 4.293\n",
      "  iterations_since_restore: 1096\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.76666666666667\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.093357561061946\n",
      "    mean_inference_ms: 1.4040358575394556\n",
      "    mean_processing_ms: 0.9266779985539075\n",
      "  time_since_restore: 3779.422665119171\n",
      "  time_this_iter_s: 2.0937423706054688\n",
      "  time_total_s: 3779.422665119171\n",
      "  timestamp: 1595952429\n",
      "  timesteps_since_restore: 2192000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2192000\n",
      "  training_iteration: 1096\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3779 s, 1096 iter, 2192000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.380234323791495\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1095\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.743\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.259651243686676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0010322628077119589\n",
      "        policy_loss: -0.001060405746102333\n",
      "        total_loss: -0.0002760553325060755\n",
      "        vf_explained_var: 0.5175272226333618\n",
      "        vf_loss: 0.0007843493949621916\n",
      "    load_time_ms: 1.313\n",
      "    num_steps_sampled: 2198000\n",
      "    num_steps_trained: 2198000\n",
      "    sample_time_ms: 2454.105\n",
      "    update_time_ms: 4.186\n",
      "  iterations_since_restore: 1099\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.725\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.093357561061946\n",
      "    mean_inference_ms: 1.4040358575394556\n",
      "    mean_processing_ms: 0.9266779985539075\n",
      "  time_since_restore: 3786.122864961624\n",
      "  time_this_iter_s: 2.3319547176361084\n",
      "  time_total_s: 3786.122864961624\n",
      "  timestamp: 1595952436\n",
      "  timesteps_since_restore: 2198000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2198000\n",
      "  training_iteration: 1099\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3786 s, 1099 iter, 2198000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.378002865848321\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.96\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.18114589154720306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.000776526634581387\n",
      "        policy_loss: 0.00034881592728197575\n",
      "        total_loss: 0.047944992780685425\n",
      "        vf_explained_var: 0.6379106044769287\n",
      "        vf_loss: 0.04759619012475014\n",
      "    load_time_ms: 1.067\n",
      "    num_steps_sampled: 2202000\n",
      "    num_steps_trained: 2202000\n",
      "    sample_time_ms: 2417.252\n",
      "    update_time_ms: 4.124\n",
      "  iterations_since_restore: 1101\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.20000000000001\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.084479918214722\n",
      "    mean_inference_ms: 1.4020463137976367\n",
      "    mean_processing_ms: 0.9261084600332804\n",
      "  time_since_restore: 3791.8552780151367\n",
      "  time_this_iter_s: 2.1370437145233154\n",
      "  time_total_s: 3791.8552780151367\n",
      "  timestamp: 1595952441\n",
      "  timesteps_since_restore: 2202000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2202000\n",
      "  training_iteration: 1101\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3791 s, 1101 iter, 2202000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.378002865848321\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.72\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.22103171050548553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00021198215836193413\n",
      "        policy_loss: -0.00025480843032710254\n",
      "        total_loss: 0.0007609081221744418\n",
      "        vf_explained_var: 0.4440661072731018\n",
      "        vf_loss: 0.0010157182114198804\n",
      "    load_time_ms: 1.126\n",
      "    num_steps_sampled: 2208000\n",
      "    num_steps_trained: 2208000\n",
      "    sample_time_ms: 2404.2\n",
      "    update_time_ms: 4.224\n",
      "  iterations_since_restore: 1104\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.66666666666667\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.084479918214722\n",
      "    mean_inference_ms: 1.4020463137976367\n",
      "    mean_processing_ms: 0.9261084600332804\n",
      "  time_since_restore: 3798.3954854011536\n",
      "  time_this_iter_s: 2.162501335144043\n",
      "  time_total_s: 3798.3954854011536\n",
      "  timestamp: 1595952448\n",
      "  timesteps_since_restore: 2208000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2208000\n",
      "  training_iteration: 1104\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3798 s, 1104 iter, 2208000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.386329904707704\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1105\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.323\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.19474530220031738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0008122011204250157\n",
      "        policy_loss: 0.0001784207852324471\n",
      "        total_loss: 0.048686180263757706\n",
      "        vf_explained_var: 0.6141000390052795\n",
      "        vf_loss: 0.04850775748491287\n",
      "    load_time_ms: 1.126\n",
      "    num_steps_sampled: 2212000\n",
      "    num_steps_trained: 2212000\n",
      "    sample_time_ms: 2399.375\n",
      "    update_time_ms: 4.205\n",
      "  iterations_since_restore: 1106\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.800000000000004\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.075373455147423\n",
      "    mean_inference_ms: 1.4000102498860074\n",
      "    mean_processing_ms: 0.9255257087974939\n",
      "  time_since_restore: 3803.768673181534\n",
      "  time_this_iter_s: 2.1133365631103516\n",
      "  time_total_s: 3803.768673181534\n",
      "  timestamp: 1595952453\n",
      "  timesteps_since_restore: 2212000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2212000\n",
      "  training_iteration: 1106\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3803 s, 1106 iter, 2212000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.386329904707704\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1105\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.403\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2038707137107849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007470360142178833\n",
      "        policy_loss: -0.00201204395852983\n",
      "        total_loss: -0.0009610500419512391\n",
      "        vf_explained_var: 0.7674552202224731\n",
      "        vf_loss: 0.0010509984567761421\n",
      "    load_time_ms: 1.132\n",
      "    num_steps_sampled: 2218000\n",
      "    num_steps_trained: 2218000\n",
      "    sample_time_ms: 2371.474\n",
      "    update_time_ms: 4.269\n",
      "  iterations_since_restore: 1109\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.075373455147423\n",
      "    mean_inference_ms: 1.4000102498860074\n",
      "    mean_processing_ms: 0.9255257087974939\n",
      "  time_since_restore: 3810.192195415497\n",
      "  time_this_iter_s: 2.1559112071990967\n",
      "  time_total_s: 3810.192195415497\n",
      "  timestamp: 1595952460\n",
      "  timesteps_since_restore: 2218000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2218000\n",
      "  training_iteration: 1109\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3810 s, 1109 iter, 2218000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.412251314175574\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.21315044164657593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00864934828132391\n",
      "        policy_loss: -0.0021397171076387167\n",
      "        total_loss: 0.046838730573654175\n",
      "        vf_explained_var: 0.6098895072937012\n",
      "        vf_loss: 0.04897845536470413\n",
      "    load_time_ms: 1.158\n",
      "    num_steps_sampled: 2222000\n",
      "    num_steps_trained: 2222000\n",
      "    sample_time_ms: 2347.387\n",
      "    update_time_ms: 4.295\n",
      "  iterations_since_restore: 1111\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.833333333333336\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.066368232790289\n",
      "    mean_inference_ms: 1.3979969233044174\n",
      "    mean_processing_ms: 0.9249520473990903\n",
      "  time_since_restore: 3815.6830809116364\n",
      "  time_this_iter_s: 2.1142401695251465\n",
      "  time_total_s: 3815.6830809116364\n",
      "  timestamp: 1595952465\n",
      "  timesteps_since_restore: 2222000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2222000\n",
      "  training_iteration: 1111\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3815 s, 1111 iter, 2222000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.412251314175574\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.382\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.20195020735263824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.045124999014661e-05\n",
      "        policy_loss: -0.00033683489891700447\n",
      "        total_loss: 0.000604450237005949\n",
      "        vf_explained_var: 0.6362971663475037\n",
      "        vf_loss: 0.0009412856888957322\n",
      "    load_time_ms: 1.134\n",
      "    num_steps_sampled: 2228000\n",
      "    num_steps_trained: 2228000\n",
      "    sample_time_ms: 2330.018\n",
      "    update_time_ms: 4.305\n",
      "  iterations_since_restore: 1114\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63333333333334\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.066368232790289\n",
      "    mean_inference_ms: 1.3979969233044174\n",
      "    mean_processing_ms: 0.9249520473990903\n",
      "  time_since_restore: 3822.0513305664062\n",
      "  time_this_iter_s: 2.141167402267456\n",
      "  time_total_s: 3822.0513305664062\n",
      "  timestamp: 1595952472\n",
      "  timesteps_since_restore: 2228000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2228000\n",
      "  training_iteration: 1114\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3822 s, 1114 iter, 2228000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-07-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.433384170737147\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1115\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.81\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2188989520072937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0031612415332347155\n",
      "        policy_loss: -0.0009703273535706103\n",
      "        total_loss: 0.046451810747385025\n",
      "        vf_explained_var: 0.6456739902496338\n",
      "        vf_loss: 0.047422125935554504\n",
      "    load_time_ms: 1.111\n",
      "    num_steps_sampled: 2232000\n",
      "    num_steps_trained: 2232000\n",
      "    sample_time_ms: 2335.976\n",
      "    update_time_ms: 4.313\n",
      "  iterations_since_restore: 1116\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.6\n",
      "    ram_util_percent: 64.43333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.057451404283325\n",
      "    mean_inference_ms: 1.3960043765727952\n",
      "    mean_processing_ms: 0.9243827405795189\n",
      "  time_since_restore: 3827.487339258194\n",
      "  time_this_iter_s: 2.1502230167388916\n",
      "  time_total_s: 3827.487339258194\n",
      "  timestamp: 1595952477\n",
      "  timesteps_since_restore: 2232000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2232000\n",
      "  training_iteration: 1116\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3827 s, 1116 iter, 2232000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.433384170737147\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1115\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.362\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.19622857868671417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002297686878591776\n",
      "        policy_loss: -0.0002838611544575542\n",
      "        total_loss: 0.0006092033581808209\n",
      "        vf_explained_var: 0.735134482383728\n",
      "        vf_loss: 0.000893060932867229\n",
      "    load_time_ms: 1.152\n",
      "    num_steps_sampled: 2238000\n",
      "    num_steps_trained: 2238000\n",
      "    sample_time_ms: 2340.488\n",
      "    update_time_ms: 4.335\n",
      "  iterations_since_restore: 1119\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.89999999999999\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.057451404283325\n",
      "    mean_inference_ms: 1.3960043765727952\n",
      "    mean_processing_ms: 0.9243827405795189\n",
      "  time_since_restore: 3833.9610683918\n",
      "  time_this_iter_s: 2.1373608112335205\n",
      "  time_total_s: 3833.9610683918\n",
      "  timestamp: 1595952484\n",
      "  timesteps_since_restore: 2238000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2238000\n",
      "  training_iteration: 1119\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3833 s, 1119 iter, 2238000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.437152442883633\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.338\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.22976034879684448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0032134633511304855\n",
      "        policy_loss: -0.0014324454823508859\n",
      "        total_loss: 0.046964310109615326\n",
      "        vf_explained_var: 0.6326171159744263\n",
      "        vf_loss: 0.0483967661857605\n",
      "    load_time_ms: 1.114\n",
      "    num_steps_sampled: 2242000\n",
      "    num_steps_trained: 2242000\n",
      "    sample_time_ms: 2330.864\n",
      "    update_time_ms: 4.278\n",
      "  iterations_since_restore: 1121\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.333333333333336\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.048631703536569\n",
      "    mean_inference_ms: 1.3940356759564256\n",
      "    mean_processing_ms: 0.9238184798725346\n",
      "  time_since_restore: 3839.3541922569275\n",
      "  time_this_iter_s: 2.114473581314087\n",
      "  time_total_s: 3839.3541922569275\n",
      "  timestamp: 1595952489\n",
      "  timesteps_since_restore: 2242000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2242000\n",
      "  training_iteration: 1121\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3839 s, 1121 iter, 2242000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.437152442883633\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.774\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.20160479843616486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004649074107874185\n",
      "        policy_loss: 0.0005066166049800813\n",
      "        total_loss: 0.001359191839583218\n",
      "        vf_explained_var: 0.8685117363929749\n",
      "        vf_loss: 0.0008525826851837337\n",
      "    load_time_ms: 1.078\n",
      "    num_steps_sampled: 2248000\n",
      "    num_steps_trained: 2248000\n",
      "    sample_time_ms: 2348.355\n",
      "    update_time_ms: 4.071\n",
      "  iterations_since_restore: 1124\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.048631703536569\n",
      "    mean_inference_ms: 1.3940356759564256\n",
      "    mean_processing_ms: 0.9238184798725346\n",
      "  time_since_restore: 3845.899348974228\n",
      "  time_this_iter_s: 2.125999689102173\n",
      "  time_total_s: 3845.899348974228\n",
      "  timestamp: 1595952496\n",
      "  timesteps_since_restore: 2248000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2248000\n",
      "  training_iteration: 1124\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3845 s, 1124 iter, 2248000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.436251793424612\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1125\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.646\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.20746883749961853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.755013550398871e-05\n",
      "        policy_loss: -0.00011282158084213734\n",
      "        total_loss: 0.04469393566250801\n",
      "        vf_explained_var: 0.681441068649292\n",
      "        vf_loss: 0.04480674862861633\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 2252000\n",
      "    num_steps_trained: 2252000\n",
      "    sample_time_ms: 2340.433\n",
      "    update_time_ms: 4.216\n",
      "  iterations_since_restore: 1126\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.9\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0398991056157145\n",
      "    mean_inference_ms: 1.3920876552327095\n",
      "    mean_processing_ms: 0.9232622473287241\n",
      "  time_since_restore: 3851.2579667568207\n",
      "  time_this_iter_s: 2.089116096496582\n",
      "  time_total_s: 3851.2579667568207\n",
      "  timestamp: 1595952501\n",
      "  timesteps_since_restore: 2252000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2252000\n",
      "  training_iteration: 1126\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3851 s, 1126 iter, 2252000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 13.146919375391235\n",
      "  episode_reward_mean: 12.436251793424612\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1125\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.031\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.22668908536434174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8880953323096037e-05\n",
      "        policy_loss: -0.00025423814076930285\n",
      "        total_loss: 0.0005004358245059848\n",
      "        vf_explained_var: 0.6253393292427063\n",
      "        vf_loss: 0.0007546698907390237\n",
      "    load_time_ms: 1.077\n",
      "    num_steps_sampled: 2258000\n",
      "    num_steps_trained: 2258000\n",
      "    sample_time_ms: 2355.002\n",
      "    update_time_ms: 4.046\n",
      "  iterations_since_restore: 1129\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.93333333333334\n",
      "    ram_util_percent: 64.63333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0398991056157145\n",
      "    mean_inference_ms: 1.3920876552327095\n",
      "    mean_processing_ms: 0.9232622473287241\n",
      "  time_since_restore: 3857.868639230728\n",
      "  time_this_iter_s: 2.2169880867004395\n",
      "  time_total_s: 3857.868639230728\n",
      "  timestamp: 1595952508\n",
      "  timesteps_since_restore: 2258000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2258000\n",
      "  training_iteration: 1129\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3857 s, 1129 iter, 2258000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.391875606104616\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.949\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2106596678495407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0016283313743770123\n",
      "        policy_loss: 2.0394325474626385e-05\n",
      "        total_loss: 0.0456920862197876\n",
      "        vf_explained_var: 0.672119140625\n",
      "        vf_loss: 0.045671675354242325\n",
      "    load_time_ms: 1.079\n",
      "    num_steps_sampled: 2262000\n",
      "    num_steps_trained: 2262000\n",
      "    sample_time_ms: 2354.422\n",
      "    update_time_ms: 4.03\n",
      "  iterations_since_restore: 1131\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.133333333333326\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0312562141644195\n",
      "    mean_inference_ms: 1.3901608345047944\n",
      "    mean_processing_ms: 0.9227089376038896\n",
      "  time_since_restore: 3863.255065202713\n",
      "  time_this_iter_s: 2.0963313579559326\n",
      "  time_total_s: 3863.255065202713\n",
      "  timestamp: 1595952513\n",
      "  timesteps_since_restore: 2262000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2262000\n",
      "  training_iteration: 1131\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3863 s, 1131 iter, 2262000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.391875606104616\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.22579941153526306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0007365649216808379\n",
      "        policy_loss: -0.0017729087267071009\n",
      "        total_loss: -0.0009373717475682497\n",
      "        vf_explained_var: 0.6891589164733887\n",
      "        vf_loss: 0.0008355215541087091\n",
      "    load_time_ms: 1.129\n",
      "    num_steps_sampled: 2268000\n",
      "    num_steps_trained: 2268000\n",
      "    sample_time_ms: 2338.826\n",
      "    update_time_ms: 4.194\n",
      "  iterations_since_restore: 1134\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63333333333334\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.0312562141644195\n",
      "    mean_inference_ms: 1.3901608345047944\n",
      "    mean_processing_ms: 0.9227089376038896\n",
      "  time_since_restore: 3869.6502480506897\n",
      "  time_this_iter_s: 2.1173441410064697\n",
      "  time_total_s: 3869.6502480506897\n",
      "  timestamp: 1595952519\n",
      "  timesteps_since_restore: 2268000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2268000\n",
      "  training_iteration: 1134\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3869 s, 1134 iter, 2268000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.40260657313588\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1135\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.15\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.19585870206356049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006794608198106289\n",
      "        policy_loss: -0.0008539280970580876\n",
      "        total_loss: 0.045711059123277664\n",
      "        vf_explained_var: 0.6576879024505615\n",
      "        vf_loss: 0.04656499624252319\n",
      "    load_time_ms: 1.091\n",
      "    num_steps_sampled: 2272000\n",
      "    num_steps_trained: 2272000\n",
      "    sample_time_ms: 2353.358\n",
      "    update_time_ms: 4.118\n",
      "  iterations_since_restore: 1136\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26666666666667\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.022707558636042\n",
      "    mean_inference_ms: 1.388255518655005\n",
      "    mean_processing_ms: 0.9221635893226516\n",
      "  time_since_restore: 3875.1535127162933\n",
      "  time_this_iter_s: 2.0935633182525635\n",
      "  time_total_s: 3875.1535127162933\n",
      "  timestamp: 1595952525\n",
      "  timesteps_since_restore: 2272000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2272000\n",
      "  training_iteration: 1136\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3875 s, 1136 iter, 2272000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.40260657313588\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1135\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.382\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.221858948469162\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6157329810084775e-05\n",
      "        policy_loss: 4.855298902839422e-05\n",
      "        total_loss: 0.0009047017083503306\n",
      "        vf_explained_var: 0.7779711484909058\n",
      "        vf_loss: 0.0008561446447856724\n",
      "    load_time_ms: 1.114\n",
      "    num_steps_sampled: 2278000\n",
      "    num_steps_trained: 2278000\n",
      "    sample_time_ms: 2329.12\n",
      "    update_time_ms: 4.173\n",
      "  iterations_since_restore: 1139\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10000000000001\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.022707558636042\n",
      "    mean_inference_ms: 1.388255518655005\n",
      "    mean_processing_ms: 0.9221635893226516\n",
      "  time_since_restore: 3881.5228674411774\n",
      "  time_this_iter_s: 2.1132781505584717\n",
      "  time_total_s: 3881.5228674411774\n",
      "  timestamp: 1595952531\n",
      "  timesteps_since_restore: 2278000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2278000\n",
      "  training_iteration: 1139\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3881 s, 1139 iter, 2278000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-08-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.405457828245723\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.36\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.18895187973976135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00019789495854638517\n",
      "        policy_loss: -0.00011254596756771207\n",
      "        total_loss: 0.0446099117398262\n",
      "        vf_explained_var: 0.683638334274292\n",
      "        vf_loss: 0.04472246393561363\n",
      "    load_time_ms: 1.11\n",
      "    num_steps_sampled: 2282000\n",
      "    num_steps_trained: 2282000\n",
      "    sample_time_ms: 2336.499\n",
      "    update_time_ms: 4.129\n",
      "  iterations_since_restore: 1141\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53333333333334\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.014246126221394\n",
      "    mean_inference_ms: 1.386370176971424\n",
      "    mean_processing_ms: 0.9216236175176802\n",
      "  time_since_restore: 3886.9819524288177\n",
      "  time_this_iter_s: 2.197767734527588\n",
      "  time_total_s: 3886.9819524288177\n",
      "  timestamp: 1595952537\n",
      "  timesteps_since_restore: 2282000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2282000\n",
      "  training_iteration: 1141\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3886 s, 1141 iter, 2282000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.405457828245723\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.86\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2420586347579956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005943040596321225\n",
      "        policy_loss: -0.0014786854153499007\n",
      "        total_loss: -0.0006948185036890209\n",
      "        vf_explained_var: 0.7409272789955139\n",
      "        vf_loss: 0.0007838792516849935\n",
      "    load_time_ms: 1.091\n",
      "    num_steps_sampled: 2288000\n",
      "    num_steps_trained: 2288000\n",
      "    sample_time_ms: 2340.234\n",
      "    update_time_ms: 3.978\n",
      "  iterations_since_restore: 1144\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60000000000001\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.014246126221394\n",
      "    mean_inference_ms: 1.386370176971424\n",
      "    mean_processing_ms: 0.9216236175176802\n",
      "  time_since_restore: 3893.405515432358\n",
      "  time_this_iter_s: 2.147860050201416\n",
      "  time_total_s: 3893.405515432358\n",
      "  timestamp: 1595952543\n",
      "  timesteps_since_restore: 2288000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2288000\n",
      "  training_iteration: 1144\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3893 s, 1144 iter, 2288000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.411255627600353\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1145\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.705\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.19465221464633942\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003845703788101673\n",
      "        policy_loss: 8.605861512478441e-05\n",
      "        total_loss: 0.044919438660144806\n",
      "        vf_explained_var: 0.684877872467041\n",
      "        vf_loss: 0.04483339190483093\n",
      "    load_time_ms: 1.129\n",
      "    num_steps_sampled: 2292000\n",
      "    num_steps_trained: 2292000\n",
      "    sample_time_ms: 2332.33\n",
      "    update_time_ms: 3.978\n",
      "  iterations_since_restore: 1146\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.199999999999996\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.005883149388371\n",
      "    mean_inference_ms: 1.3845083240411007\n",
      "    mean_processing_ms: 0.9210885298289088\n",
      "  time_since_restore: 3898.827629327774\n",
      "  time_this_iter_s: 2.095440149307251\n",
      "  time_total_s: 3898.827629327774\n",
      "  timestamp: 1595952549\n",
      "  timesteps_since_restore: 2292000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2292000\n",
      "  training_iteration: 1146\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3898 s, 1146 iter, 2292000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.411255627600353\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1145\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.635\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.24379417300224304\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0011976358946412802\n",
      "        policy_loss: -0.002003161469474435\n",
      "        total_loss: -0.0012650828575715423\n",
      "        vf_explained_var: 0.3556790351867676\n",
      "        vf_loss: 0.0007380822207778692\n",
      "    load_time_ms: 1.093\n",
      "    num_steps_sampled: 2298000\n",
      "    num_steps_trained: 2298000\n",
      "    sample_time_ms: 2346.525\n",
      "    update_time_ms: 3.933\n",
      "  iterations_since_restore: 1149\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43333333333334\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 6.005883149388371\n",
      "    mean_inference_ms: 1.3845083240411007\n",
      "    mean_processing_ms: 0.9210885298289088\n",
      "  time_since_restore: 3905.3377780914307\n",
      "  time_this_iter_s: 2.1226966381073\n",
      "  time_total_s: 3905.3377780914307\n",
      "  timestamp: 1595952555\n",
      "  timesteps_since_restore: 2298000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2298000\n",
      "  training_iteration: 1149\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3905 s, 1149 iter, 2298000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.413482962599915\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.59\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.20482958853244781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0022709050681442022\n",
      "        policy_loss: -0.0029943713452667\n",
      "        total_loss: 0.04255976900458336\n",
      "        vf_explained_var: 0.6760824918746948\n",
      "        vf_loss: 0.04555415362119675\n",
      "    load_time_ms: 1.08\n",
      "    num_steps_sampled: 2302000\n",
      "    num_steps_trained: 2302000\n",
      "    sample_time_ms: 2337.057\n",
      "    update_time_ms: 3.982\n",
      "  iterations_since_restore: 1151\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.06666666666666\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.997594524192001\n",
      "    mean_inference_ms: 1.3826640911183574\n",
      "    mean_processing_ms: 0.9205598420945599\n",
      "  time_since_restore: 3910.7025015354156\n",
      "  time_this_iter_s: 2.102842330932617\n",
      "  time_total_s: 3910.7025015354156\n",
      "  timestamp: 1595952560\n",
      "  timesteps_since_restore: 2302000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2302000\n",
      "  training_iteration: 1151\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3910 s, 1151 iter, 2302000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.413482962599915\n",
      "  episode_reward_min: 11.937700817122522\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.184\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.20962820947170258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00019491421699058264\n",
      "        policy_loss: -0.0004481411015149206\n",
      "        total_loss: 0.000337779987603426\n",
      "        vf_explained_var: 0.7414171695709229\n",
      "        vf_loss: 0.0007859198958612978\n",
      "    load_time_ms: 1.079\n",
      "    num_steps_sampled: 2308000\n",
      "    num_steps_trained: 2308000\n",
      "    sample_time_ms: 2397.498\n",
      "    update_time_ms: 3.965\n",
      "  iterations_since_restore: 1154\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.13333333333333\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.997594524192001\n",
      "    mean_inference_ms: 1.3826640911183574\n",
      "    mean_processing_ms: 0.9205598420945599\n",
      "  time_since_restore: 3917.7255187034607\n",
      "  time_this_iter_s: 2.101961612701416\n",
      "  time_total_s: 3917.7255187034607\n",
      "  timestamp: 1595952568\n",
      "  timesteps_since_restore: 2308000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2308000\n",
      "  training_iteration: 1154\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3917 s, 1154 iter, 2308000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.433017078412435\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1155\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.22\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.22352254390716553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006797571666538715\n",
      "        policy_loss: -0.0036885614972561598\n",
      "        total_loss: 0.041178055107593536\n",
      "        vf_explained_var: 0.6873689889907837\n",
      "        vf_loss: 0.04486660659313202\n",
      "    load_time_ms: 1.061\n",
      "    num_steps_sampled: 2312000\n",
      "    num_steps_trained: 2312000\n",
      "    sample_time_ms: 2391.397\n",
      "    update_time_ms: 3.92\n",
      "  iterations_since_restore: 1156\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53333333333334\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.989303602637225\n",
      "    mean_inference_ms: 1.3808214667973389\n",
      "    mean_processing_ms: 0.9200235913604495\n",
      "  time_since_restore: 3923.0868351459503\n",
      "  time_this_iter_s: 2.1065030097961426\n",
      "  time_total_s: 3923.0868351459503\n",
      "  timestamp: 1595952573\n",
      "  timesteps_since_restore: 2312000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2312000\n",
      "  training_iteration: 1156\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3923 s, 1156 iter, 2312000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.433017078412435\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1155\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 21.677\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2168918401002884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0013841402251273394\n",
      "        policy_loss: -0.001679389039054513\n",
      "        total_loss: -0.0009492039680480957\n",
      "        vf_explained_var: 0.740170419216156\n",
      "        vf_loss: 0.0007301806472241879\n",
      "    load_time_ms: 1.056\n",
      "    num_steps_sampled: 2318000\n",
      "    num_steps_trained: 2318000\n",
      "    sample_time_ms: 2372.664\n",
      "    update_time_ms: 3.967\n",
      "  iterations_since_restore: 1159\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.8\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.989303602637225\n",
      "    mean_inference_ms: 1.3808214667973389\n",
      "    mean_processing_ms: 0.9200235913604495\n",
      "  time_since_restore: 3929.4056627750397\n",
      "  time_this_iter_s: 2.1012368202209473\n",
      "  time_total_s: 3929.4056627750397\n",
      "  timestamp: 1595952579\n",
      "  timesteps_since_restore: 2318000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2318000\n",
      "  training_iteration: 1159\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3929 s, 1159 iter, 2318000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.429356469571548\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.074\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.23311619460582733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0036134852562099695\n",
      "        policy_loss: 0.000280391686828807\n",
      "        total_loss: 0.04741156846284866\n",
      "        vf_explained_var: 0.6616232395172119\n",
      "        vf_loss: 0.047131165862083435\n",
      "    load_time_ms: 1.061\n",
      "    num_steps_sampled: 2322000\n",
      "    num_steps_trained: 2322000\n",
      "    sample_time_ms: 2378.357\n",
      "    update_time_ms: 4.289\n",
      "  iterations_since_restore: 1161\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56666666666668\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.981039624992402\n",
      "    mean_inference_ms: 1.3789822816380763\n",
      "    mean_processing_ms: 0.9194800846330812\n",
      "  time_since_restore: 3934.835064649582\n",
      "  time_this_iter_s: 2.1153900623321533\n",
      "  time_total_s: 3934.835064649582\n",
      "  timestamp: 1595952585\n",
      "  timesteps_since_restore: 2322000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2322000\n",
      "  training_iteration: 1161\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3934 s, 1161 iter, 2322000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.429356469571548\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.207\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.17554892599582672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.262481186538935e-05\n",
      "        policy_loss: -9.542464977130294e-05\n",
      "        total_loss: 0.0008706283406354487\n",
      "        vf_explained_var: 0.5956748127937317\n",
      "        vf_loss: 0.0009660400100983679\n",
      "    load_time_ms: 1.068\n",
      "    num_steps_sampled: 2328000\n",
      "    num_steps_trained: 2328000\n",
      "    sample_time_ms: 2356.604\n",
      "    update_time_ms: 4.366\n",
      "  iterations_since_restore: 1164\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.981039624992402\n",
      "    mean_inference_ms: 1.3789822816380763\n",
      "    mean_processing_ms: 0.9194800846330812\n",
      "  time_since_restore: 3941.644431591034\n",
      "  time_this_iter_s: 2.1272406578063965\n",
      "  time_total_s: 3941.644431591034\n",
      "  timestamp: 1595952592\n",
      "  timesteps_since_restore: 2328000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2328000\n",
      "  training_iteration: 1164\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3941 s, 1164 iter, 2328000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.436925272321405\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1165\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2321406900882721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0067092040553689\n",
      "        policy_loss: -0.0021184373181313276\n",
      "        total_loss: 0.04424776881933212\n",
      "        vf_explained_var: 0.6672747135162354\n",
      "        vf_loss: 0.04636620730161667\n",
      "    load_time_ms: 1.078\n",
      "    num_steps_sampled: 2332000\n",
      "    num_steps_trained: 2332000\n",
      "    sample_time_ms: 2362.243\n",
      "    update_time_ms: 4.465\n",
      "  iterations_since_restore: 1166\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.4\n",
      "    ram_util_percent: 64.33333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.972783128776667\n",
      "    mean_inference_ms: 1.377150148852872\n",
      "    mean_processing_ms: 0.9189350995389544\n",
      "  time_since_restore: 3947.064297437668\n",
      "  time_this_iter_s: 2.149078845977783\n",
      "  time_total_s: 3947.064297437668\n",
      "  timestamp: 1595952597\n",
      "  timesteps_since_restore: 2332000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2332000\n",
      "  training_iteration: 1166\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3947 s, 1166 iter, 2332000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.436925272321405\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1165\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.844\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.19146692752838135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.628620758419856e-05\n",
      "        policy_loss: 0.00019506931130308658\n",
      "        total_loss: 0.001047181081958115\n",
      "        vf_explained_var: 0.6883940696716309\n",
      "        vf_loss: 0.0008521158597432077\n",
      "    load_time_ms: 1.126\n",
      "    num_steps_sampled: 2338000\n",
      "    num_steps_trained: 2338000\n",
      "    sample_time_ms: 2371.955\n",
      "    update_time_ms: 4.401\n",
      "  iterations_since_restore: 1169\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.89999999999999\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.972783128776667\n",
      "    mean_inference_ms: 1.377150148852872\n",
      "    mean_processing_ms: 0.9189350995389544\n",
      "  time_since_restore: 3953.486446619034\n",
      "  time_this_iter_s: 2.1576383113861084\n",
      "  time_total_s: 3953.486446619034\n",
      "  timestamp: 1595952603\n",
      "  timesteps_since_restore: 2338000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2338000\n",
      "  training_iteration: 1169\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3953 s, 1169 iter, 2338000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.432058471253347\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.79\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.22506994009017944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0070295678451657295\n",
      "        policy_loss: -0.0023988953325897455\n",
      "        total_loss: 0.042678143829107285\n",
      "        vf_explained_var: 0.6882553696632385\n",
      "        vf_loss: 0.04507704824209213\n",
      "    load_time_ms: 1.123\n",
      "    num_steps_sampled: 2342000\n",
      "    num_steps_trained: 2342000\n",
      "    sample_time_ms: 2367.28\n",
      "    update_time_ms: 4.138\n",
      "  iterations_since_restore: 1171\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.5\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.964589307317858\n",
      "    mean_inference_ms: 1.3753306560503287\n",
      "    mean_processing_ms: 0.9183930097585176\n",
      "  time_since_restore: 3958.864953517914\n",
      "  time_this_iter_s: 2.1028008460998535\n",
      "  time_total_s: 3958.864953517914\n",
      "  timestamp: 1595952609\n",
      "  timesteps_since_restore: 2342000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2342000\n",
      "  training_iteration: 1171\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3958 s, 1171 iter, 2342000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.432058471253347\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.788\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.21579395234584808\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00027980023878626525\n",
      "        policy_loss: -0.0008929882314987481\n",
      "        total_loss: -0.00014412069867830724\n",
      "        vf_explained_var: 0.7007656097412109\n",
      "        vf_loss: 0.0007488891133107245\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 2348000\n",
      "    num_steps_trained: 2348000\n",
      "    sample_time_ms: 2329.579\n",
      "    update_time_ms: 4.068\n",
      "  iterations_since_restore: 1174\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.66666666666667\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.964589307317858\n",
      "    mean_inference_ms: 1.3753306560503287\n",
      "    mean_processing_ms: 0.9183930097585176\n",
      "  time_since_restore: 3965.2947211265564\n",
      "  time_this_iter_s: 2.1587305068969727\n",
      "  time_total_s: 3965.2947211265564\n",
      "  timestamp: 1595952615\n",
      "  timesteps_since_restore: 2348000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2348000\n",
      "  training_iteration: 1174\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3965 s, 1174 iter, 2348000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.41705615453375\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1175\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.864\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.23196032643318176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0016787416534498334\n",
      "        policy_loss: -0.0009598293108865619\n",
      "        total_loss: 0.04481634125113487\n",
      "        vf_explained_var: 0.6753460764884949\n",
      "        vf_loss: 0.04577616974711418\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 2352000\n",
      "    num_steps_trained: 2352000\n",
      "    sample_time_ms: 2325.407\n",
      "    update_time_ms: 3.981\n",
      "  iterations_since_restore: 1176\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.599999999999994\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.956385713571337\n",
      "    mean_inference_ms: 1.3735072676473379\n",
      "    mean_processing_ms: 0.9178529004638257\n",
      "  time_since_restore: 3970.6737217903137\n",
      "  time_this_iter_s: 2.111999273300171\n",
      "  time_total_s: 3970.6737217903137\n",
      "  timestamp: 1595952621\n",
      "  timesteps_since_restore: 2352000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2352000\n",
      "  training_iteration: 1176\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3970 s, 1176 iter, 2352000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.41705615453375\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1175\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.975\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.19593973457813263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9847184375976212e-05\n",
      "        policy_loss: -6.18667618255131e-05\n",
      "        total_loss: 0.0008248634403571486\n",
      "        vf_explained_var: 0.5660617351531982\n",
      "        vf_loss: 0.0008867247379384935\n",
      "    load_time_ms: 1.093\n",
      "    num_steps_sampled: 2358000\n",
      "    num_steps_trained: 2358000\n",
      "    sample_time_ms: 2326.757\n",
      "    update_time_ms: 4.248\n",
      "  iterations_since_restore: 1179\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.43333333333334\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.956385713571337\n",
      "    mean_inference_ms: 1.3735072676473379\n",
      "    mean_processing_ms: 0.9178529004638257\n",
      "  time_since_restore: 3977.1138615608215\n",
      "  time_this_iter_s: 2.182846784591675\n",
      "  time_total_s: 3977.1138615608215\n",
      "  timestamp: 1595952627\n",
      "  timesteps_since_restore: 2358000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2358000\n",
      "  training_iteration: 1179\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3977 s, 1179 iter, 2358000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.42777051656411\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.218\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.23149365186691284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0027651956770569086\n",
      "        policy_loss: -0.0002355728211114183\n",
      "        total_loss: 0.0455392524600029\n",
      "        vf_explained_var: 0.6792237758636475\n",
      "        vf_loss: 0.04577481746673584\n",
      "    load_time_ms: 1.113\n",
      "    num_steps_sampled: 2362000\n",
      "    num_steps_trained: 2362000\n",
      "    sample_time_ms: 2325.963\n",
      "    update_time_ms: 4.169\n",
      "  iterations_since_restore: 1181\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.39999999999999\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.948144115321099\n",
      "    mean_inference_ms: 1.371670288390206\n",
      "    mean_processing_ms: 0.9173085593358457\n",
      "  time_since_restore: 3982.4874765872955\n",
      "  time_this_iter_s: 2.117863178253174\n",
      "  time_total_s: 3982.4874765872955\n",
      "  timestamp: 1595952633\n",
      "  timesteps_since_restore: 2362000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2362000\n",
      "  training_iteration: 1181\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3982 s, 1181 iter, 2362000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.42777051656411\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.19950540363788605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00038791823317296803\n",
      "        policy_loss: -0.000666761421598494\n",
      "        total_loss: 0.00016998767387121916\n",
      "        vf_explained_var: 0.7294909358024597\n",
      "        vf_loss: 0.0008367541013285518\n",
      "    load_time_ms: 1.105\n",
      "    num_steps_sampled: 2368000\n",
      "    num_steps_trained: 2368000\n",
      "    sample_time_ms: 2314.928\n",
      "    update_time_ms: 4.456\n",
      "  iterations_since_restore: 1184\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.73333333333333\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.948144115321099\n",
      "    mean_inference_ms: 1.371670288390206\n",
      "    mean_processing_ms: 0.9173085593358457\n",
      "  time_since_restore: 3988.8112733364105\n",
      "  time_this_iter_s: 2.104787826538086\n",
      "  time_total_s: 3988.8112733364105\n",
      "  timestamp: 1595952639\n",
      "  timesteps_since_restore: 2368000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2368000\n",
      "  training_iteration: 1184\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3988 s, 1184 iter, 2368000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.438696675353551\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1185\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.277\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.24014264345169067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006848648190498352\n",
      "        policy_loss: -0.00103457598015666\n",
      "        total_loss: 0.04513999819755554\n",
      "        vf_explained_var: 0.6779944896697998\n",
      "        vf_loss: 0.046174582093954086\n",
      "    load_time_ms: 1.118\n",
      "    num_steps_sampled: 2372000\n",
      "    num_steps_trained: 2372000\n",
      "    sample_time_ms: 2332.702\n",
      "    update_time_ms: 4.413\n",
      "  iterations_since_restore: 1186\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.36666666666666\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.939881212913035\n",
      "    mean_inference_ms: 1.3698226123988297\n",
      "    mean_processing_ms: 0.9167626455356391\n",
      "  time_since_restore: 3994.3679037094116\n",
      "  time_this_iter_s: 2.116320848464966\n",
      "  time_total_s: 3994.3679037094116\n",
      "  timestamp: 1595952644\n",
      "  timesteps_since_restore: 2372000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2372000\n",
      "  training_iteration: 1186\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 3994 s, 1186 iter, 2372000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.438696675353551\n",
      "  episode_reward_min: 12.016594478977373\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1185\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.121\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.18160445988178253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005784950335510075\n",
      "        policy_loss: 0.000504752155393362\n",
      "        total_loss: 0.001375900232233107\n",
      "        vf_explained_var: 0.6889755725860596\n",
      "        vf_loss: 0.0008711399859748781\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 2378000\n",
      "    num_steps_trained: 2378000\n",
      "    sample_time_ms: 2330.414\n",
      "    update_time_ms: 4.256\n",
      "  iterations_since_restore: 1189\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.39999999999999\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.939881212913035\n",
      "    mean_inference_ms: 1.3698226123988297\n",
      "    mean_processing_ms: 0.9167626455356391\n",
      "  time_since_restore: 4000.7810690402985\n",
      "  time_this_iter_s: 2.131272077560425\n",
      "  time_total_s: 4000.7810690402985\n",
      "  timestamp: 1595952651\n",
      "  timesteps_since_restore: 2378000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2378000\n",
      "  training_iteration: 1189\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4000 s, 1189 iter, 2378000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.46171841144857\n",
      "  episode_reward_min: 12.124696183017663\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1190\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.797\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2257494181394577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007896671071648598\n",
      "        policy_loss: -0.002860561478883028\n",
      "        total_loss: 0.04181556776165962\n",
      "        vf_explained_var: 0.7024919986724854\n",
      "        vf_loss: 0.044676125049591064\n",
      "    load_time_ms: 1.106\n",
      "    num_steps_sampled: 2382000\n",
      "    num_steps_trained: 2382000\n",
      "    sample_time_ms: 2336.734\n",
      "    update_time_ms: 4.316\n",
      "  iterations_since_restore: 1191\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.06666666666666\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.93163557639361\n",
      "    mean_inference_ms: 1.3679771270724204\n",
      "    mean_processing_ms: 0.9162171548693534\n",
      "  time_since_restore: 4006.2147085666656\n",
      "  time_this_iter_s: 2.1609911918640137\n",
      "  time_total_s: 4006.2147085666656\n",
      "  timestamp: 1595952656\n",
      "  timesteps_since_restore: 2382000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2382000\n",
      "  training_iteration: 1191\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4006 s, 1191 iter, 2382000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-03\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.46171841144857\n",
      "  episode_reward_min: 12.124696183017663\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1190\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.348\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.21038872003555298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.802687807474285e-05\n",
      "        policy_loss: -7.142019603634253e-05\n",
      "        total_loss: 0.000635449425317347\n",
      "        vf_explained_var: 0.7062204480171204\n",
      "        vf_loss: 0.0007068866398185492\n",
      "    load_time_ms: 1.099\n",
      "    num_steps_sampled: 2388000\n",
      "    num_steps_trained: 2388000\n",
      "    sample_time_ms: 2342.445\n",
      "    update_time_ms: 4.059\n",
      "  iterations_since_restore: 1194\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.83333333333333\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.93163557639361\n",
      "    mean_inference_ms: 1.3679771270724204\n",
      "    mean_processing_ms: 0.9162171548693534\n",
      "  time_since_restore: 4012.5979158878326\n",
      "  time_this_iter_s: 2.1273624897003174\n",
      "  time_total_s: 4012.5979158878326\n",
      "  timestamp: 1595952663\n",
      "  timesteps_since_restore: 2388000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2388000\n",
      "  training_iteration: 1194\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4012 s, 1194 iter, 2388000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.450021518556467\n",
      "  episode_reward_min: 12.124696183017663\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1195\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.76\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.22948306798934937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008244342170655727\n",
      "        policy_loss: -0.008990691974759102\n",
      "        total_loss: 0.034200236201286316\n",
      "        vf_explained_var: 0.7123733758926392\n",
      "        vf_loss: 0.043190911412239075\n",
      "    load_time_ms: 1.063\n",
      "    num_steps_sampled: 2392000\n",
      "    num_steps_trained: 2392000\n",
      "    sample_time_ms: 2328.477\n",
      "    update_time_ms: 4.079\n",
      "  iterations_since_restore: 1196\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.96666666666667\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.923419677642593\n",
      "    mean_inference_ms: 1.3661370415302627\n",
      "    mean_processing_ms: 0.91567598675211\n",
      "  time_since_restore: 4018.0182600021362\n",
      "  time_this_iter_s: 2.1045725345611572\n",
      "  time_total_s: 4018.0182600021362\n",
      "  timestamp: 1595952668\n",
      "  timesteps_since_restore: 2392000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2392000\n",
      "  training_iteration: 1196\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4018 s, 1196 iter, 2392000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.450021518556467\n",
      "  episode_reward_min: 12.124696183017663\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1195\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.626\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.18830133974552155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00029949963209219277\n",
      "        policy_loss: -0.0013390292879194021\n",
      "        total_loss: -0.0005298328469507396\n",
      "        vf_explained_var: 0.7066363096237183\n",
      "        vf_loss: 0.0008091922500170767\n",
      "    load_time_ms: 1.044\n",
      "    num_steps_sampled: 2398000\n",
      "    num_steps_trained: 2398000\n",
      "    sample_time_ms: 2333.279\n",
      "    update_time_ms: 3.98\n",
      "  iterations_since_restore: 1199\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4\n",
      "    ram_util_percent: 64.43333333333334\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.923419677642593\n",
      "    mean_inference_ms: 1.3661370415302627\n",
      "    mean_processing_ms: 0.91567598675211\n",
      "  time_since_restore: 4024.4759352207184\n",
      "  time_this_iter_s: 2.1212806701660156\n",
      "  time_total_s: 4024.4759352207184\n",
      "  timestamp: 1595952675\n",
      "  timesteps_since_restore: 2398000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2398000\n",
      "  training_iteration: 1199\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4024 s, 1199 iter, 2398000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-20\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.453035737907305\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.803\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2568891644477844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007324553560465574\n",
      "        policy_loss: -0.002049574861302972\n",
      "        total_loss: 0.04336506873369217\n",
      "        vf_explained_var: 0.6922601461410522\n",
      "        vf_loss: 0.04541464149951935\n",
      "    load_time_ms: 1.035\n",
      "    num_steps_sampled: 2402000\n",
      "    num_steps_trained: 2402000\n",
      "    sample_time_ms: 2332.727\n",
      "    update_time_ms: 3.954\n",
      "  iterations_since_restore: 1201\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.93333333333334\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.915227904948889\n",
      "    mean_inference_ms: 1.3643019526525728\n",
      "    mean_processing_ms: 0.9151381355074047\n",
      "  time_since_restore: 4029.906809568405\n",
      "  time_this_iter_s: 2.118521213531494\n",
      "  time_total_s: 4029.906809568405\n",
      "  timestamp: 1595952680\n",
      "  timesteps_since_restore: 2402000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2402000\n",
      "  training_iteration: 1201\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4029 s, 1201 iter, 2402000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-27\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.453035737907305\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.832\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1769646555185318\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006397590623237193\n",
      "        policy_loss: -0.0013655085349455476\n",
      "        total_loss: -0.000589628703892231\n",
      "        vf_explained_var: 0.7647919058799744\n",
      "        vf_loss: 0.0007758848951198161\n",
      "    load_time_ms: 1.098\n",
      "    num_steps_sampled: 2408000\n",
      "    num_steps_trained: 2408000\n",
      "    sample_time_ms: 2340.411\n",
      "    update_time_ms: 4.108\n",
      "  iterations_since_restore: 1204\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.89999999999999\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.915227904948889\n",
      "    mean_inference_ms: 1.3643019526525728\n",
      "    mean_processing_ms: 0.9151381355074047\n",
      "  time_since_restore: 4036.3693566322327\n",
      "  time_this_iter_s: 2.1924796104431152\n",
      "  time_total_s: 4036.3693566322327\n",
      "  timestamp: 1595952687\n",
      "  timesteps_since_restore: 2408000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2408000\n",
      "  training_iteration: 1204\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4036 s, 1204 iter, 2408000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.4437066502343\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1205\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.343\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2659292221069336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004197858739644289\n",
      "        policy_loss: -0.0009797224774956703\n",
      "        total_loss: 0.04375293850898743\n",
      "        vf_explained_var: 0.7011680006980896\n",
      "        vf_loss: 0.04473266005516052\n",
      "    load_time_ms: 1.129\n",
      "    num_steps_sampled: 2412000\n",
      "    num_steps_trained: 2412000\n",
      "    sample_time_ms: 2333.846\n",
      "    update_time_ms: 4.073\n",
      "  iterations_since_restore: 1206\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.6\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.907101884710862\n",
      "    mean_inference_ms: 1.3624808871360274\n",
      "    mean_processing_ms: 0.9146050802643362\n",
      "  time_since_restore: 4041.7178630828857\n",
      "  time_this_iter_s: 2.0713298320770264\n",
      "  time_total_s: 4041.7178630828857\n",
      "  timestamp: 1595952692\n",
      "  timesteps_since_restore: 2412000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2412000\n",
      "  training_iteration: 1206\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4041 s, 1206 iter, 2412000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.4437066502343\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1205\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.485\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.16957956552505493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00017480537644587457\n",
      "        policy_loss: -0.00043021488818340003\n",
      "        total_loss: 0.0004373302508611232\n",
      "        vf_explained_var: 0.7452543377876282\n",
      "        vf_loss: 0.0008675217977724969\n",
      "    load_time_ms: 1.179\n",
      "    num_steps_sampled: 2418000\n",
      "    num_steps_trained: 2418000\n",
      "    sample_time_ms: 2327.894\n",
      "    update_time_ms: 4.088\n",
      "  iterations_since_restore: 1209\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.907101884710862\n",
      "    mean_inference_ms: 1.3624808871360274\n",
      "    mean_processing_ms: 0.9146050802643362\n",
      "  time_since_restore: 4048.120638847351\n",
      "  time_this_iter_s: 2.123603343963623\n",
      "  time_total_s: 4048.120638847351\n",
      "  timestamp: 1595952698\n",
      "  timesteps_since_restore: 2418000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2418000\n",
      "  training_iteration: 1209\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4048 s, 1209 iter, 2418000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.42549517734954\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.217\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.25882405042648315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008906768634915352\n",
      "        policy_loss: -0.0019437151495367289\n",
      "        total_loss: 0.04248332977294922\n",
      "        vf_explained_var: 0.7058299779891968\n",
      "        vf_loss: 0.04442703351378441\n",
      "    load_time_ms: 1.161\n",
      "    num_steps_sampled: 2422000\n",
      "    num_steps_trained: 2422000\n",
      "    sample_time_ms: 2343.15\n",
      "    update_time_ms: 4.12\n",
      "  iterations_since_restore: 1211\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63333333333333\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.899056121413229\n",
      "    mean_inference_ms: 1.3606781157727204\n",
      "    mean_processing_ms: 0.9140760068676437\n",
      "  time_since_restore: 4053.699999809265\n",
      "  time_this_iter_s: 2.0839266777038574\n",
      "  time_total_s: 4053.699999809265\n",
      "  timestamp: 1595952704\n",
      "  timesteps_since_restore: 2422000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2422000\n",
      "  training_iteration: 1211\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4053 s, 1211 iter, 2422000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.83804018639208\n",
      "  episode_reward_mean: 12.42549517734954\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.832\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.16028404235839844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0012250315630808473\n",
      "        policy_loss: -0.002798727946355939\n",
      "        total_loss: -0.0019248452736064792\n",
      "        vf_explained_var: 0.6782882809638977\n",
      "        vf_loss: 0.0008738908800296485\n",
      "    load_time_ms: 1.169\n",
      "    num_steps_sampled: 2428000\n",
      "    num_steps_trained: 2428000\n",
      "    sample_time_ms: 2336.918\n",
      "    update_time_ms: 3.899\n",
      "  iterations_since_restore: 1214\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.93333333333334\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.899056121413229\n",
      "    mean_inference_ms: 1.3606781157727204\n",
      "    mean_processing_ms: 0.9140760068676437\n",
      "  time_since_restore: 4060.093519926071\n",
      "  time_this_iter_s: 2.13419771194458\n",
      "  time_total_s: 4060.093519926071\n",
      "  timestamp: 1595952710\n",
      "  timesteps_since_restore: 2428000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2428000\n",
      "  training_iteration: 1214\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4060 s, 1214 iter, 2428000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.409494219359537\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1215\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.831\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.27817127108573914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004099640063941479\n",
      "        policy_loss: -0.0024046734906733036\n",
      "        total_loss: 0.04382369667291641\n",
      "        vf_explained_var: 0.6777675151824951\n",
      "        vf_loss: 0.046228375285863876\n",
      "    load_time_ms: 1.169\n",
      "    num_steps_sampled: 2432000\n",
      "    num_steps_trained: 2432000\n",
      "    sample_time_ms: 2343.618\n",
      "    update_time_ms: 3.846\n",
      "  iterations_since_restore: 1216\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.0\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.891078736551239\n",
      "    mean_inference_ms: 1.358891024302292\n",
      "    mean_processing_ms: 0.9135514085463537\n",
      "  time_since_restore: 4065.5084142684937\n",
      "  time_this_iter_s: 2.148332118988037\n",
      "  time_total_s: 4065.5084142684937\n",
      "  timestamp: 1595952716\n",
      "  timesteps_since_restore: 2432000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2432000\n",
      "  training_iteration: 1216\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4065 s, 1216 iter, 2432000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.409494219359537\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1215\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.09\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.15106262266635895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.6567478673532605e-05\n",
      "        policy_loss: 8.232688560383394e-05\n",
      "        total_loss: 0.0009571647387929261\n",
      "        vf_explained_var: 0.8093166947364807\n",
      "        vf_loss: 0.0008748458349145949\n",
      "    load_time_ms: 1.152\n",
      "    num_steps_sampled: 2438000\n",
      "    num_steps_trained: 2438000\n",
      "    sample_time_ms: 2340.924\n",
      "    update_time_ms: 3.822\n",
      "  iterations_since_restore: 1219\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.891078736551239\n",
      "    mean_inference_ms: 1.358891024302292\n",
      "    mean_processing_ms: 0.9135514085463537\n",
      "  time_since_restore: 4071.884371995926\n",
      "  time_this_iter_s: 2.134838819503784\n",
      "  time_total_s: 4071.884371995926\n",
      "  timestamp: 1595952722\n",
      "  timesteps_since_restore: 2438000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2438000\n",
      "  training_iteration: 1219\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4071 s, 1219 iter, 2438000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.417671549111404\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.26858440041542053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011360061034793034\n",
      "        policy_loss: -9.959030285244808e-05\n",
      "        total_loss: 0.04441475495696068\n",
      "        vf_explained_var: 0.7035876512527466\n",
      "        vf_loss: 0.04451431706547737\n",
      "    load_time_ms: 1.207\n",
      "    num_steps_sampled: 2442000\n",
      "    num_steps_trained: 2442000\n",
      "    sample_time_ms: 2324.224\n",
      "    update_time_ms: 3.729\n",
      "  iterations_since_restore: 1221\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.63333333333333\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.883161662003201\n",
      "    mean_inference_ms: 1.357117853156353\n",
      "    mean_processing_ms: 0.913031391150498\n",
      "  time_since_restore: 4077.2983963489532\n",
      "  time_this_iter_s: 2.1377222537994385\n",
      "  time_total_s: 4077.2983963489532\n",
      "  timestamp: 1595952728\n",
      "  timesteps_since_restore: 2442000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2442000\n",
      "  training_iteration: 1221\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4077 s, 1221 iter, 2442000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-14\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.417671549111404\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.131\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1761072427034378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.842647831537761e-05\n",
      "        policy_loss: -9.986400255002081e-05\n",
      "        total_loss: 0.0006285276613198221\n",
      "        vf_explained_var: 0.8477863073348999\n",
      "        vf_loss: 0.0007283860468305647\n",
      "    load_time_ms: 1.134\n",
      "    num_steps_sampled: 2448000\n",
      "    num_steps_trained: 2448000\n",
      "    sample_time_ms: 2337.838\n",
      "    update_time_ms: 4.157\n",
      "  iterations_since_restore: 1224\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.86666666666666\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.883161662003201\n",
      "    mean_inference_ms: 1.357117853156353\n",
      "    mean_processing_ms: 0.913031391150498\n",
      "  time_since_restore: 4083.8325567245483\n",
      "  time_this_iter_s: 2.1969330310821533\n",
      "  time_total_s: 4083.8325567245483\n",
      "  timestamp: 1595952734\n",
      "  timesteps_since_restore: 2448000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2448000\n",
      "  training_iteration: 1224\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4083 s, 1224 iter, 2448000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.406310094412502\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.416\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.2696773111820221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008923317305743694\n",
      "        policy_loss: -0.0016995297046378255\n",
      "        total_loss: 0.040956299751996994\n",
      "        vf_explained_var: 0.7306527495384216\n",
      "        vf_loss: 0.0426558256149292\n",
      "    load_time_ms: 1.38\n",
      "    num_steps_sampled: 2452000\n",
      "    num_steps_trained: 2452000\n",
      "    sample_time_ms: 2580.552\n",
      "    update_time_ms: 5.583\n",
      "  iterations_since_restore: 1226\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.64999999999999\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875332516056319\n",
      "    mean_inference_ms: 1.3553643796396984\n",
      "    mean_processing_ms: 0.9125248230614237\n",
      "  time_since_restore: 4091.8289427757263\n",
      "  time_this_iter_s: 4.317427635192871\n",
      "  time_total_s: 4091.8289427757263\n",
      "  timestamp: 1595952742\n",
      "  timesteps_since_restore: 2452000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2452000\n",
      "  training_iteration: 1226\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4091 s, 1226 iter, 2452000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.406310094412502\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.244\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1518879383802414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003711777681019157\n",
      "        policy_loss: -0.00033372401958331466\n",
      "        total_loss: 0.0004745101905427873\n",
      "        vf_explained_var: 0.8487313389778137\n",
      "        vf_loss: 0.000808239565230906\n",
      "    load_time_ms: 1.382\n",
      "    num_steps_sampled: 2456000\n",
      "    num_steps_trained: 2456000\n",
      "    sample_time_ms: 2730.905\n",
      "    update_time_ms: 7.244\n",
      "  iterations_since_restore: 1228\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.06666666666666\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.875332516056319\n",
      "    mean_inference_ms: 1.3553643796396984\n",
      "    mean_processing_ms: 0.9125248230614237\n",
      "  time_since_restore: 4097.589451313019\n",
      "  time_this_iter_s: 2.2584116458892822\n",
      "  time_total_s: 4097.589451313019\n",
      "  timestamp: 1595952748\n",
      "  timesteps_since_restore: 2456000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2456000\n",
      "  training_iteration: 1228\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4097 s, 1228 iter, 2456000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.406308347904638\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.99\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.16075091063976288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005847433349117637\n",
      "        policy_loss: -0.0002547104377299547\n",
      "        total_loss: 0.44197383522987366\n",
      "        vf_explained_var: -0.0011671781539916992\n",
      "        vf_loss: 0.4422284960746765\n",
      "    load_time_ms: 1.366\n",
      "    num_steps_sampled: 2460000\n",
      "    num_steps_trained: 2460000\n",
      "    sample_time_ms: 2733.869\n",
      "    update_time_ms: 7.358\n",
      "  iterations_since_restore: 1230\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35\n",
      "    ram_util_percent: 64.325\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.867837031332479\n",
      "    mean_inference_ms: 1.353685684873034\n",
      "    mean_processing_ms: 0.912039180702415\n",
      "  time_since_restore: 4103.027705192566\n",
      "  time_this_iter_s: 3.288695812225342\n",
      "  time_total_s: 4103.027705192566\n",
      "  timestamp: 1595952753\n",
      "  timesteps_since_restore: 2460000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2460000\n",
      "  training_iteration: 1230\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4103 s, 1230 iter, 2460000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.40630834790464\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.268\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1259351372718811\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001748613896779716\n",
      "        policy_loss: -0.003213921096175909\n",
      "        total_loss: -0.0023208041675388813\n",
      "        vf_explained_var: 0.7858640551567078\n",
      "        vf_loss: 0.0008931102929636836\n",
      "    load_time_ms: 1.337\n",
      "    num_steps_sampled: 2466000\n",
      "    num_steps_trained: 2466000\n",
      "    sample_time_ms: 2758.828\n",
      "    update_time_ms: 7.388\n",
      "  iterations_since_restore: 1233\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.55000000000001\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.867837031332479\n",
      "    mean_inference_ms: 1.353685684873034\n",
      "    mean_processing_ms: 0.9120391807024152\n",
      "  time_since_restore: 4109.768049001694\n",
      "  time_this_iter_s: 2.492105722427368\n",
      "  time_total_s: 4109.768049001694\n",
      "  timestamp: 1595952760\n",
      "  timesteps_since_restore: 2466000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2466000\n",
      "  training_iteration: 1233\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4109 s, 1233 iter, 2466000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-46\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.410482071659452\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1235\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.93\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.12435740977525711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.409078923752531e-05\n",
      "        policy_loss: -0.000300688756396994\n",
      "        total_loss: 0.44805359840393066\n",
      "        vf_explained_var: 0.0042882561683654785\n",
      "        vf_loss: 0.44835424423217773\n",
      "    load_time_ms: 1.155\n",
      "    num_steps_sampled: 2470000\n",
      "    num_steps_trained: 2470000\n",
      "    sample_time_ms: 2739.57\n",
      "    update_time_ms: 7.086\n",
      "  iterations_since_restore: 1235\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.67999999999999\n",
      "    ram_util_percent: 64.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.86043943180338\n",
      "    mean_inference_ms: 1.3520301970864548\n",
      "    mean_processing_ms: 0.9115570114639311\n",
      "  time_since_restore: 4115.379075527191\n",
      "  time_this_iter_s: 3.380901336669922\n",
      "  time_total_s: 4115.379075527191\n",
      "  timestamp: 1595952766\n",
      "  timesteps_since_restore: 2470000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2470000\n",
      "  training_iteration: 1235\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4115 s, 1235 iter, 2470000 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.729962182386172\n",
      "  episode_reward_mean: 12.410482071659452\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1235\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.559\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.08773336559534073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005582178710028529\n",
      "        policy_loss: -0.0005986595060676336\n",
      "        total_loss: 0.0004916610778309405\n",
      "        vf_explained_var: 0.785201907157898\n",
      "        vf_loss: 0.0010903270449489355\n",
      "    load_time_ms: 1.085\n",
      "    num_steps_sampled: 2476000\n",
      "    num_steps_trained: 2476000\n",
      "    sample_time_ms: 2377.705\n",
      "    update_time_ms: 4.11\n",
      "  iterations_since_restore: 1238\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.03333333333333\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.860439431803382\n",
      "    mean_inference_ms: 1.3520301970864548\n",
      "    mean_processing_ms: 0.911557011463931\n",
      "  time_since_restore: 4121.743109464645\n",
      "  time_this_iter_s: 2.115360975265503\n",
      "  time_total_s: 4121.743109464645\n",
      "  timestamp: 1595952772\n",
      "  timesteps_since_restore: 2476000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2476000\n",
      "  training_iteration: 1238\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4121 s, 1238 iter, 2476000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-12-58\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.748563142154454\n",
      "  episode_reward_mean: 12.421977135778796\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.497\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.10400185734033585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0012276415945962071\n",
      "        policy_loss: -0.0007451555575244129\n",
      "        total_loss: 0.45723918080329895\n",
      "        vf_explained_var: -0.008112311363220215\n",
      "        vf_loss: 0.45798423886299133\n",
      "    load_time_ms: 1.086\n",
      "    num_steps_sampled: 2480000\n",
      "    num_steps_trained: 2480000\n",
      "    sample_time_ms: 2378.932\n",
      "    update_time_ms: 4.099\n",
      "  iterations_since_restore: 1240\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.720000000000006\n",
      "    ram_util_percent: 64.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.853109366601314\n",
      "    mean_inference_ms: 1.350389666626452\n",
      "    mean_processing_ms: 0.9110797585450274\n",
      "  time_since_restore: 4127.194897174835\n",
      "  time_this_iter_s: 3.314765214920044\n",
      "  time_total_s: 4127.194897174835\n",
      "  timestamp: 1595952778\n",
      "  timesteps_since_restore: 2480000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2480000\n",
      "  training_iteration: 1240\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4127 s, 1240 iter, 2480000 ts, 12.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.748563142154454\n",
      "  episode_reward_mean: 12.421977135778796\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.401\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.05739126726984978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001321038289461285\n",
      "        policy_loss: -0.0009486046037636697\n",
      "        total_loss: 0.0003256797790527344\n",
      "        vf_explained_var: 0.6746050119400024\n",
      "        vf_loss: 0.0012742914259433746\n",
      "    load_time_ms: 1.102\n",
      "    num_steps_sampled: 2486000\n",
      "    num_steps_trained: 2486000\n",
      "    sample_time_ms: 2352.684\n",
      "    update_time_ms: 4.046\n",
      "  iterations_since_restore: 1243\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.63333333333334\n",
      "    ram_util_percent: 64.33333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.853109366601313\n",
      "    mean_inference_ms: 1.350389666626452\n",
      "    mean_processing_ms: 0.9110797585450274\n",
      "  time_since_restore: 4133.656223773956\n",
      "  time_this_iter_s: 2.15267276763916\n",
      "  time_total_s: 4133.656223773956\n",
      "  timestamp: 1595952784\n",
      "  timesteps_since_restore: 2486000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2486000\n",
      "  training_iteration: 1243\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4133 s, 1243 iter, 2486000 ts, 12.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-10\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.458003690012749\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1245\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.249\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.08169496059417725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002180603099986911\n",
      "        policy_loss: -0.002288175979629159\n",
      "        total_loss: 0.45668280124664307\n",
      "        vf_explained_var: 0.0028893351554870605\n",
      "        vf_loss: 0.4589709937572479\n",
      "    load_time_ms: 1.089\n",
      "    num_steps_sampled: 2490000\n",
      "    num_steps_trained: 2490000\n",
      "    sample_time_ms: 2331.886\n",
      "    update_time_ms: 4.007\n",
      "  iterations_since_restore: 1245\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18\n",
      "    ram_util_percent: 64.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.845831263427511\n",
      "    mean_inference_ms: 1.3487601937601568\n",
      "    mean_processing_ms: 0.9106060256931989\n",
      "  time_since_restore: 4139.058041095734\n",
      "  time_this_iter_s: 3.2674663066864014\n",
      "  time_total_s: 4139.058041095734\n",
      "  timestamp: 1595952790\n",
      "  timesteps_since_restore: 2490000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2490000\n",
      "  training_iteration: 1245\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4139 s, 1245 iter, 2490000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.458003690012747\n",
      "  episode_reward_min: 12.12794616039066\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1245\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.177\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.0721910148859024\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004023342858999968\n",
      "        policy_loss: -0.001987550174817443\n",
      "        total_loss: -0.0009674105676822364\n",
      "        vf_explained_var: 0.8669627904891968\n",
      "        vf_loss: 0.0010201296536251903\n",
      "    load_time_ms: 1.107\n",
      "    num_steps_sampled: 2496000\n",
      "    num_steps_trained: 2496000\n",
      "    sample_time_ms: 2343.073\n",
      "    update_time_ms: 4.087\n",
      "  iterations_since_restore: 1248\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33333333333333\n",
      "    ram_util_percent: 64.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8458312634275105\n",
      "    mean_inference_ms: 1.3487601937601565\n",
      "    mean_processing_ms: 0.9106060256931992\n",
      "  time_since_restore: 4145.535041093826\n",
      "  time_this_iter_s: 2.1352221965789795\n",
      "  time_total_s: 4145.535041093826\n",
      "  timestamp: 1595952796\n",
      "  timesteps_since_restore: 2496000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2496000\n",
      "  training_iteration: 1248\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4145 s, 1248 iter, 2496000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.477831567181711\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1250\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.459\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.12697093188762665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006414475501514971\n",
      "        policy_loss: -0.0009733762708492577\n",
      "        total_loss: 0.44277122616767883\n",
      "        vf_explained_var: -0.0005331039428710938\n",
      "        vf_loss: 0.44374462962150574\n",
      "    load_time_ms: 1.104\n",
      "    num_steps_sampled: 2500000\n",
      "    num_steps_trained: 2500000\n",
      "    sample_time_ms: 2340.411\n",
      "    update_time_ms: 4.239\n",
      "  iterations_since_restore: 1250\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.06\n",
      "    ram_util_percent: 64.25999999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.838618577205727\n",
      "    mean_inference_ms: 1.3471457598496628\n",
      "    mean_processing_ms: 0.9101396355963223\n",
      "  time_since_restore: 4150.96284365654\n",
      "  time_this_iter_s: 3.2986550331115723\n",
      "  time_total_s: 4150.96284365654\n",
      "  timestamp: 1595952802\n",
      "  timesteps_since_restore: 2500000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2500000\n",
      "  training_iteration: 1250\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4150 s, 1250 iter, 2500000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.477831567181711\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1250\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.168\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.0888102576136589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003189712588209659\n",
      "        policy_loss: -0.0003102002083323896\n",
      "        total_loss: 0.0005822009989060462\n",
      "        vf_explained_var: 0.8029764294624329\n",
      "        vf_loss: 0.00089240912348032\n",
      "    load_time_ms: 1.083\n",
      "    num_steps_sampled: 2506000\n",
      "    num_steps_trained: 2506000\n",
      "    sample_time_ms: 2355.403\n",
      "    update_time_ms: 4.266\n",
      "  iterations_since_restore: 1253\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.3\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.838618577205727\n",
      "    mean_inference_ms: 1.3471457598496628\n",
      "    mean_processing_ms: 0.9101396355963222\n",
      "  time_since_restore: 4157.583161115646\n",
      "  time_this_iter_s: 2.2895920276641846\n",
      "  time_total_s: 4157.583161115646\n",
      "  timestamp: 1595952808\n",
      "  timesteps_since_restore: 2506000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2506000\n",
      "  training_iteration: 1253\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4157 s, 1253 iter, 2506000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.478468197009779\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1255\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.208\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1124965250492096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00042261191993020475\n",
      "        policy_loss: -0.000764144875574857\n",
      "        total_loss: 0.4427143931388855\n",
      "        vf_explained_var: 0.004185497760772705\n",
      "        vf_loss: 0.4434785544872284\n",
      "    load_time_ms: 1.074\n",
      "    num_steps_sampled: 2510000\n",
      "    num_steps_trained: 2510000\n",
      "    sample_time_ms: 2387.546\n",
      "    update_time_ms: 4.353\n",
      "  iterations_since_restore: 1255\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.89999999999999\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.831462661034193\n",
      "    mean_inference_ms: 1.345541767902679\n",
      "    mean_processing_ms: 0.9096798280743202\n",
      "  time_since_restore: 4163.307812929153\n",
      "  time_this_iter_s: 3.4967968463897705\n",
      "  time_total_s: 4163.307812929153\n",
      "  timestamp: 1595952814\n",
      "  timesteps_since_restore: 2510000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2510000\n",
      "  training_iteration: 1255\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4163 s, 1255 iter, 2510000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.478468197009779\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1255\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.128\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.09749043732881546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006612896686419845\n",
      "        policy_loss: 0.0004788837395608425\n",
      "        total_loss: 0.001263400074094534\n",
      "        vf_explained_var: 0.8676521182060242\n",
      "        vf_loss: 0.0007845227955840528\n",
      "    load_time_ms: 1.045\n",
      "    num_steps_sampled: 2516000\n",
      "    num_steps_trained: 2516000\n",
      "    sample_time_ms: 2456.056\n",
      "    update_time_ms: 4.867\n",
      "  iterations_since_restore: 1258\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.0\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.831462661034193\n",
      "    mean_inference_ms: 1.345541767902679\n",
      "    mean_processing_ms: 0.9096798280743201\n",
      "  time_since_restore: 4170.473591804504\n",
      "  time_this_iter_s: 2.5738580226898193\n",
      "  time_total_s: 4170.473591804504\n",
      "  timestamp: 1595952821\n",
      "  timesteps_since_restore: 2516000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2516000\n",
      "  training_iteration: 1258\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4170 s, 1258 iter, 2516000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.476730748860836\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.828\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1279943585395813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.207516995957121e-05\n",
      "        policy_loss: 0.00018625115626491606\n",
      "        total_loss: 0.44077610969543457\n",
      "        vf_explained_var: -0.0009535551071166992\n",
      "        vf_loss: 0.44058990478515625\n",
      "    load_time_ms: 1.168\n",
      "    num_steps_sampled: 2520000\n",
      "    num_steps_trained: 2520000\n",
      "    sample_time_ms: 2557.07\n",
      "    update_time_ms: 4.923\n",
      "  iterations_since_restore: 1260\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.93333333333334\n",
      "    ram_util_percent: 64.63333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.824499873003915\n",
      "    mean_inference_ms: 1.3439875892865192\n",
      "    mean_processing_ms: 0.9092373099846306\n",
      "  time_since_restore: 4176.934494972229\n",
      "  time_this_iter_s: 3.8645894527435303\n",
      "  time_total_s: 4176.934494972229\n",
      "  timestamp: 1595952828\n",
      "  timesteps_since_restore: 2520000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2520000\n",
      "  training_iteration: 1260\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4176 s, 1260 iter, 2520000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.476730748860835\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.488\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.008798339404165745\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0010435049189254642\n",
      "        policy_loss: -0.0002738580806180835\n",
      "        total_loss: 0.0018786363070830703\n",
      "        vf_explained_var: 0.9410486221313477\n",
      "        vf_loss: 0.002152503002434969\n",
      "    load_time_ms: 1.237\n",
      "    num_steps_sampled: 2524000\n",
      "    num_steps_trained: 2524000\n",
      "    sample_time_ms: 2632.794\n",
      "    update_time_ms: 5.052\n",
      "  iterations_since_restore: 1262\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.83333333333333\n",
      "    ram_util_percent: 64.53333333333333\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.824499873003917\n",
      "    mean_inference_ms: 1.3439875892865185\n",
      "    mean_processing_ms: 0.9092373099846306\n",
      "  time_since_restore: 4182.037725925446\n",
      "  time_this_iter_s: 2.408299446105957\n",
      "  time_total_s: 4182.037725925446\n",
      "  timestamp: 1595952833\n",
      "  timesteps_since_restore: 2524000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2524000\n",
      "  training_iteration: 1262\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4182 s, 1262 iter, 2524000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.455300517129848\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1265\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 32.254\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1019439548254013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0025898932944983244\n",
      "        policy_loss: -0.002416477771475911\n",
      "        total_loss: 0.44226017594337463\n",
      "        vf_explained_var: -0.007249116897583008\n",
      "        vf_loss: 0.44467663764953613\n",
      "    load_time_ms: 1.442\n",
      "    num_steps_sampled: 2530000\n",
      "    num_steps_trained: 2530000\n",
      "    sample_time_ms: 2672.688\n",
      "    update_time_ms: 5.189\n",
      "  iterations_since_restore: 1265\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.2\n",
      "    ram_util_percent: 64.66\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8176831515970635\n",
      "    mean_inference_ms: 1.342467574994101\n",
      "    mean_processing_ms: 0.9088132446522579\n",
      "  time_since_restore: 4190.5330286026\n",
      "  time_this_iter_s: 3.803236484527588\n",
      "  time_total_s: 4190.5330286026\n",
      "  timestamp: 1595952841\n",
      "  timesteps_since_restore: 2530000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2530000\n",
      "  training_iteration: 1265\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4190 s, 1265 iter, 2530000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.45530051712985\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1265\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 34.745\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.055340591818094254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005226940847933292\n",
      "        policy_loss: -0.0020419196225702763\n",
      "        total_loss: 0.0007760391454212368\n",
      "        vf_explained_var: 0.9191386699676514\n",
      "        vf_loss: 0.0028179464861750603\n",
      "    load_time_ms: 1.534\n",
      "    num_steps_sampled: 2534000\n",
      "    num_steps_trained: 2534000\n",
      "    sample_time_ms: 2749.912\n",
      "    update_time_ms: 6.0\n",
      "  iterations_since_restore: 1267\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.975\n",
      "    ram_util_percent: 64.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.817683151597065\n",
      "    mean_inference_ms: 1.3424675749941013\n",
      "    mean_processing_ms: 0.9088132446522579\n",
      "  time_since_restore: 4195.937232971191\n",
      "  time_this_iter_s: 2.722891330718994\n",
      "  time_total_s: 4195.937232971191\n",
      "  timestamp: 1595952847\n",
      "  timesteps_since_restore: 2534000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2534000\n",
      "  training_iteration: 1267\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4195 s, 1267 iter, 2534000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.45530051712985\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1265\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 33.346\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.09820658713579178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.002448157640174e-05\n",
      "        policy_loss: 0.00029508472653105855\n",
      "        total_loss: 0.0010871978010982275\n",
      "        vf_explained_var: 0.7776104211807251\n",
      "        vf_loss: 0.0007921155192889273\n",
      "    load_time_ms: 1.408\n",
      "    num_steps_sampled: 2538000\n",
      "    num_steps_trained: 2538000\n",
      "    sample_time_ms: 2776.396\n",
      "    update_time_ms: 5.777\n",
      "  iterations_since_restore: 1269\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.725\n",
      "    ram_util_percent: 64.975\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.817683151597065\n",
      "    mean_inference_ms: 1.3424675749941013\n",
      "    mean_processing_ms: 0.9088132446522579\n",
      "  time_since_restore: 4201.35143828392\n",
      "  time_this_iter_s: 2.7267184257507324\n",
      "  time_total_s: 4201.35143828392\n",
      "  timestamp: 1595952852\n",
      "  timesteps_since_restore: 2538000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2538000\n",
      "  training_iteration: 1269\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4201 s, 1269 iter, 2538000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.458047745721155\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1270\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 38.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1107841208577156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00040439903386868536\n",
      "        policy_loss: -0.0005619888543151319\n",
      "        total_loss: 0.4426925480365753\n",
      "        vf_explained_var: 0.0023884177207946777\n",
      "        vf_loss: 0.4432544410228729\n",
      "    load_time_ms: 1.694\n",
      "    num_steps_sampled: 2540000\n",
      "    num_steps_trained: 2540000\n",
      "    sample_time_ms: 2900.574\n",
      "    update_time_ms: 5.616\n",
      "  iterations_since_restore: 1270\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.72857142857143\n",
      "    ram_util_percent: 65.35714285714286\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.811208069672563\n",
      "    mean_inference_ms: 1.3410309041032267\n",
      "    mean_processing_ms: 0.9084270432982716\n",
      "  time_since_restore: 4206.541128158569\n",
      "  time_this_iter_s: 5.189689874649048\n",
      "  time_total_s: 4206.541128158569\n",
      "  timestamp: 1595952857\n",
      "  timesteps_since_restore: 2540000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2540000\n",
      "  training_iteration: 1270\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4206 s, 1270 iter, 2540000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.458047745721155\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1270\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 43.727\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.33622846007347107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00699498550966382\n",
      "        policy_loss: -0.0018742609536275268\n",
      "        total_loss: 0.04123760759830475\n",
      "        vf_explained_var: 0.7364262342453003\n",
      "        vf_loss: 0.04311187192797661\n",
      "    load_time_ms: 1.729\n",
      "    num_steps_sampled: 2542000\n",
      "    num_steps_trained: 2542000\n",
      "    sample_time_ms: 3172.629\n",
      "    update_time_ms: 6.654\n",
      "  iterations_since_restore: 1271\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.6\n",
      "    ram_util_percent: 65.82499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8112080696725625\n",
      "    mean_inference_ms: 1.3410309041032267\n",
      "    mean_processing_ms: 0.9084270432982714\n",
      "  time_since_restore: 4212.035414457321\n",
      "  time_this_iter_s: 5.494286298751831\n",
      "  time_total_s: 4212.035414457321\n",
      "  timestamp: 1595952863\n",
      "  timesteps_since_restore: 2542000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2542000\n",
      "  training_iteration: 1271\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4212 s, 1271 iter, 2542000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.458047745721155\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1270\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 46.37\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.08060649037361145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002352698502363637\n",
      "        policy_loss: 5.720615263271611e-06\n",
      "        total_loss: 0.0008702645427547395\n",
      "        vf_explained_var: 0.8556041121482849\n",
      "        vf_loss: 0.0008645412744954228\n",
      "    load_time_ms: 1.872\n",
      "    num_steps_sampled: 2546000\n",
      "    num_steps_trained: 2546000\n",
      "    sample_time_ms: 3613.884\n",
      "    update_time_ms: 9.585\n",
      "  iterations_since_restore: 1273\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.11428571428573\n",
      "    ram_util_percent: 66.37142857142858\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.8112080696725625\n",
      "    mean_inference_ms: 1.3410309041032267\n",
      "    mean_processing_ms: 0.9084270432982714\n",
      "  time_since_restore: 4221.453967094421\n",
      "  time_this_iter_s: 4.580205202102661\n",
      "  time_total_s: 4221.453967094421\n",
      "  timestamp: 1595952872\n",
      "  timesteps_since_restore: 2546000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2546000\n",
      "  training_iteration: 1273\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4221 s, 1273 iter, 2546000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.459019901169816\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1275\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 39.132\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1160372868180275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0010699626291170716\n",
      "        policy_loss: 0.0006475629634223878\n",
      "        total_loss: 0.4454241394996643\n",
      "        vf_explained_var: -0.0024999380111694336\n",
      "        vf_loss: 0.4447766840457916\n",
      "    load_time_ms: 1.667\n",
      "    num_steps_sampled: 2550000\n",
      "    num_steps_trained: 2550000\n",
      "    sample_time_ms: 3550.361\n",
      "    update_time_ms: 9.458\n",
      "  iterations_since_restore: 1275\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.75000000000001\n",
      "    ram_util_percent: 66.475\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.805383388808448\n",
      "    mean_inference_ms: 1.339767750612464\n",
      "    mean_processing_ms: 0.9080726464145057\n",
      "  time_since_restore: 4226.698914051056\n",
      "  time_this_iter_s: 2.861175775527954\n",
      "  time_total_s: 4226.698914051056\n",
      "  timestamp: 1595952878\n",
      "  timesteps_since_restore: 2550000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2550000\n",
      "  training_iteration: 1275\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4226 s, 1275 iter, 2550000 ts, 12.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.459019901169814\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1275\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 35.426\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.06845808774232864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001724923960864544\n",
      "        policy_loss: -0.0015596385346725583\n",
      "        total_loss: -0.0006107635563239455\n",
      "        vf_explained_var: 0.801252007484436\n",
      "        vf_loss: 0.0009488716023042798\n",
      "    load_time_ms: 1.568\n",
      "    num_steps_sampled: 2556000\n",
      "    num_steps_trained: 2556000\n",
      "    sample_time_ms: 3422.355\n",
      "    update_time_ms: 7.918\n",
      "  iterations_since_restore: 1278\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.85\n",
      "    ram_util_percent: 66.35000000000001\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.805383388808448\n",
      "    mean_inference_ms: 1.339767750612464\n",
      "    mean_processing_ms: 0.9080726464145059\n",
      "  time_since_restore: 4233.448363780975\n",
      "  time_this_iter_s: 2.3828928470611572\n",
      "  time_total_s: 4233.448363780975\n",
      "  timestamp: 1595952884\n",
      "  timesteps_since_restore: 2556000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2556000\n",
      "  training_iteration: 1278\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4233 s, 1278 iter, 2556000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-50\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.460718218682423\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 28.013\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.06786765903234482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0010718416888266802\n",
      "        policy_loss: -0.0013258199905976653\n",
      "        total_loss: 0.45578649640083313\n",
      "        vf_explained_var: -0.006925463676452637\n",
      "        vf_loss: 0.45711225271224976\n",
      "    load_time_ms: 1.242\n",
      "    num_steps_sampled: 2560000\n",
      "    num_steps_trained: 2560000\n",
      "    sample_time_ms: 3168.289\n",
      "    update_time_ms: 7.858\n",
      "  iterations_since_restore: 1280\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.25\n",
      "    ram_util_percent: 66.27499999999999\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.799620414583796\n",
      "    mean_inference_ms: 1.3385283183916459\n",
      "    mean_processing_ms: 0.9077215206312009\n",
      "  time_since_restore: 4238.715114116669\n",
      "  time_this_iter_s: 3.174062728881836\n",
      "  time_total_s: 4238.715114116669\n",
      "  timestamp: 1595952890\n",
      "  timesteps_since_restore: 2560000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2560000\n",
      "  training_iteration: 1280\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4238 s, 1280 iter, 2560000 ts, 12.5 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.460718218682423\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 17.964\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.043729186058044434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.000673932081554085\n",
      "        policy_loss: 4.146194623899646e-05\n",
      "        total_loss: 0.0010409641545265913\n",
      "        vf_explained_var: 0.867794930934906\n",
      "        vf_loss: 0.0009995056316256523\n",
      "    load_time_ms: 0.987\n",
      "    num_steps_sampled: 2566000\n",
      "    num_steps_trained: 2566000\n",
      "    sample_time_ms: 2355.212\n",
      "    update_time_ms: 4.476\n",
      "  iterations_since_restore: 1283\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.60000000000001\n",
      "    ram_util_percent: 66.3\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.799620414583796\n",
      "    mean_inference_ms: 1.3385283183916459\n",
      "    mean_processing_ms: 0.9077215206312007\n",
      "  time_since_restore: 4245.3042113780975\n",
      "  time_this_iter_s: 2.25407075881958\n",
      "  time_total_s: 4245.3042113780975\n",
      "  timestamp: 1595952896\n",
      "  timesteps_since_restore: 2566000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2566000\n",
      "  training_iteration: 1283\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4245 s, 1283 iter, 2566000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=12226)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=12227)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=12228)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=12223)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=12224)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-28_19-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: 12.976854022894397\n",
      "  episode_reward_mean: 12.468243100794254\n",
      "  episode_reward_min: 12.176705114728456\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1285\n",
      "  experiment_id: 67a41ba09bf0458c86f090b701c4ea07\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 19.307\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.0822712704539299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3188709747046232e-05\n",
      "        policy_loss: -0.00012389468611218035\n",
      "        total_loss: 0.44883477687835693\n",
      "        vf_explained_var: 0.0036395788192749023\n",
      "        vf_loss: 0.44895869493484497\n",
      "    load_time_ms: 1.011\n",
      "    num_steps_sampled: 2570000\n",
      "    num_steps_trained: 2570000\n",
      "    sample_time_ms: 2438.691\n",
      "    update_time_ms: 4.509\n",
      "  iterations_since_restore: 1285\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.78333333333333\n",
      "    ram_util_percent: 66.56666666666668\n",
      "  pid: 12225\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.793959699865546\n",
      "    mean_inference_ms: 1.3373201761842726\n",
      "    mean_processing_ms: 0.9073800691529453\n",
      "  time_since_restore: 4251.40500831604\n",
      "  time_this_iter_s: 3.8652682304382324\n",
      "  time_total_s: 4251.40500831604\n",
      "  timestamp: 1595952902\n",
      "  timesteps_since_restore: 2570000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2570000\n",
      "  training_iteration: 1285\n",
      "  trial_id: 82c0698a\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example16\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [6 CPUs, 0 GPUs], [pid=12225], 4251 s, 1285 iter, 2570000 ts, 12.5 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 20,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1500,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizing the results\n",
    "\n",
    "The simulation results are saved within the `ray_results/training_example` directory (we defined `training_example` at the start of this tutorial). The `ray_results` folder is by default located at your root `~/ray_results`. \n",
    "\n",
    "You can run `tensorboard --logdir=~/ray_results/training_example` (install it with `pip install tensorboard`) to visualize the different data outputted by your simulation.\n",
    "\n",
    "For more instructions about visualizing, please see `tutorial05_visualize.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Restart from a checkpoint / Transfer learning\n",
    "\n",
    "If you wish to do transfer learning, or to resume a previous training, you will need to start the simulation from a previous checkpoint. To do that, you can add a `restore` parameter in the `run_experiments` argument, as follows:\n",
    "\n",
    "```python\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"restore\": \"/ray_results/experiment/dir/checkpoint_50/checkpoint-50\"\n",
    "        \"checkpoint_freq\": 1,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 1,\n",
    "        },\n",
    "    },\n",
    "})\n",
    "```\n",
    "\n",
    "The `\"restore\"` path should be such that the `[restore]/.tune_metadata` file exists.\n",
    "\n",
    "There is also a `\"resume\"` parameter that you can set to `True` if you just wish to continue the training from a previously saved checkpoint, in case you are still training on the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = run_experiments({\n",
    "#     flow_params[\"exp_tag\"]: {\n",
    "#         \"run\": alg_run,\n",
    "#         \"env\": gym_name,\n",
    "#         \"config\": {\n",
    "#             **config\n",
    "#         },\n",
    "#         \"restore\": \"/ray_results/training_example13/PPO_EnergyOptPOEnv-v0_0_2020-07-23_13-30-07yze28sum/checkpoint_400/checkpoint-400\", \n",
    "#         \"checkpoint_freq\": 20,\n",
    "#         \"checkpoint_at_end\": True,\n",
    "#         \"max_failures\": 999,\n",
    "#         \"stop\": {\n",
    "#             \"training_iteration\": 700,\n",
    "#         },\n",
    "#     },\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.vehicles import Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
