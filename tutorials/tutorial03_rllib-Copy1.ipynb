{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Running RLlib Experiments\n",
    "\n",
    "This tutorial walks you through the process of running traffic simulations in Flow with trainable RLlib-powered agents. Autonomous agents will learn to maximize a certain reward over the rollouts, using the [**RLlib**](https://ray.readthedocs.io/en/latest/rllib.html) library ([citation](https://arxiv.org/abs/1712.09381)) ([installation instructions](https://flow.readthedocs.io/en/latest/flow_setup.html#optional-install-ray-rllib)). Simulations of this form will depict the propensity of RL agents to influence the traffic of a human fleet in order to make the whole fleet more efficient (for some given metrics). \n",
    "\n",
    "In this tutorial, we simulate an initially perturbed single lane ring road, where we introduce a single autonomous vehicle. We witness that, after some training, that the autonomous vehicle learns to dissipate the formation and propagation of \"phantom jams\" which form when only human driver dynamics are involved.\n",
    "\n",
    "## 1. Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *network*, and an *environment*. Networks describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc... in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act as the primary interface between the reinforcement learning algorithm and the network. Moreover, custom environments may be used to modify the dynamical features of an network. Finally, in the RL case, it is in the *environment* that the state/action spaces and the reward function are defined. \n",
    "\n",
    "## 2. Setting up a Network\n",
    "Flow contains a plethora of pre-designed networks used to replicate highways, intersections, and merges in both closed and open settings. All these networks are located in flow/networks. For this tutorial, which involves a single lane ring road, we will use the network `RingNetwork`.\n",
    "\n",
    "### 2.1 Setting up Network Parameters\n",
    "\n",
    "The network mentioned at the start of this section, as well as all other networks in Flow, are parameterized by the following arguments: \n",
    "* name\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "\n",
    "These parameters are explained in detail in `tutorial01_sumo.ipynb`. Moreover, all parameters excluding vehicles (covered in section 2.2) do not change from the previous tutorial. Accordingly, we specify them nearly as we have before, and leave further explanations of the parameters to `tutorial01_sumo.ipynb`.\n",
    "\n",
    "We begin by choosing the network the experiment will be trained on. We use one of Flow's builtin networks, located in `flow.networks`. A list of all available networks can be found by running the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flow.networks as networks\n",
    "\n",
    "# print(networks.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we choose to use the ring road network. The network class is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.networks import RingNetwork\n",
    "\n",
    "# ring road network class\n",
    "network_name = RingNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key difference between SUMO and RLlib experiments is that, in RLlib experiments, the network classes do not need to be defined; instead users should simply name the network class they wish to use. Later on, an environment setup module will import the correct network class based on the provided names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter classes to the network class\n",
    "from flow.core.params import NetParams, InitialConfig\n",
    "\n",
    "# name of the network\n",
    "name = \"training_example15\"\n",
    "\n",
    "# network-specific parameters\n",
    "from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)\n",
    "\n",
    "# initial configuration to vehicles\n",
    "initial_config = InitialConfig(spacing=\"uniform\", perturbation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Trainable Autonomous Vehicles\n",
    "The `Vehicles` class stores state information on all vehicles in the network. This class is used to identify the dynamical features of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various `get` methods within this class.\n",
    "\n",
    "The dynamics of vehicles in the `Vehicles` class can either be depicted by sumo or by the dynamical methods located in flow/controllers. For human-driven vehicles, we use the IDM model for acceleration behavior, with exogenous gaussian acceleration noise with std 0.2 m/s2 to induce perturbations that produce stop-and-go behavior. In addition, we use the `ContinousRouter` routing controller so that the vehicles may maintain their routes closed networks.\n",
    "\n",
    "As we have done in `tutorial01_sumo.ipynb`, human-driven vehicles are defined in the `VehicleParams` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles class\n",
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# vehicles dynamics models\n",
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "#vehicles.add(\"human\",\n",
    "#             acceleration_controller=(IDMController, {}),\n",
    "#             routing_controller=(ContinuousRouter, {}),\n",
    "#             num_vehicles=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above addition to the `Vehicles` class only accounts for 21 of the 22 vehicles that are placed in the network. We now add an additional trainable autuonomous vehicle whose actions are dictated by an RL agent. This is done by specifying an `RLController` as the acceleraton controller to the vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this controller serves primarirly as a placeholder that marks the vehicle as a component of the RL agent, meaning that lane changing and routing actions can also be specified by the RL agent to this vehicle.\n",
    "\n",
    "We finally add the vehicle as follows, while again using the `ContinuousRouter` to perpetually maintain the vehicle within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow.energy_models.toyota_energy import TacomaEnergy\n",
    "# vehicles.add(veh_id=\"rl\",\n",
    "#              acceleration_controller=(RLController, {}),\n",
    "#              routing_controller=(ContinuousRouter, {}),\n",
    "#              initial_speed =20,\n",
    "#              energy_model = TacomaEnergy,\n",
    "#              num_vehicles=1)\n",
    "\n",
    "\n",
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             initial_speed =20,\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several environments in Flow exist to train RL agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. The use of an environment allows us to view the cumulative reward simulation rollouts receive, along with to specify the state/action spaces.\n",
    "\n",
    "Sumo envrionments in Flow are parametrized by three components:\n",
    "* `SumoParams`\n",
    "* `EnvParams`\n",
    "* `Network`\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length of any simulation step and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and deactivate the GUI. \n",
    "\n",
    "**Note** For training purposes, it is highly recommanded to deactivate the GUI in order to avoid global slow down. In such case, one just needs to specify the following: `render=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specifies environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the network. For the environment `WaveAttenuationPOEnv`, these parameters are used to dictate bounds on the accelerations of the autonomous vehicles, as well as the range of ring lengths (and accordingly network densities) the agent is trained on.\n",
    "\n",
    "Finally, it is important to specify here the *horizon* of the experiment, which is the duration of one episode (during which the RL-agent acquire data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "# Define horizon as a variable to ensure consistent use across notebook\n",
    "HORIZON=1000\n",
    "\n",
    "env_params = EnvParams(\n",
    "    # length of one rollout\n",
    "    horizon=HORIZON,\n",
    "\n",
    "    additional_params={\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 1,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 1,\n",
    "        # bounds on the ranges of ring road lengths the autonomous vehicle \n",
    "        # is trained on\n",
    "        \"ring_length\": [220, 270],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initializing a Gym Environment\n",
    "\n",
    "Now, we have to specify our Gym Environment and the algorithm that our RL agents will use. Similar to the network, we choose to use on of Flow's builtin environments, a list of which is provided by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Env', 'AccelEnv', 'LaneChangeAccelEnv', 'LaneChangeAccelPOEnv', 'TrafficLightGridTestEnv', 'MergePOEnv', 'BottleneckEnv', 'BottleneckAccelEnv', 'WaveAttenuationEnv', 'WaveAttenuationPOEnv', 'EnergyOptEnv', 'EnergyOptPOEnv', 'TrafficLightGridEnv', 'TrafficLightGridPOEnv', 'TrafficLightGridBenchmarkEnv', 'BottleneckDesiredVelocityEnv', 'TestEnv', 'BayBridgeEnv', 'SingleStraightRoad', 'BottleNeckAccelEnv', 'DesiredVelocityEnv', 'PO_TrafficLightGridEnv', 'GreenWaveTestEnv']\n"
     ]
    }
   ],
   "source": [
    "import flow.envs as flowenvs\n",
    "\n",
    "print(flowenvs.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the environment \"WaveAttenuationPOEnv\", which is used to train autonomous vehicles to attenuate the formation and propagation of waves in a partially observable variable density ring road. To create the Gym Environment, the only necessary parameters are the environment name plus the previously defined variables. These are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.envs import EnergyOptPOEnv\n",
    "\n",
    "env_name = EnergyOptPOEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from flow.envs import WaveAttenuationPOEnv\n",
    "\n",
    "# env_name = WaveAttenuationPOEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Setting up Flow Parameters\n",
    "\n",
    "RLlib experiments both generate a `params.json` file for each experiment run. For RLlib experiments, the parameters defining the Flow network and environment must be stored as well. As such, in this section we define the dictionary `flow_params`, which contains the variables required by the utility function `make_create_env`. `make_create_env` is a higher-order function which returns a function `create_env` that initializes a Gym environment corresponding to the Flow network specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=network_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # simulation-related parameters\n",
    "    sim=sim_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Running RL experiments in Ray\n",
    "\n",
    "### 4.1 Import \n",
    "\n",
    "First, we must import modules required to run experiments in Ray. The `json` package is required to store the Flow experiment parameters in the `params.json` file, as is `FlowParamsEncoder`. Ray-related imports are required: the PPO algorithm agent, `ray.tune`'s experiment runner, and environment helper methods `register_env` and `make_create_env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "# from ray.rllib.agents.agent import get_agent_class\n",
    "#from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Ray\n",
    "Here, we initialize Ray and experiment-based constant variables specifying parallelism in the experiment as well as experiment batch size in terms of number of rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-23 18:17:23,170\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-07-23_18-17-23_169420_16993/logs.\n",
      "2020-07-23 18:17:23,287\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:41576 to respond...\n",
      "2020-07-23 18:17:23,436\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:17355 to respond...\n",
      "2020-07-23 18:17:23,442\tINFO services.py:809 -- Starting Redis shard with 3.3 GB max memory.\n",
      "2020-07-23 18:17:23,518\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-07-23_18-17-23_169420_16993/logs.\n",
      "2020-07-23 18:17:23,524\tWARNING services.py:1330 -- WARNING: The default object store size of 4.96 GB will use more than 50% of the available memory on this node (8.74 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2020-07-23 18:17:23,527\tINFO services.py:1475 -- Starting the Plasma object store with 4.96 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.100.38',\n",
       " 'redis_address': '192.168.100.38:41576',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-23_18-17-23_169420_16993/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-23_18-17-23_169420_16993/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-07-23_18-17-23_169420_16993'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 4\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "#ray.shutdown()\n",
    "ray.init(num_cpus=N_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configuration and Setup\n",
    "Here, we copy and modify the default configuration for the [PPO algorithm](https://arxiv.org/abs/1707.06347). The agent has the number of parallel workers specified, a batch size corresponding to `N_ROLLOUTS` rollouts (each of which has length `HORIZON` steps), a discount rate $\\gamma$ of 0.999, two hidden layers of size 16, uses Generalized Advantage Estimation, $\\lambda$ of 0.97, and other parameters as set below.\n",
    "\n",
    "Once `config` contains the desired parameters, a JSON string corresponding to the `flow_params` specified in section 3 is generated. The `FlowParamsEncoder` maps objects to string representations so that the experiment can be reproduced later. That string representation is stored within the `env_config` section of the `config` dictionary. Later, `config` is written out to the file `params.json`. \n",
    "\n",
    "Next, we call `make_create_env` and pass in the `flow_params` to return a function we can use to register our Flow environment with Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Running Experiments\n",
    "\n",
    "Here, we use the `run_experiments` function from `ray.tune`. The function takes a dictionary with one key, a name corresponding to the experiment, and one value, itself a dictionary containing parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-23 18:17:23,862\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-07-23 18:17:23,988\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-07-23 18:17:24,012\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-07-23 18:17:24,014\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.8/16.5 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-23 18:17:24,101\tWARNING util.py:145 -- The `start_trial` operation took 0.11872601509094238 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:27,659\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28,824\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.826642: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.863038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.864174: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcff8000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.870600: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.875173: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.875213: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:28.875264: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:29,087\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:30,129\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fd02bf41d90>}\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:30,130\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd02bf41a10>}\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:30,130\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd02c376b10>}\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:30,188\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:33,414\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:34,999\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.002078: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.034957: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.035318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f22b8000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.035370: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.039243: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.039279: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m 2020-07-23 18:17:35.039317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35,025\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.027866: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.044447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.044794: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa91c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.044846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.048610: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.048647: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35.048684: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35,136\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.138735: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.149880: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1999965000 Hz\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.150083: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f65b8000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.150110: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.152503: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.152529: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m 2020-07-23 18:17:35.152552: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (solom-XPS-13-9380): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:35,261\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:36,312\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m WARNING:tensorflow:From /home/solom/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,480\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float64, min=0.0, max=1.333, mean=0.444)}}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,481\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,481\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((3,), dtype=float64, min=0.0, max=1.333, mean=0.444)\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,482\tINFO sampler.py:407 -- Filtered obs: np.ndarray((3,), dtype=float64, min=0.0, max=1.333, mean=0.444)\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,484\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float64, min=0.0, max=1.333, mean=0.444),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,484\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:37,550\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m { 'default_policy': ( np.ndarray((1, 1), dtype=float32, min=-0.281, max=-0.281, mean=-0.281),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.386, max=0.386, mean=0.386),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.006, max=-0.005, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.001, max=0.001, mean=0.001)})}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:38,667\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=0.401, mean=0.295),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'actions': np.ndarray((200, 1), dtype=float32, min=-2.876, max=2.908, mean=0.12),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'advantages': np.ndarray((200,), dtype=float32, min=-0.108, max=0.0, mean=-0.077),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.006, max=-0.005, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'eps_id': np.ndarray((200,), dtype=int64, min=352347578.0, max=352347578.0, mean=352347578.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'new_obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.473, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.473, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'prev_actions': np.ndarray((200, 1), dtype=float32, min=-2.876, max=2.908, mean=0.122),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.007, max=0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'rewards': np.ndarray((200,), dtype=float32, min=-0.007, max=-0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'value_targets': np.ndarray((200,), dtype=float32, min=-0.107, max=0.001, mean=-0.076),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m                         'vf_preds': np.ndarray((200,), dtype=float32, min=0.001, max=0.001, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m 2020-07-23 18:17:38,674\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=0.401, mean=0.295),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'actions': np.ndarray((200, 1), dtype=float32, min=-2.876, max=2.908, mean=0.12),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.108, max=0.0, mean=-0.077),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.006, max=-0.005, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=352347578.0, max=352347578.0, mean=352347578.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'new_obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.473, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'obs': np.ndarray((200, 3), dtype=float32, min=0.0, max=1.473, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'prev_actions': np.ndarray((200, 1), dtype=float32, min=-2.876, max=2.908, mean=0.122),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.007, max=0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.007, max=-0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-0.107, max=0.001, mean=-0.076),\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.001, max=0.001, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,545\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/kernel:0' shape=(3, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,545\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,545\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,545\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/kernel:0' shape=(16, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/kernel:0' shape=(3, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/kernel:0' shape=(16, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,546\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,547\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m { 'inputs': [ np.ndarray((1000, 1), dtype=float32, min=-2.936, max=3.252, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000,), dtype=float32, min=-0.007, max=-0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000, 3), dtype=float32, min=0.0, max=1.482, mean=0.45),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000, 1), dtype=float32, min=-2.936, max=3.252, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000,), dtype=float32, min=-1.907, max=3.44, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000, 2), dtype=float32, min=-0.006, max=-0.004, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000,), dtype=float32, min=-0.107, max=0.001, mean=-0.069),\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m               np.ndarray((1000,), dtype=float32, min=0.001, max=0.001, mean=0.001)],\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:17:39,547\tINFO multi_gpu_impl.py:191 -- Divided 1000 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-17-40\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 435.806\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4178375005722046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4992763428599574e-05\n",
      "        policy_loss: -0.0006492786342278123\n",
      "        total_loss: 0.00418465631082654\n",
      "        vf_explained_var: 0.0005594491958618164\n",
      "        vf_loss: 0.004828924313187599\n",
      "    load_time_ms: 64.083\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "    sample_time_ms: 5238.806\n",
      "    update_time_ms: 832.057\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.369999999999997\n",
      "    ram_util_percent: 53.14\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 6.652189493179321\n",
      "  time_this_iter_s: 6.652189493179321\n",
      "  time_total_s: 6.652189493179321\n",
      "  timestamp: 1595517460\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m /home/solom/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m /home/solom/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 6 s, 1 iter, 1000 ts, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.559589467910891\n",
      "  episode_reward_mean: -2.59874393187739\n",
      "  episode_reward_min: -2.6481059844164805\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 163.263\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4167766571044922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0007202945416793e-05\n",
      "        policy_loss: -0.00025424384512007236\n",
      "        total_loss: 0.004469640552997589\n",
      "        vf_explained_var: 0.0033866167068481445\n",
      "        vf_loss: 0.0047233859077095985\n",
      "    load_time_ms: 22.178\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "    sample_time_ms: 3779.978\n",
      "    update_time_ms: 281.051\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.283333333333335\n",
      "    ram_util_percent: 53.36666666666665\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.248975245665996\n",
      "    mean_inference_ms: 0.9498005504017467\n",
      "    mean_processing_ms: 1.382619906694461\n",
      "  time_since_restore: 12.840100765228271\n",
      "  time_this_iter_s: 3.885529041290283\n",
      "  time_total_s: 12.840100765228271\n",
      "  timestamp: 1595517466\n",
      "  timesteps_since_restore: 3000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 12 s, 3 iter, 3000 ts, -2.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.379719092244184\n",
      "  episode_reward_mean: -2.58663299138639\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 6\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 93.492\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0062500000931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4163509607315063\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4566004210792016e-05\n",
      "        policy_loss: -0.0005302257486619055\n",
      "        total_loss: 0.004219315480440855\n",
      "        vf_explained_var: 0.005437731742858887\n",
      "        vf_loss: 0.004749426618218422\n",
      "    load_time_ms: 11.69\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "    sample_time_ms: 3160.833\n",
      "    update_time_ms: 142.924\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.6\n",
      "    ram_util_percent: 53.279999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 5.022046757967528\n",
      "    mean_inference_ms: 0.9494246381503699\n",
      "    mean_processing_ms: 1.3787062235120138\n",
      "  time_since_restore: 20.574921131134033\n",
      "  time_this_iter_s: 3.197859048843384\n",
      "  time_total_s: 20.574921131134033\n",
      "  timestamp: 1595517474\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 20 s, 6 iter, 6000 ts, -2.59 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.379719092244184\n",
      "  episode_reward_mean: -2.5631394261098546\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 9\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 70.075\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0007812500116415322\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4094527959823608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.179860141244717e-05\n",
      "        policy_loss: -0.0009104652563109994\n",
      "        total_loss: 0.0030051611829549074\n",
      "        vf_explained_var: 0.0007041096687316895\n",
      "        vf_loss: 0.003915615379810333\n",
      "    load_time_ms: 8.171\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "    sample_time_ms: 2998.111\n",
      "    update_time_ms: 96.882\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.849999999999998\n",
      "    ram_util_percent: 53.300000000000004\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.86535072853809\n",
      "    mean_inference_ms: 0.9419195988791194\n",
      "    mean_processing_ms: 1.376029918757733\n",
      "  time_since_restore: 28.7015221118927\n",
      "  time_this_iter_s: 3.7867324352264404\n",
      "  time_total_s: 28.7015221118927\n",
      "  timestamp: 1595517482\n",
      "  timesteps_since_restore: 9000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 28 s, 9 iter, 9000 ts, -2.56 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.3628428581012884\n",
      "  episode_reward_mean: -2.538489268219056\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.193\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.765625145519152e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.405464768409729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7344296793453395e-05\n",
      "        policy_loss: -0.0006767578306607902\n",
      "        total_loss: 0.0031246270518749952\n",
      "        vf_explained_var: 0.027339279651641846\n",
      "        vf_loss: 0.003801394486799836\n",
      "    load_time_ms: 1.199\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    sample_time_ms: 2746.639\n",
      "    update_time_ms: 4.721\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.65\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.792535560445543\n",
      "    mean_inference_ms: 0.9424063091322888\n",
      "    mean_processing_ms: 1.3757853377458291\n",
      "  time_since_restore: 36.799407958984375\n",
      "  time_this_iter_s: 3.2030622959136963\n",
      "  time_total_s: 36.799407958984375\n",
      "  timestamp: 1595517490\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 36 s, 12 iter, 12000 ts, -2.54 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.3628428581012884\n",
      "  episode_reward_mean: -2.518784872364507\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 15\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.220703143189894e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.396195411682129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.790449020219967e-05\n",
      "        policy_loss: -0.0005429148441180587\n",
      "        total_loss: 0.0031007956713438034\n",
      "        vf_explained_var: -0.014307618141174316\n",
      "        vf_loss: 0.0036437201779335737\n",
      "    load_time_ms: 1.214\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "    sample_time_ms: 2669.949\n",
      "    update_time_ms: 4.767\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.1\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.734225571229428\n",
      "    mean_inference_ms: 0.9419247824367265\n",
      "    mean_processing_ms: 1.3764935422665576\n",
      "  time_since_restore: 44.45006346702576\n",
      "  time_this_iter_s: 3.487846851348877\n",
      "  time_total_s: 44.45006346702576\n",
      "  timestamp: 1595517498\n",
      "  timesteps_since_restore: 15000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 44 s, 15 iter, 15000 ts, -2.52 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.200937072489547\n",
      "  episode_reward_mean: -2.4753272897951075\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 18\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.996\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5258789289873675e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3913122415542603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1553743610857055e-05\n",
      "        policy_loss: -0.0004947442794218659\n",
      "        total_loss: 0.0030539536383002996\n",
      "        vf_explained_var: 0.01582878828048706\n",
      "        vf_loss: 0.003548701060935855\n",
      "    load_time_ms: 1.256\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "    sample_time_ms: 2727.396\n",
      "    update_time_ms: 4.767\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.166666666666668\n",
      "    ram_util_percent: 53.45000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.694460556114718\n",
      "    mean_inference_ms: 0.9425934548750795\n",
      "    mean_processing_ms: 1.3787046319874483\n",
      "  time_since_restore: 52.56270408630371\n",
      "  time_this_iter_s: 3.9819185733795166\n",
      "  time_total_s: 52.56270408630371\n",
      "  timestamp: 1595517506\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.9/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 52 s, 18 iter, 18000 ts, -2.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.1293425106893538\n",
      "  episode_reward_mean: -2.4498303555896563\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 21\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.098\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.9073486612342094e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3816004991531372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.297692455817014e-05\n",
      "        policy_loss: -0.0004356751451268792\n",
      "        total_loss: 0.0026545561850070953\n",
      "        vf_explained_var: 0.06901532411575317\n",
      "        vf_loss: 0.003090234939008951\n",
      "    load_time_ms: 1.27\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "    sample_time_ms: 2749.215\n",
      "    update_time_ms: 4.638\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.0\n",
      "    ram_util_percent: 53.53333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.667468152483856\n",
      "    mean_inference_ms: 0.9445457316416792\n",
      "    mean_processing_ms: 1.38006611812507\n",
      "  time_since_restore: 61.46434760093689\n",
      "  time_this_iter_s: 4.419841766357422\n",
      "  time_total_s: 61.46434760093689\n",
      "  timestamp: 1595517515\n",
      "  timesteps_since_restore: 21000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 61 s, 21 iter, 21000 ts, -2.45 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.0926997608494196\n",
      "  episode_reward_mean: -2.4115117555012326\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 24\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.703\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.3841858265427618e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.375921368598938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9417941075516865e-05\n",
      "        policy_loss: -0.000444591511040926\n",
      "        total_loss: 0.00203327857889235\n",
      "        vf_explained_var: 0.11073195934295654\n",
      "        vf_loss: 0.0024778780061751604\n",
      "    load_time_ms: 1.292\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    sample_time_ms: 2800.504\n",
      "    update_time_ms: 4.545\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.7\n",
      "    ram_util_percent: 53.48\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.644538361921481\n",
      "    mean_inference_ms: 0.9458729329856701\n",
      "    mean_processing_ms: 1.381221965497781\n",
      "  time_since_restore: 69.3482301235199\n",
      "  time_this_iter_s: 3.563488006591797\n",
      "  time_total_s: 69.3482301235199\n",
      "  timestamp: 1595517523\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 69 s, 24 iter, 24000 ts, -2.41 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.073989239380445\n",
      "  episode_reward_mean: -2.378139577377459\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 27\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.926\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.9802322831784522e-09\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3728405237197876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.321027907077223e-05\n",
      "        policy_loss: -0.0006228894926607609\n",
      "        total_loss: 0.002709657419472933\n",
      "        vf_explained_var: 0.087222158908844\n",
      "        vf_loss: 0.003332550171762705\n",
      "    load_time_ms: 1.294\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "    sample_time_ms: 2925.518\n",
      "    update_time_ms: 4.799\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.3\n",
      "    ram_util_percent: 53.28333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.627792553417907\n",
      "    mean_inference_ms: 0.9474055883623774\n",
      "    mean_processing_ms: 1.3823891500561079\n",
      "  time_since_restore: 78.22396683692932\n",
      "  time_this_iter_s: 4.332005023956299\n",
      "  time_total_s: 78.22396683692932\n",
      "  timestamp: 1595517531\n",
      "  timesteps_since_restore: 27000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 78 s, 27 iter, 27000 ts, -2.38 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.0314402042156794\n",
      "  episode_reward_mean: -2.3564644899625766\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 30\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.483\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.7252903539730653e-10\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3652536869049072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.679262565332465e-05\n",
      "        policy_loss: -0.0007911710999906063\n",
      "        total_loss: 0.0018408956238999963\n",
      "        vf_explained_var: 0.1761588454246521\n",
      "        vf_loss: 0.0026320586912333965\n",
      "    load_time_ms: 1.26\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    sample_time_ms: 2794.233\n",
      "    update_time_ms: 5.028\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.02\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.608168185601738\n",
      "    mean_inference_ms: 0.9475099110225889\n",
      "    mean_processing_ms: 1.3830546967314041\n",
      "  time_since_restore: 85.37135148048401\n",
      "  time_this_iter_s: 3.162203311920166\n",
      "  time_total_s: 85.37135148048401\n",
      "  timestamp: 1595517539\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 85 s, 30 iter, 30000 ts, -2.36 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.993290569574247\n",
      "  episode_reward_mean: -2.324917909714485\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 33\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.465\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.6566129424663316e-11\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3621950149536133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7908334484673105e-05\n",
      "        policy_loss: -0.00035298443981446326\n",
      "        total_loss: 0.002557059284299612\n",
      "        vf_explained_var: 0.156116783618927\n",
      "        vf_loss: 0.002910060342401266\n",
      "    load_time_ms: 1.265\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "    sample_time_ms: 2713.149\n",
      "    update_time_ms: 5.192\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.9\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.587248360847697\n",
      "    mean_inference_ms: 0.946783470622777\n",
      "    mean_processing_ms: 1.3833119652177874\n",
      "  time_since_restore: 93.30091762542725\n",
      "  time_this_iter_s: 3.8592734336853027\n",
      "  time_total_s: 93.30091762542725\n",
      "  timestamp: 1595517547\n",
      "  timesteps_since_restore: 33000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 93 s, 33 iter, 33000 ts, -2.32 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9531865851314698\n",
      "  episode_reward_mean: -2.3003037368044748\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 36\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.053\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.8207661780829145e-12\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3595019578933716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.020780397695489e-05\n",
      "        policy_loss: -0.0005984497256577015\n",
      "        total_loss: 0.001987155992537737\n",
      "        vf_explained_var: 0.2914762496948242\n",
      "        vf_loss: 0.002585602691397071\n",
      "    load_time_ms: 1.246\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    sample_time_ms: 2667.771\n",
      "    update_time_ms: 5.086\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.159999999999997\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.568982984180952\n",
      "    mean_inference_ms: 0.9460922802311907\n",
      "    mean_processing_ms: 1.3835213113965512\n",
      "  time_since_restore: 100.9457015991211\n",
      "  time_this_iter_s: 3.5154826641082764\n",
      "  time_total_s: 100.9457015991211\n",
      "  timestamp: 1595517554\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 100 s, 36 iter, 36000 ts, -2.3 rew\n",
      "\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9531865851314698\n",
      "  episode_reward_mean: -2.300303736804475\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.603\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.4551915445207286e-12\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3464877605438232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.819555270951241e-05\n",
      "        policy_loss: -0.0022724189329892397\n",
      "        total_loss: -0.000270675664069131\n",
      "        vf_explained_var: 0.302731454372406\n",
      "        vf_loss: 0.0020017565693706274\n",
      "    load_time_ms: 1.178\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "    sample_time_ms: 2551.384\n",
      "    update_time_ms: 4.781\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.76666666666667\n",
      "    ram_util_percent: 53.29999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.568982984180951\n",
      "    mean_inference_ms: 0.9460922802311903\n",
      "    mean_processing_ms: 1.3835213113965512\n",
      "  time_since_restore: 106.15444254875183\n",
      "  time_this_iter_s: 2.215508460998535\n",
      "  time_total_s: 106.15444254875183\n",
      "  timestamp: 1595517559\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 106 s, 38 iter, 38000 ts, -2.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9531865851314698\n",
      "  episode_reward_mean: -2.2760013928509695\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 39\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.597\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.6379788613018216e-13\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3236244916915894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001296087575610727\n",
      "        policy_loss: -0.0023867455311119556\n",
      "        total_loss: -0.0006942481850273907\n",
      "        vf_explained_var: 0.285461962223053\n",
      "        vf_loss: 0.0016925070667639375\n",
      "    load_time_ms: 1.155\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    sample_time_ms: 2611.995\n",
      "    update_time_ms: 5.09\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.674999999999997\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.556864717774531\n",
      "    mean_inference_ms: 0.9463001881056761\n",
      "    mean_processing_ms: 1.3841310176253674\n",
      "  time_since_restore: 111.84716415405273\n",
      "  time_this_iter_s: 2.2518293857574463\n",
      "  time_total_s: 111.84716415405273\n",
      "  timestamp: 1595517565\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 111 s, 40 iter, 40000 ts, -2.28 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2.253148507170355\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.417\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.094947153254554e-14\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3496125936508179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.484708213363774e-05\n",
      "        policy_loss: -0.0006503601325675845\n",
      "        total_loss: 0.002143569989129901\n",
      "        vf_explained_var: 0.3346250653266907\n",
      "        vf_loss: 0.002793920924887061\n",
      "    load_time_ms: 1.197\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "    sample_time_ms: 2729.612\n",
      "    update_time_ms: 4.546\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.525000000000002\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.545208667544874\n",
      "    mean_inference_ms: 0.9462487350881245\n",
      "    mean_processing_ms: 1.3845965256514756\n",
      "  time_since_restore: 117.10004878044128\n",
      "  time_this_iter_s: 3.0023105144500732\n",
      "  time_total_s: 117.10004878044128\n",
      "  timestamp: 1595517570\n",
      "  timesteps_since_restore: 42000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 117 s, 42 iter, 42000 ts, -2.25 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2.233996989499085\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 45\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.028\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1368683941568192e-14\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3454850912094116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.865717710345052e-05\n",
      "        policy_loss: -0.0009596259333193302\n",
      "        total_loss: 0.002514539286494255\n",
      "        vf_explained_var: 0.1880333423614502\n",
      "        vf_loss: 0.0034741712734103203\n",
      "    load_time_ms: 1.243\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "    sample_time_ms: 2655.575\n",
      "    update_time_ms: 4.636\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.974999999999998\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.53313380524589\n",
      "    mean_inference_ms: 0.9459452311558806\n",
      "    mean_processing_ms: 1.384880853586584\n",
      "  time_since_restore: 124.3577024936676\n",
      "  time_this_iter_s: 3.03661847114563\n",
      "  time_total_s: 124.3577024936676\n",
      "  timestamp: 1595517578\n",
      "  timesteps_since_restore: 45000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 124 s, 45 iter, 45000 ts, -2.23 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2.2350869515203473\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 48\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.893\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.421085492696024e-15\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3530704975128174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.227276490884833e-05\n",
      "        policy_loss: -0.0002702331403270364\n",
      "        total_loss: 0.008746691048145294\n",
      "        vf_explained_var: -0.3325235843658447\n",
      "        vf_loss: 0.009016933850944042\n",
      "    load_time_ms: 1.283\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    sample_time_ms: 2567.646\n",
      "    update_time_ms: 4.865\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.916666666666668\n",
      "    ram_util_percent: 53.300000000000004\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.522631312956234\n",
      "    mean_inference_ms: 0.945754618923706\n",
      "    mean_processing_ms: 1.3851439753771586\n",
      "  time_since_restore: 132.21714282035828\n",
      "  time_this_iter_s: 3.8486366271972656\n",
      "  time_total_s: 132.21714282035828\n",
      "  timestamp: 1595517586\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 132 s, 48 iter, 48000 ts, -2.24 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2.2347810540794906\n",
      "  episode_reward_min: -2.7812310064886363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 51\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.374\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.77635686587003e-16\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3477741479873657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.5106370230787434e-05\n",
      "        policy_loss: -4.908847768092528e-05\n",
      "        total_loss: 0.007831107825040817\n",
      "        vf_explained_var: -0.3938990831375122\n",
      "        vf_loss: 0.007880202494561672\n",
      "    load_time_ms: 1.314\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "    sample_time_ms: 2524.479\n",
      "    update_time_ms: 4.925\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.48\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.512615606570973\n",
      "    mean_inference_ms: 0.9455493336868354\n",
      "    mean_processing_ms: 1.3853581586658663\n",
      "  time_since_restore: 139.7350845336914\n",
      "  time_this_iter_s: 3.2271389961242676\n",
      "  time_total_s: 139.7350845336914\n",
      "  timestamp: 1595517593\n",
      "  timesteps_since_restore: 51000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 139 s, 51 iter, 51000 ts, -2.23 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2.251049841206979\n",
      "  episode_reward_min: -2.809899009878476\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 54\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.537\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.2204460823375376e-17\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3480870723724365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1168182027176954e-05\n",
      "        policy_loss: -8.788299601292238e-05\n",
      "        total_loss: 0.013264545239508152\n",
      "        vf_explained_var: -0.369767427444458\n",
      "        vf_loss: 0.013352428562939167\n",
      "    load_time_ms: 1.377\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "    sample_time_ms: 2648.672\n",
      "    update_time_ms: 8.239\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.95\n",
      "    ram_util_percent: 53.28333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.503424040224232\n",
      "    mean_inference_ms: 0.9453377997736467\n",
      "    mean_processing_ms: 1.3855244742934132\n",
      "  time_since_restore: 148.26126790046692\n",
      "  time_this_iter_s: 3.9618353843688965\n",
      "  time_total_s: 148.26126790046692\n",
      "  timestamp: 1595517602\n",
      "  timesteps_since_restore: 54000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 148 s, 54 iter, 54000 ts, -2.25 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -157.29696407551444\n",
      "  episode_reward_min: -8836.99777249184\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 57\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.366\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.775557602921922e-18\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3548749685287476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.084918823034968e-06\n",
      "        policy_loss: 0.00014093970821704715\n",
      "        total_loss: 1019529.375\n",
      "        vf_explained_var: -1.8715858459472656e-05\n",
      "        vf_loss: 1019529.0625\n",
      "    load_time_ms: 1.324\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "    sample_time_ms: 2716.702\n",
      "    update_time_ms: 8.228\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.71999999999999\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.4950776527113785\n",
      "    mean_inference_ms: 0.9451310679198044\n",
      "    mean_processing_ms: 1.3856545543748993\n",
      "  time_since_restore: 155.97295331954956\n",
      "  time_this_iter_s: 3.501558780670166\n",
      "  time_total_s: 155.97295331954956\n",
      "  timestamp: 1595517609\n",
      "  timesteps_since_restore: 57000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 155 s, 57 iter, 57000 ts, -157 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -759.9252362363129\n",
      "  episode_reward_min: -17063.319642761962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 60\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.564\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.4694470036524025e-19\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.356276273727417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1600723698611546e-07\n",
      "        policy_loss: -6.570243567693979e-05\n",
      "        total_loss: 3307978.75\n",
      "        vf_explained_var: -2.944469451904297e-05\n",
      "        vf_loss: 3307978.75\n",
      "    load_time_ms: 1.279\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    sample_time_ms: 2760.374\n",
      "    update_time_ms: 7.958\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.8\n",
      "    ram_util_percent: 53.300000000000004\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.487609812682034\n",
      "    mean_inference_ms: 0.9449559940648221\n",
      "    mean_processing_ms: 1.3856992170867117\n",
      "  time_since_restore: 164.53590059280396\n",
      "  time_this_iter_s: 3.861011505126953\n",
      "  time_total_s: 164.53590059280396\n",
      "  timestamp: 1595517618\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 164 s, 60 iter, 60000 ts, -760 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -974.4451540394409\n",
      "  episode_reward_min: -17063.319642761962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 63\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.008\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.336808754565503e-20\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3549503087997437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0014176723416313e-06\n",
      "        policy_loss: 4.808807352674194e-05\n",
      "        total_loss: 2524134.5\n",
      "        vf_explained_var: -1.7762184143066406e-05\n",
      "        vf_loss: 2524134.5\n",
      "    load_time_ms: 1.248\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "    sample_time_ms: 2717.833\n",
      "    update_time_ms: 4.854\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.475\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.4801532428777\n",
      "    mean_inference_ms: 0.9446707738299458\n",
      "    mean_processing_ms: 1.385714470359468\n",
      "  time_since_restore: 171.85252571105957\n",
      "  time_this_iter_s: 3.0872304439544678\n",
      "  time_total_s: 171.85252571105957\n",
      "  timestamp: 1595517625\n",
      "  timesteps_since_restore: 63000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 171 s, 63 iter, 63000 ts, -974 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -1399.0190488325122\n",
      "  episode_reward_min: -17063.319642761962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 66\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.395\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.421010943206879e-21\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3663159608840942\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.409717687143711e-07\n",
      "        policy_loss: 5.870342283742502e-05\n",
      "        total_loss: 2642745.25\n",
      "        vf_explained_var: -1.430511474609375e-05\n",
      "        vf_loss: 2642745.25\n",
      "    load_time_ms: 1.249\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "    sample_time_ms: 2697.494\n",
      "    update_time_ms: 4.88\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.980000000000004\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.473657420940256\n",
      "    mean_inference_ms: 0.9445035873691336\n",
      "    mean_processing_ms: 1.3858188071514703\n",
      "  time_since_restore: 179.82549667358398\n",
      "  time_this_iter_s: 3.5184743404388428\n",
      "  time_total_s: 179.82549667358398\n",
      "  timestamp: 1595517633\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 179 s, 66 iter, 66000 ts, -1.4e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -1859.779423661346\n",
      "  episode_reward_min: -25251.147091211416\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 69\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.594\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 6.776263679008599e-22\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3592431545257568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.14276972960215e-06\n",
      "        policy_loss: 0.00010577726061455905\n",
      "        total_loss: 3485913.25\n",
      "        vf_explained_var: -1.9788742065429688e-05\n",
      "        vf_loss: 3485913.0\n",
      "    load_time_ms: 1.297\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "    sample_time_ms: 2627.204\n",
      "    update_time_ms: 4.793\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.119999999999997\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.467384381438154\n",
      "    mean_inference_ms: 0.9442861023460788\n",
      "    mean_processing_ms: 1.3858908557213543\n",
      "  time_since_restore: 187.34138798713684\n",
      "  time_this_iter_s: 3.2417235374450684\n",
      "  time_total_s: 187.34138798713684\n",
      "  timestamp: 1595517641\n",
      "  timesteps_since_restore: 69000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 187 s, 69 iter, 69000 ts, -1.86e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -2860.4056459844037\n",
      "  episode_reward_min: -35338.292431101836\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 72\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.295\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 8.470329598760748e-23\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3557274341583252\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.141066819509433e-07\n",
      "        policy_loss: 2.989292079291772e-05\n",
      "        total_loss: 12981328.0\n",
      "        vf_explained_var: -1.609325408935547e-05\n",
      "        vf_loss: 12981328.0\n",
      "    load_time_ms: 1.316\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    sample_time_ms: 2648.619\n",
      "    update_time_ms: 5.003\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.316666666666666\n",
      "    ram_util_percent: 53.31666666666666\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.4622600624775846\n",
      "    mean_inference_ms: 0.9442355503098558\n",
      "    mean_processing_ms: 1.3859844286530252\n",
      "  time_since_restore: 195.65852451324463\n",
      "  time_this_iter_s: 3.537639617919922\n",
      "  time_total_s: 195.65852451324463\n",
      "  timestamp: 1595517649\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 195 s, 72 iter, 72000 ts, -2.86e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:20:49,801\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 286.0x the scale of `vf_clip_param`. This means that it will take more than 286.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:20:52,638\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 286.0x the scale of `vf_clip_param`. This means that it will take more than 286.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:20:54,671\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 286.0x the scale of `vf_clip_param`. This means that it will take more than 286.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-20-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -3402.498231574002\n",
      "  episode_reward_min: -36600.106556821855\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 75\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.844\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0587911998450935e-23\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3561689853668213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6610493364387366e-07\n",
      "        policy_loss: 5.740833148593083e-05\n",
      "        total_loss: 7899234.5\n",
      "        vf_explained_var: -1.1563301086425781e-05\n",
      "        vf_loss: 7899234.5\n",
      "    load_time_ms: 1.295\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "    sample_time_ms: 2697.293\n",
      "    update_time_ms: 4.823\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.28\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.4579501096000955\n",
      "    mean_inference_ms: 0.9442789964284882\n",
      "    mean_processing_ms: 1.3860676249917823\n",
      "  time_since_restore: 203.68002033233643\n",
      "  time_this_iter_s: 3.1744160652160645\n",
      "  time_total_s: 203.68002033233643\n",
      "  timestamp: 1595517657\n",
      "  timesteps_since_restore: 75000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 203 s, 75 iter, 75000 ts, -3.4e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:20:57,850\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 340.0x the scale of `vf_clip_param`. This means that it will take more than 340.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:00,356\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 340.0x the scale of `vf_clip_param`. This means that it will take more than 340.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:02,627\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 340.0x the scale of `vf_clip_param`. This means that it will take more than 340.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -4501.990354205035\n",
      "  episode_reward_min: -36600.106556821855\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 78\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.323488999806367e-24\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3626883029937744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1976232744691515e-07\n",
      "        policy_loss: -0.0001131763492594473\n",
      "        total_loss: 11067283.0\n",
      "        vf_explained_var: -7.3909759521484375e-06\n",
      "        vf_loss: 11067282.0\n",
      "    load_time_ms: 1.291\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "    sample_time_ms: 2704.726\n",
      "    update_time_ms: 4.902\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.425\n",
      "    ram_util_percent: 53.325\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.454099588950631\n",
      "    mean_inference_ms: 0.944389815401703\n",
      "    mean_processing_ms: 1.3861685869680995\n",
      "  time_since_restore: 211.5472288131714\n",
      "  time_this_iter_s: 3.111065626144409\n",
      "  time_total_s: 211.5472288131714\n",
      "  timestamp: 1595517665\n",
      "  timesteps_since_restore: 78000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 211 s, 78 iter, 78000 ts, -4.5e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:05,743\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 450.0x the scale of `vf_clip_param`. This means that it will take more than 450.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:07,656\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 450.0x the scale of `vf_clip_param`. This means that it will take more than 450.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:09,669\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 450.0x the scale of `vf_clip_param`. This means that it will take more than 450.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -5418.755682573017\n",
      "  episode_reward_min: -36600.106556821855\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 81\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.261\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.6543612497579586e-25\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.36110258102417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.136728304933058e-07\n",
      "        policy_loss: -0.00013169669546186924\n",
      "        total_loss: 10036577.0\n",
      "        vf_explained_var: -9.298324584960938e-06\n",
      "        vf_loss: 10036577.0\n",
      "    load_time_ms: 1.23\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "    sample_time_ms: 2646.521\n",
      "    update_time_ms: 4.715\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.5\n",
      "    ram_util_percent: 53.379999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.4501756134585095\n",
      "    mean_inference_ms: 0.9444243535748029\n",
      "    mean_processing_ms: 1.3862618733774092\n",
      "  time_since_restore: 218.96600675582886\n",
      "  time_this_iter_s: 3.5124356746673584\n",
      "  time_total_s: 218.96600675582886\n",
      "  timestamp: 1595517673\n",
      "  timesteps_since_restore: 81000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 218 s, 81 iter, 81000 ts, -5.42e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:13,192\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 542.0x the scale of `vf_clip_param`. This means that it will take more than 542.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:15,628\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 542.0x the scale of `vf_clip_param`. This means that it will take more than 542.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:17,747\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 542.0x the scale of `vf_clip_param`. This means that it will take more than 542.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-20\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -6585.690481153411\n",
      "  episode_reward_min: -51117.029107478906\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 84\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.0679515621974483e-26\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3611503839492798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.177370563709701e-06\n",
      "        policy_loss: -0.00020933938503731042\n",
      "        total_loss: 11354262.0\n",
      "        vf_explained_var: -8.58306884765625e-06\n",
      "        vf_loss: 11354262.0\n",
      "    load_time_ms: 1.225\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    sample_time_ms: 2584.888\n",
      "    update_time_ms: 4.659\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.679999999999996\n",
      "    ram_util_percent: 53.31999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.446485632149616\n",
      "    mean_inference_ms: 0.9444760605318405\n",
      "    mean_processing_ms: 1.3863842406132512\n",
      "  time_since_restore: 226.7333471775055\n",
      "  time_this_iter_s: 3.2336227893829346\n",
      "  time_total_s: 226.7333471775055\n",
      "  timestamp: 1595517680\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 226 s, 84 iter, 84000 ts, -6.59e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:20,985\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 659.0x the scale of `vf_clip_param`. This means that it will take more than 659.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:23,050\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 659.0x the scale of `vf_clip_param`. This means that it will take more than 659.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:25,185\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 659.0x the scale of `vf_clip_param`. This means that it will take more than 659.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -7497.848917845529\n",
      "  episode_reward_min: -51117.029107478906\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 87\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.469\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.5849394527468104e-27\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.362385869026184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1167526281496976e-06\n",
      "        policy_loss: -0.0002299637853866443\n",
      "        total_loss: 8110918.5\n",
      "        vf_explained_var: -6.9141387939453125e-06\n",
      "        vf_loss: 8110918.5\n",
      "    load_time_ms: 1.236\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "    sample_time_ms: 2526.37\n",
      "    update_time_ms: 4.816\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.56\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.442716937605805\n",
      "    mean_inference_ms: 0.9444604347052841\n",
      "    mean_processing_ms: 1.3864743571027958\n",
      "  time_since_restore: 234.08347010612488\n",
      "  time_this_iter_s: 3.171299695968628\n",
      "  time_total_s: 234.08347010612488\n",
      "  timestamp: 1595517688\n",
      "  timesteps_since_restore: 87000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 234 s, 87 iter, 87000 ts, -7.5e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:28,361\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 750.0x the scale of `vf_clip_param`. This means that it will take more than 750.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:30,416\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 750.0x the scale of `vf_clip_param`. This means that it will take more than 750.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:32,736\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 750.0x the scale of `vf_clip_param`. This means that it will take more than 750.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -8602.786954425012\n",
      "  episode_reward_min: -66279.83895356947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 90\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.554\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.231174315933513e-28\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3617253303527832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.313822033618635e-07\n",
      "        policy_loss: -0.00021597814338747412\n",
      "        total_loss: 14241528.0\n",
      "        vf_explained_var: -5.0067901611328125e-06\n",
      "        vf_loss: 14241528.0\n",
      "    load_time_ms: 1.249\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    sample_time_ms: 2601.714\n",
      "    update_time_ms: 4.85\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.68\n",
      "    ram_util_percent: 53.339999999999996\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.439267518159627\n",
      "    mean_inference_ms: 0.9444555034741732\n",
      "    mean_processing_ms: 1.3865575674769461\n",
      "  time_since_restore: 241.8551585674286\n",
      "  time_this_iter_s: 3.417722225189209\n",
      "  time_total_s: 241.8551585674286\n",
      "  timestamp: 1595517696\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 241 s, 90 iter, 90000 ts, -8.6e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:36,158\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 860.0x the scale of `vf_clip_param`. This means that it will take more than 860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:38,755\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 860.0x the scale of `vf_clip_param`. This means that it will take more than 860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:40,640\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 860.0x the scale of `vf_clip_param`. This means that it will take more than 860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -9893.673782608006\n",
      "  episode_reward_min: -71316.28269522227\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 93\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.699\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 4.038967894916891e-29\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3555971384048462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0169148936256533e-06\n",
      "        policy_loss: -0.00023697089636698365\n",
      "        total_loss: 16035525.0\n",
      "        vf_explained_var: -1.8835067749023438e-05\n",
      "        vf_loss: 16035525.0\n",
      "    load_time_ms: 1.266\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "    sample_time_ms: 2660.185\n",
      "    update_time_ms: 4.838\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.95\n",
      "    ram_util_percent: 53.35\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.436056762380611\n",
      "    mean_inference_ms: 0.9444559315773875\n",
      "    mean_processing_ms: 1.386654100035644\n",
      "  time_since_restore: 250.48794221878052\n",
      "  time_this_iter_s: 4.172050476074219\n",
      "  time_total_s: 250.48794221878052\n",
      "  timestamp: 1595517704\n",
      "  timesteps_since_restore: 93000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 250 s, 93 iter, 93000 ts, -9.89e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:44,817\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 989.0x the scale of `vf_clip_param`. This means that it will take more than 989.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:47,571\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 989.0x the scale of `vf_clip_param`. This means that it will take more than 989.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:49,697\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 989.0x the scale of `vf_clip_param`. This means that it will take more than 989.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -10880.038554039706\n",
      "  episode_reward_min: -71316.28269522227\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 96\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.815\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 5.048709868646114e-30\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3588064908981323\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.974339612497715e-07\n",
      "        policy_loss: -0.00019774341490119696\n",
      "        total_loss: 15050316.0\n",
      "        vf_explained_var: -8.106231689453125e-06\n",
      "        vf_loss: 15050316.0\n",
      "    load_time_ms: 1.293\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    sample_time_ms: 2752.276\n",
      "    update_time_ms: 4.77\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.26\n",
      "    ram_util_percent: 53.36\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.433501215619267\n",
      "    mean_inference_ms: 0.9445633590729798\n",
      "    mean_processing_ms: 1.386779987503984\n",
      "  time_since_restore: 258.82158398628235\n",
      "  time_this_iter_s: 3.4752278327941895\n",
      "  time_total_s: 258.82158398628235\n",
      "  timestamp: 1595517713\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 258 s, 96 iter, 96000 ts, -1.09e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:53,177\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1088.0x the scale of `vf_clip_param`. This means that it will take more than 1088.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:55,185\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1088.0x the scale of `vf_clip_param`. This means that it will take more than 1088.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:21:57,659\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1088.0x the scale of `vf_clip_param`. This means that it will take more than 1088.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -12068.6191867606\n",
      "  episode_reward_min: -71316.28269522227\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.787\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 6.3108873358076425e-31\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3610119819641113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.068778818895225e-07\n",
      "        policy_loss: -0.0001689109776634723\n",
      "        total_loss: 19316322.0\n",
      "        vf_explained_var: -4.172325134277344e-06\n",
      "        vf_loss: 19316322.0\n",
      "    load_time_ms: 1.257\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "    sample_time_ms: 2798.466\n",
      "    update_time_ms: 4.69\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.2\n",
      "    ram_util_percent: 53.379999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.431217814434323\n",
      "    mean_inference_ms: 0.9446906392835256\n",
      "    mean_processing_ms: 1.3869065399469838\n",
      "  time_since_restore: 266.8076663017273\n",
      "  time_this_iter_s: 3.5253961086273193\n",
      "  time_total_s: 266.8076663017273\n",
      "  timestamp: 1595517721\n",
      "  timesteps_since_restore: 99000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 266 s, 99 iter, 99000 ts, -1.21e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:01,188\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1207.0x the scale of `vf_clip_param`. This means that it will take more than 1207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:03,470\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1207.0x the scale of `vf_clip_param`. This means that it will take more than 1207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:05,916\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1207.0x the scale of `vf_clip_param`. This means that it will take more than 1207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -13198.074979214522\n",
      "  episode_reward_min: -71316.28269522227\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 102\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.042\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 7.888609169759553e-32\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3598777055740356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.320349267392885e-06\n",
      "        policy_loss: -0.0003779068065341562\n",
      "        total_loss: 9656015.0\n",
      "        vf_explained_var: -5.4836273193359375e-06\n",
      "        vf_loss: 9656014.0\n",
      "    load_time_ms: 1.269\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "    sample_time_ms: 2799.397\n",
      "    update_time_ms: 4.804\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.7\n",
      "    ram_util_percent: 53.35999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.41138024655516\n",
      "    mean_inference_ms: 0.9442610654147723\n",
      "    mean_processing_ms: 1.3868545319954435\n",
      "  time_since_restore: 274.70054936408997\n",
      "  time_this_iter_s: 3.1941230297088623\n",
      "  time_total_s: 274.70054936408997\n",
      "  timestamp: 1595517729\n",
      "  timesteps_since_restore: 102000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 274 s, 102 iter, 102000 ts, -1.32e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:09,114\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1320.0x the scale of `vf_clip_param`. This means that it will take more than 1320.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:11,626\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1320.0x the scale of `vf_clip_param`. This means that it will take more than 1320.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:13,604\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1320.0x the scale of `vf_clip_param`. This means that it will take more than 1320.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -14044.024211909378\n",
      "  episode_reward_min: -71316.28269522227\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 105\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.111\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.860761462199441e-33\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3495745658874512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1328201544056355e-07\n",
      "        policy_loss: -8.823156531434506e-05\n",
      "        total_loss: 2836417.25\n",
      "        vf_explained_var: -3.2067298889160156e-05\n",
      "        vf_loss: 2836417.0\n",
      "    load_time_ms: 1.289\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "    sample_time_ms: 2730.291\n",
      "    update_time_ms: 4.682\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.36666666666667\n",
      "    ram_util_percent: 53.383333333333326\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.3942322367028375\n",
      "    mean_inference_ms: 0.9444232370992535\n",
      "    mean_processing_ms: 1.3873390520993423\n",
      "  time_since_restore: 283.04289078712463\n",
      "  time_this_iter_s: 3.8742523193359375\n",
      "  time_total_s: 283.04289078712463\n",
      "  timestamp: 1595517737\n",
      "  timesteps_since_restore: 105000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:17,483\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1404.0x the scale of `vf_clip_param`. This means that it will take more than 1404.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 283 s, 105 iter, 105000 ts, -1.4e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:19,843\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1404.0x the scale of `vf_clip_param`. This means that it will take more than 1404.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:22,007\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1404.0x the scale of `vf_clip_param`. This means that it will take more than 1404.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:25,182\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1579.0x the scale of `vf_clip_param`. This means that it will take more than 1579.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -15786.9792583173\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 108\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.573\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.2325951827749302e-33\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3596272468566895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3865828805137426e-06\n",
      "        policy_loss: -0.0002708864340092987\n",
      "        total_loss: 18957362.0\n",
      "        vf_explained_var: -6.079673767089844e-06\n",
      "        vf_loss: 18957364.0\n",
      "    load_time_ms: 1.253\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    sample_time_ms: 2704.626\n",
      "    update_time_ms: 4.739\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.759999999999998\n",
      "    ram_util_percent: 53.42\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.38598660247811\n",
      "    mean_inference_ms: 0.9448907337931264\n",
      "    mean_processing_ms: 1.3879171674145434\n",
      "  time_since_restore: 290.71477484703064\n",
      "  time_this_iter_s: 3.170588970184326\n",
      "  time_total_s: 290.71477484703064\n",
      "  timestamp: 1595517745\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 290 s, 108 iter, 108000 ts, -1.58e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:27,846\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1579.0x the scale of `vf_clip_param`. This means that it will take more than 1579.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:29,926\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1579.0x the scale of `vf_clip_param`. This means that it will take more than 1579.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -17119.184463006783\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 111\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.593\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5407439784686627e-34\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3598076105117798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.758813823471428e-06\n",
      "        policy_loss: -0.00031004715128801763\n",
      "        total_loss: 9870117.0\n",
      "        vf_explained_var: -6.67572021484375e-06\n",
      "        vf_loss: 9870118.0\n",
      "    load_time_ms: 1.317\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "    sample_time_ms: 2743.165\n",
      "    update_time_ms: 4.559\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.183333333333337\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.380129410664946\n",
      "    mean_inference_ms: 0.9453531191888697\n",
      "    mean_processing_ms: 1.388457979852996\n",
      "  time_since_restore: 299.33821415901184\n",
      "  time_this_iter_s: 3.902190685272217\n",
      "  time_total_s: 299.33821415901184\n",
      "  timestamp: 1595517753\n",
      "  timesteps_since_restore: 111000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 299 s, 111 iter, 111000 ts, -1.71e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:33,832\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1712.0x the scale of `vf_clip_param`. This means that it will take more than 1712.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:36,786\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1712.0x the scale of `vf_clip_param`. This means that it will take more than 1712.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -17119.18446300678\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 111\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.747\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.851859946171657e-35\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.317049264907837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.697501481132349e-06\n",
      "        policy_loss: 3.307914812467061e-05\n",
      "        total_loss: 2787387.0\n",
      "        vf_explained_var: -7.081031799316406e-05\n",
      "        vf_loss: 2787387.0\n",
      "    load_time_ms: 1.265\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "    sample_time_ms: 2753.583\n",
      "    update_time_ms: 4.789\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.975\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.380129410664946\n",
      "    mean_inference_ms: 0.9453531191888698\n",
      "    mean_processing_ms: 1.3884579798529955\n",
      "  time_since_restore: 305.118337392807\n",
      "  time_this_iter_s: 2.8484604358673096\n",
      "  time_total_s: 305.118337392807\n",
      "  timestamp: 1595517759\n",
      "  timesteps_since_restore: 113000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 305 s, 113 iter, 113000 ts, -1.71e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:39,645\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1712.0x the scale of `vf_clip_param`. This means that it will take more than 1712.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:43,647\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1860.0x the scale of `vf_clip_param`. This means that it will take more than 1860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -18596.956685310237\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 114\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.52\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 9.629649865429142e-36\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1867026090621948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7962634956347756e-05\n",
      "        policy_loss: -0.0007151756435632706\n",
      "        total_loss: 0.0013314018724486232\n",
      "        vf_explained_var: 0.9760705828666687\n",
      "        vf_loss: 0.0020465736743062735\n",
      "    load_time_ms: 1.246\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "    sample_time_ms: 2815.948\n",
      "    update_time_ms: 5.066\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.23333333333333\n",
      "    ram_util_percent: 53.43333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.376373842921708\n",
      "    mean_inference_ms: 0.9459177884863592\n",
      "    mean_processing_ms: 1.3889388911348812\n",
      "  time_since_restore: 311.5881817340851\n",
      "  time_this_iter_s: 2.477954149246216\n",
      "  time_total_s: 311.5881817340851\n",
      "  timestamp: 1595517766\n",
      "  timesteps_since_restore: 115000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 311 s, 115 iter, 115000 ts, -1.86e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:46,132\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1860.0x the scale of `vf_clip_param`. This means that it will take more than 1860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:48,453\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1860.0x the scale of `vf_clip_param`. This means that it will take more than 1860.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -19632.36775511023\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 117\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.971\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.4074124663572855e-36\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3580584526062012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.42925978252606e-06\n",
      "        policy_loss: -0.00019736861577257514\n",
      "        total_loss: 5628546.0\n",
      "        vf_explained_var: -1.633167266845703e-05\n",
      "        vf_loss: 5628545.5\n",
      "    load_time_ms: 1.272\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "    sample_time_ms: 2941.992\n",
      "    update_time_ms: 4.682\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.28\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.374174038769939\n",
      "    mean_inference_ms: 0.9465901625078694\n",
      "    mean_processing_ms: 1.3891951706990582\n",
      "  time_since_restore: 317.36686635017395\n",
      "  time_this_iter_s: 3.467529535293579\n",
      "  time_total_s: 317.36686635017395\n",
      "  timestamp: 1595517771\n",
      "  timesteps_since_restore: 117000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 317 s, 117 iter, 117000 ts, -1.96e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:51,925\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1963.0x the scale of `vf_clip_param`. This means that it will take more than 1963.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:54,242\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1963.0x the scale of `vf_clip_param`. This means that it will take more than 1963.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:56,365\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1963.0x the scale of `vf_clip_param`. This means that it will take more than 1963.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -21361.917056781054\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 26.202\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.009265582946607e-37\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3605947494506836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.99168925973936e-06\n",
      "        policy_loss: -0.0003378539113327861\n",
      "        total_loss: 15131324.0\n",
      "        vf_explained_var: -6.556510925292969e-06\n",
      "        vf_loss: 15131324.0\n",
      "    load_time_ms: 1.304\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    sample_time_ms: 2938.371\n",
      "    update_time_ms: 4.794\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.660000000000004\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.371591539356466\n",
      "    mean_inference_ms: 0.9469565876534851\n",
      "    mean_processing_ms: 1.3893946911707689\n",
      "  time_since_restore: 325.22714018821716\n",
      "  time_this_iter_s: 3.441302537918091\n",
      "  time_total_s: 325.22714018821716\n",
      "  timestamp: 1595517779\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 325 s, 120 iter, 120000 ts, -2.14e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:22:59,811\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2136.0x the scale of `vf_clip_param`. This means that it will take more than 2136.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:02,234\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2136.0x the scale of `vf_clip_param`. This means that it will take more than 2136.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:04,308\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2136.0x the scale of `vf_clip_param`. This means that it will take more than 2136.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -22435.424683265217\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.572\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.7615819786832586e-38\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3590537309646606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7631053879085812e-06\n",
      "        policy_loss: -0.00023891449382063001\n",
      "        total_loss: 9913006.0\n",
      "        vf_explained_var: -7.748603820800781e-06\n",
      "        vf_loss: 9913006.0\n",
      "    load_time_ms: 1.207\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "    sample_time_ms: 2749.891\n",
      "    update_time_ms: 4.618\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.18\n",
      "    ram_util_percent: 53.239999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.368849267493676\n",
      "    mean_inference_ms: 0.9471225181051675\n",
      "    mean_processing_ms: 1.3895529652896141\n",
      "  time_since_restore: 332.9869248867035\n",
      "  time_this_iter_s: 3.2947731018066406\n",
      "  time_total_s: 332.9869248867035\n",
      "  timestamp: 1595517787\n",
      "  timesteps_since_restore: 123000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 332 s, 123 iter, 123000 ts, -2.24e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:07,607\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2244.0x the scale of `vf_clip_param`. This means that it will take more than 2244.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:09,841\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2244.0x the scale of `vf_clip_param`. This means that it will take more than 2244.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:11,737\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2244.0x the scale of `vf_clip_param`. This means that it will take more than 2244.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -23937.7793425324\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 126\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.311\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3603966236114502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9707083538378356e-06\n",
      "        policy_loss: -0.0002723197976592928\n",
      "        total_loss: 14468628.0\n",
      "        vf_explained_var: -7.62939453125e-06\n",
      "        vf_loss: 14468628.0\n",
      "    load_time_ms: 1.22\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "    sample_time_ms: 2674.122\n",
      "    update_time_ms: 4.859\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.649999999999995\n",
      "    ram_util_percent: 53.28333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.3665211233361045\n",
      "    mean_inference_ms: 0.9473018463159998\n",
      "    mean_processing_ms: 1.3896482211869978\n",
      "  time_since_restore: 341.02254247665405\n",
      "  time_this_iter_s: 3.9256246089935303\n",
      "  time_total_s: 341.02254247665405\n",
      "  timestamp: 1595517795\n",
      "  timesteps_since_restore: 126000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 341 s, 126 iter, 126000 ts, -2.39e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:15,668\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2394.0x the scale of `vf_clip_param`. This means that it will take more than 2394.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:17,921\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2394.0x the scale of `vf_clip_param`. This means that it will take more than 2394.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:20,058\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2394.0x the scale of `vf_clip_param`. This means that it will take more than 2394.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -25478.581807981347\n",
      "  episode_reward_min: -83938.92479503428\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 129\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.746\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3617721796035767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.9724239993811352e-06\n",
      "        policy_loss: -0.00033951617660932243\n",
      "        total_loss: 12846639.0\n",
      "        vf_explained_var: -6.318092346191406e-06\n",
      "        vf_loss: 12846639.0\n",
      "    load_time_ms: 1.239\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "    sample_time_ms: 2665.735\n",
      "    update_time_ms: 5.004\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.440000000000005\n",
      "    ram_util_percent: 53.25999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.365276740166486\n",
      "    mean_inference_ms: 0.9476400030938836\n",
      "    mean_processing_ms: 1.389772612695061\n",
      "  time_since_restore: 348.82060527801514\n",
      "  time_this_iter_s: 3.431246280670166\n",
      "  time_total_s: 348.82060527801514\n",
      "  timestamp: 1595517803\n",
      "  timesteps_since_restore: 129000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:23,493\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2548.0x the scale of `vf_clip_param`. This means that it will take more than 2548.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 348 s, 129 iter, 129000 ts, -2.55e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:26,241\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2548.0x the scale of `vf_clip_param`. This means that it will take more than 2548.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:28,170\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2548.0x the scale of `vf_clip_param`. This means that it will take more than 2548.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -27329.54489471196\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 132\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.693\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.358754277229309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.979576493089553e-06\n",
      "        policy_loss: -0.00026898097712546587\n",
      "        total_loss: 14853820.0\n",
      "        vf_explained_var: -1.4662742614746094e-05\n",
      "        vf_loss: 14853820.0\n",
      "    load_time_ms: 1.236\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    sample_time_ms: 2745.345\n",
      "    update_time_ms: 5.128\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.233333333333334\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.365738975897197\n",
      "    mean_inference_ms: 0.9482637087441749\n",
      "    mean_processing_ms: 1.3899861756944778\n",
      "  time_since_restore: 357.52447986602783\n",
      "  time_this_iter_s: 4.045692443847656\n",
      "  time_total_s: 357.52447986602783\n",
      "  timestamp: 1595517812\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 357 s, 132 iter, 132000 ts, -2.73e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:32,221\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2733.0x the scale of `vf_clip_param`. This means that it will take more than 2733.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:34,753\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2733.0x the scale of `vf_clip_param`. This means that it will take more than 2733.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:36,788\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2733.0x the scale of `vf_clip_param`. This means that it will take more than 2733.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -28838.254172890982\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 135\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.719\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.36360502243042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6370287287136307e-06\n",
      "        policy_loss: -0.00029801655909977853\n",
      "        total_loss: 15253509.0\n",
      "        vf_explained_var: -8.821487426757812e-06\n",
      "        vf_loss: 15253509.0\n",
      "    load_time_ms: 1.246\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "    sample_time_ms: 2803.313\n",
      "    update_time_ms: 5.15\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.060000000000002\n",
      "    ram_util_percent: 53.279999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.367054345257999\n",
      "    mean_inference_ms: 0.9490212374287538\n",
      "    mean_processing_ms: 1.3902403422701672\n",
      "  time_since_restore: 365.50884318351746\n",
      "  time_this_iter_s: 3.436769723892212\n",
      "  time_total_s: 365.50884318351746\n",
      "  timestamp: 1595517820\n",
      "  timesteps_since_restore: 135000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 365 s, 135 iter, 135000 ts, -2.88e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:40,229\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2884.0x the scale of `vf_clip_param`. This means that it will take more than 2884.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:42,638\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2884.0x the scale of `vf_clip_param`. This means that it will take more than 2884.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:44,514\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2884.0x the scale of `vf_clip_param`. This means that it will take more than 2884.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -30416.511948211402\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 138\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.247\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.361175775527954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.80837433214765e-07\n",
      "        policy_loss: -0.00013735389802604914\n",
      "        total_loss: 18233296.0\n",
      "        vf_explained_var: -1.3113021850585938e-05\n",
      "        vf_loss: 18233298.0\n",
      "    load_time_ms: 1.265\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "    sample_time_ms: 2831.459\n",
      "    update_time_ms: 4.669\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.099999999999998\n",
      "    ram_util_percent: 53.25000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.367570461155285\n",
      "    mean_inference_ms: 0.9495801270309329\n",
      "    mean_processing_ms: 1.3903673562987149\n",
      "  time_since_restore: 374.08488726615906\n",
      "  time_this_iter_s: 4.312615633010864\n",
      "  time_total_s: 374.08488726615906\n",
      "  timestamp: 1595517828\n",
      "  timesteps_since_restore: 138000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 374 s, 138 iter, 138000 ts, -3.04e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:48,830\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3042.0x the scale of `vf_clip_param`. This means that it will take more than 3042.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:51,226\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3042.0x the scale of `vf_clip_param`. This means that it will take more than 3042.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:53,299\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3042.0x the scale of `vf_clip_param`. This means that it will take more than 3042.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -32139.82865677051\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 141\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.36137855052948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2101999093138147e-06\n",
      "        policy_loss: -0.00026918030926026404\n",
      "        total_loss: 18085950.0\n",
      "        vf_explained_var: -1.7642974853515625e-05\n",
      "        vf_loss: 18085950.0\n",
      "    load_time_ms: 1.239\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "    sample_time_ms: 2852.087\n",
      "    update_time_ms: 4.894\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.233333333333334\n",
      "    ram_util_percent: 53.28333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.367795517721928\n",
      "    mean_inference_ms: 0.9500488044000663\n",
      "    mean_processing_ms: 1.3904677855686247\n",
      "  time_since_restore: 382.3854353427887\n",
      "  time_this_iter_s: 3.8563592433929443\n",
      "  time_total_s: 382.3854353427887\n",
      "  timestamp: 1595517837\n",
      "  timesteps_since_restore: 141000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 382 s, 141 iter, 141000 ts, -3.21e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:57,165\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3214.0x the scale of `vf_clip_param`. This means that it will take more than 3214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:23:59,877\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3214.0x the scale of `vf_clip_param`. This means that it will take more than 3214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9232900454108732\n",
      "  episode_reward_mean: -32139.82865677051\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 141\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.011\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3148738145828247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.978963609872153e-06\n",
      "        policy_loss: 1.1110305422334932e-05\n",
      "        total_loss: 3113969.75\n",
      "        vf_explained_var: -0.00010514259338378906\n",
      "        vf_loss: 3113969.5\n",
      "    load_time_ms: 1.184\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "    sample_time_ms: 2722.56\n",
      "    update_time_ms: 4.973\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.300000000000004\n",
      "    ram_util_percent: 53.29999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.367795517721928\n",
      "    mean_inference_ms: 0.9500488044000663\n",
      "    mean_processing_ms: 1.3904677855686245\n",
      "  time_since_restore: 387.63303112983704\n",
      "  time_this_iter_s: 2.5548582077026367\n",
      "  time_total_s: 387.63303112983704\n",
      "  timestamp: 1595517842\n",
      "  timesteps_since_restore: 143000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 387 s, 143 iter, 143000 ts, -3.21e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:02,441\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3214.0x the scale of `vf_clip_param`. This means that it will take more than 3214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:05,638\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3357.0x the scale of `vf_clip_param`. This means that it will take more than 3357.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1.9836056388508754\n",
      "  episode_reward_mean: -33566.09483620264\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 22.981\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1708017587661743\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1027335858670995e-05\n",
      "        policy_loss: -0.0003070239908993244\n",
      "        total_loss: 0.0033126715570688248\n",
      "        vf_explained_var: 0.9705492258071899\n",
      "        vf_loss: 0.003619708586484194\n",
      "    load_time_ms: 1.168\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "    sample_time_ms: 2691.519\n",
      "    update_time_ms: 5.279\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.23333333333333\n",
      "    ram_util_percent: 53.20000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.369002010460862\n",
      "    mean_inference_ms: 0.9506865484872452\n",
      "    mean_processing_ms: 1.3906205932500686\n",
      "  time_since_restore: 392.7932782173157\n",
      "  time_this_iter_s: 1.9722635746002197\n",
      "  time_total_s: 392.7932782173157\n",
      "  timestamp: 1595517847\n",
      "  timesteps_since_restore: 145000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 392 s, 145 iter, 145000 ts, -3.36e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:07,618\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3357.0x the scale of `vf_clip_param`. This means that it will take more than 3357.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:10,127\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3357.0x the scale of `vf_clip_param`. This means that it will take more than 3357.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.011228161568473\n",
      "  episode_reward_mean: -35068.300955114355\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 147\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.644\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3641377687454224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.711164026753977e-06\n",
      "        policy_loss: -0.0004202613781671971\n",
      "        total_loss: 10301477.0\n",
      "        vf_explained_var: -6.9141387939453125e-06\n",
      "        vf_loss: 10301477.0\n",
      "    load_time_ms: 1.219\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "    sample_time_ms: 2830.776\n",
      "    update_time_ms: 5.069\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.0\n",
      "    ram_util_percent: 53.2\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.370421536498765\n",
      "    mean_inference_ms: 0.9513193497201636\n",
      "    mean_processing_ms: 1.3907630762393433\n",
      "  time_since_restore: 398.46950459480286\n",
      "  time_this_iter_s: 3.178441286087036\n",
      "  time_total_s: 398.46950459480286\n",
      "  timestamp: 1595517853\n",
      "  timesteps_since_restore: 147000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 398 s, 147 iter, 147000 ts, -3.51e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:13,309\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3507.0x the scale of `vf_clip_param`. This means that it will take more than 3507.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:15,257\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3507.0x the scale of `vf_clip_param`. This means that it will take more than 3507.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:17,303\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3507.0x the scale of `vf_clip_param`. This means that it will take more than 3507.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:21,308\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3679.0x the scale of `vf_clip_param`. This means that it will take more than 3679.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.1843934631617343\n",
      "  episode_reward_mean: -36786.18875977458\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 150\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.299\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.361046552658081\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.842473233817145e-06\n",
      "        policy_loss: -0.0003743896377272904\n",
      "        total_loss: 16933448.0\n",
      "        vf_explained_var: -2.7298927307128906e-05\n",
      "        vf_loss: 16933448.0\n",
      "    load_time_ms: 1.225\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    sample_time_ms: 2752.922\n",
      "    update_time_ms: 4.933\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.7\n",
      "    ram_util_percent: 53.28333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.371704682445961\n",
      "    mean_inference_ms: 0.9518631442606674\n",
      "    mean_processing_ms: 1.390892861304237\n",
      "  time_since_restore: 406.4419605731964\n",
      "  time_this_iter_s: 4.000784873962402\n",
      "  time_total_s: 406.4419605731964\n",
      "  timestamp: 1595517861\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 406 s, 150 iter, 150000 ts, -3.68e+04 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:23,403\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3679.0x the scale of `vf_clip_param`. This means that it will take more than 3679.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:25,368\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3679.0x the scale of `vf_clip_param`. This means that it will take more than 3679.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.4744405330623827\n",
      "  episode_reward_mean: -38017.38827542755\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 153\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.451\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.35776686668396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.751052306848578e-06\n",
      "        policy_loss: -0.0003563127538654953\n",
      "        total_loss: 6132072.5\n",
      "        vf_explained_var: -3.3736228942871094e-05\n",
      "        vf_loss: 6132072.5\n",
      "    load_time_ms: 1.222\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "    sample_time_ms: 2636.242\n",
      "    update_time_ms: 4.958\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.560000000000002\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.372983439556854\n",
      "    mean_inference_ms: 0.952385160389233\n",
      "    mean_processing_ms: 1.3910050942280594\n",
      "  time_since_restore: 414.3796203136444\n",
      "  time_this_iter_s: 3.901430130004883\n",
      "  time_total_s: 414.3796203136444\n",
      "  timestamp: 1595517869\n",
      "  timesteps_since_restore: 153000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 414 s, 153 iter, 153000 ts, -3.8e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:29,273\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3802.0x the scale of `vf_clip_param`. This means that it will take more than 3802.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:31,281\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3802.0x the scale of `vf_clip_param`. This means that it will take more than 3802.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:33,640\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3802.0x the scale of `vf_clip_param`. This means that it will take more than 3802.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.837500077081288\n",
      "  episode_reward_mean: -38951.96435891147\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 156\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.542\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3623420000076294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4473069970554207e-06\n",
      "        policy_loss: -0.0002534785307943821\n",
      "        total_loss: 9193746.0\n",
      "        vf_explained_var: -2.193450927734375e-05\n",
      "        vf_loss: 9193746.0\n",
      "    load_time_ms: 1.248\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    sample_time_ms: 2747.465\n",
      "    update_time_ms: 5.098\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.8\n",
      "    ram_util_percent: 53.300000000000004\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.374464345055893\n",
      "    mean_inference_ms: 0.9529455871338035\n",
      "    mean_processing_ms: 1.3911326289536745\n",
      "  time_since_restore: 423.16530418395996\n",
      "  time_this_iter_s: 4.440068244934082\n",
      "  time_total_s: 423.16530418395996\n",
      "  timestamp: 1595517878\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 423 s, 156 iter, 156000 ts, -3.9e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:38,087\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3895.0x the scale of `vf_clip_param`. This means that it will take more than 3895.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:40,836\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3895.0x the scale of `vf_clip_param`. This means that it will take more than 3895.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:43,014\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3895.0x the scale of `vf_clip_param`. This means that it will take more than 3895.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.837500077081288\n",
      "  episode_reward_mean: -40354.501357015564\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 159\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.246\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3660136461257935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.2742618714109994e-06\n",
      "        policy_loss: -0.000451169500593096\n",
      "        total_loss: 14979471.0\n",
      "        vf_explained_var: -2.09808349609375e-05\n",
      "        vf_loss: 14979471.0\n",
      "    load_time_ms: 1.206\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "    sample_time_ms: 2932.236\n",
      "    update_time_ms: 4.999\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.066666666666666\n",
      "    ram_util_percent: 53.300000000000004\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.37613502198569\n",
      "    mean_inference_ms: 0.9535447806282721\n",
      "    mean_processing_ms: 1.3913066386664388\n",
      "  time_since_restore: 432.14547181129456\n",
      "  time_this_iter_s: 4.074417352676392\n",
      "  time_total_s: 432.14547181129456\n",
      "  timestamp: 1595517887\n",
      "  timesteps_since_restore: 159000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 432 s, 159 iter, 159000 ts, -4.04e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:47,093\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4035.0x the scale of `vf_clip_param`. This means that it will take more than 4035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:49,463\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4035.0x the scale of `vf_clip_param`. This means that it will take more than 4035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:51,445\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4035.0x the scale of `vf_clip_param`. This means that it will take more than 4035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2.837500077081288\n",
      "  episode_reward_mean: -41339.28909470646\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 162\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3684098720550537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.706297426717356e-06\n",
      "        policy_loss: -0.0005402765236794949\n",
      "        total_loss: 15498929.0\n",
      "        vf_explained_var: -9.775161743164062e-06\n",
      "        vf_loss: 15498929.0\n",
      "    load_time_ms: 1.189\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "    sample_time_ms: 2900.456\n",
      "    update_time_ms: 4.943\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.540000000000003\n",
      "    ram_util_percent: 53.25999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.378046741598089\n",
      "    mean_inference_ms: 0.9541870503169804\n",
      "    mean_processing_ms: 1.3915123819421031\n",
      "  time_since_restore: 439.86893033981323\n",
      "  time_this_iter_s: 3.393984317779541\n",
      "  time_total_s: 439.86893033981323\n",
      "  timestamp: 1595517894\n",
      "  timesteps_since_restore: 162000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 439 s, 162 iter, 162000 ts, -4.13e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:54,845\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4134.0x the scale of `vf_clip_param`. This means that it will take more than 4134.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:57,206\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4134.0x the scale of `vf_clip_param`. This means that it will take more than 4134.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:24:59,488\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4134.0x the scale of `vf_clip_param`. This means that it will take more than 4134.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -1898.3607448578896\n",
      "  episode_reward_mean: -42419.06787188265\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 165\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 23.672\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3683229684829712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4525055576086743e-06\n",
      "        policy_loss: -0.00022071886633057147\n",
      "        total_loss: 16271586.0\n",
      "        vf_explained_var: -1.2993812561035156e-05\n",
      "        vf_loss: 16271586.0\n",
      "    load_time_ms: 1.182\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "    sample_time_ms: 2872.15\n",
      "    update_time_ms: 4.908\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.875\n",
      "    ram_util_percent: 53.27499999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.380061677981164\n",
      "    mean_inference_ms: 0.9548401719668398\n",
      "    mean_processing_ms: 1.3916881226349378\n",
      "  time_since_restore: 447.8198547363281\n",
      "  time_this_iter_s: 3.3349859714508057\n",
      "  time_total_s: 447.8198547363281\n",
      "  timestamp: 1595517902\n",
      "  timesteps_since_restore: 165000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:02,827\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4242.0x the scale of `vf_clip_param`. This means that it will take more than 4242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 447 s, 165 iter, 165000 ts, -4.24e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:05,240\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4242.0x the scale of `vf_clip_param`. This means that it will take more than 4242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:07,110\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4242.0x the scale of `vf_clip_param`. This means that it will take more than 4242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5686.5466966780095\n",
      "  episode_reward_mean: -43417.472185801686\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 24.051\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3701380491256714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1936298253422137e-06\n",
      "        policy_loss: -0.0002772808074951172\n",
      "        total_loss: 7507077.0\n",
      "        vf_explained_var: -1.2516975402832031e-05\n",
      "        vf_loss: 7507077.0\n",
      "    load_time_ms: 1.209\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    sample_time_ms: 2674.93\n",
      "    update_time_ms: 4.697\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.26\n",
      "    ram_util_percent: 53.25999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.382010315598654\n",
      "    mean_inference_ms: 0.9554621935184627\n",
      "    mean_processing_ms: 1.3918577034754172\n",
      "  time_since_restore: 455.19674491882324\n",
      "  time_this_iter_s: 3.117141008377075\n",
      "  time_total_s: 455.19674491882324\n",
      "  timestamp: 1595517910\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 455 s, 168 iter, 168000 ts, -4.34e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:10,231\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4342.0x the scale of `vf_clip_param`. This means that it will take more than 4342.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:12,900\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4342.0x the scale of `vf_clip_param`. This means that it will take more than 4342.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:14,888\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4342.0x the scale of `vf_clip_param`. This means that it will take more than 4342.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5686.5466966780095\n",
      "  episode_reward_mean: -43645.47183653359\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 171\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.454\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.368542194366455\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.026379206654383e-06\n",
      "        policy_loss: -0.00033890915801748633\n",
      "        total_loss: 12001176.0\n",
      "        vf_explained_var: -7.033348083496094e-06\n",
      "        vf_loss: 12001176.0\n",
      "    load_time_ms: 1.333\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "    sample_time_ms: 2660.411\n",
      "    update_time_ms: 5.223\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.94\n",
      "    ram_util_percent: 53.32000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.3838453461569\n",
      "    mean_inference_ms: 0.9560586315219921\n",
      "    mean_processing_ms: 1.3920313193895486\n",
      "  time_since_restore: 463.5284676551819\n",
      "  time_this_iter_s: 3.6969285011291504\n",
      "  time_total_s: 463.5284676551819\n",
      "  timestamp: 1595517918\n",
      "  timesteps_since_restore: 171000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 463 s, 171 iter, 171000 ts, -4.36e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:18,590\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4365.0x the scale of `vf_clip_param`. This means that it will take more than 4365.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:21,094\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4365.0x the scale of `vf_clip_param`. This means that it will take more than 4365.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:22,994\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4365.0x the scale of `vf_clip_param`. This means that it will take more than 4365.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5686.5466966780095\n",
      "  episode_reward_mean: -44056.020979530556\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 174\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 30.83\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3693220615386963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4181604203477036e-06\n",
      "        policy_loss: -0.00033083342714235187\n",
      "        total_loss: 6398822.5\n",
      "        vf_explained_var: -1.9431114196777344e-05\n",
      "        vf_loss: 6398822.5\n",
      "    load_time_ms: 1.336\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "    sample_time_ms: 2639.439\n",
      "    update_time_ms: 4.909\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.040000000000003\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.385314294730216\n",
      "    mean_inference_ms: 0.9565777749802806\n",
      "    mean_processing_ms: 1.3921953689124207\n",
      "  time_since_restore: 471.33123207092285\n",
      "  time_this_iter_s: 3.4202051162719727\n",
      "  time_total_s: 471.33123207092285\n",
      "  timestamp: 1595517926\n",
      "  timesteps_since_restore: 174000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 471 s, 174 iter, 174000 ts, -4.41e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:26,418\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4406.0x the scale of `vf_clip_param`. This means that it will take more than 4406.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:28,699\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4406.0x the scale of `vf_clip_param`. This means that it will take more than 4406.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:30,635\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4406.0x the scale of `vf_clip_param`. This means that it will take more than 4406.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:34,229\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4424.0x the scale of `vf_clip_param`. This means that it will take more than 4424.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44243.990453465914\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 177\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.864\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3708478212356567\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1852849840797717e-06\n",
      "        policy_loss: -0.0002584543253760785\n",
      "        total_loss: 9618334.0\n",
      "        vf_explained_var: -1.3589859008789062e-05\n",
      "        vf_loss: 9618335.0\n",
      "    load_time_ms: 1.381\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "    sample_time_ms: 2656.779\n",
      "    update_time_ms: 4.829\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.86\n",
      "    ram_util_percent: 53.239999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.386529457852255\n",
      "    mean_inference_ms: 0.9570270088964871\n",
      "    mean_processing_ms: 1.392351598479394\n",
      "  time_since_restore: 479.11334013938904\n",
      "  time_this_iter_s: 3.5897858142852783\n",
      "  time_total_s: 479.11334013938904\n",
      "  timestamp: 1595517934\n",
      "  timesteps_since_restore: 177000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 479 s, 177 iter, 177000 ts, -4.42e+04 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:38,233\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4424.0x the scale of `vf_clip_param`. This means that it will take more than 4424.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:40,937\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4424.0x the scale of `vf_clip_param`. This means that it will take more than 4424.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44243.99045346593\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 177\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 31.019\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.342880129814148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.666711793608556e-07\n",
      "        policy_loss: -5.547714317799546e-05\n",
      "        total_loss: 7033645.5\n",
      "        vf_explained_var: -0.00011277198791503906\n",
      "        vf_loss: 7033645.0\n",
      "    load_time_ms: 1.34\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "    sample_time_ms: 2748.324\n",
      "    update_time_ms: 5.233\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.425000000000004\n",
      "    ram_util_percent: 53.2\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.386529457852255\n",
      "    mean_inference_ms: 0.9570270088964871\n",
      "    mean_processing_ms: 1.3923515984793937\n",
      "  time_since_restore: 485.7900185585022\n",
      "  time_this_iter_s: 2.6986496448516846\n",
      "  time_total_s: 485.7900185585022\n",
      "  timestamp: 1595517940\n",
      "  timesteps_since_restore: 179000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 485 s, 179 iter, 179000 ts, -4.42e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:45,317\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4479.0x the scale of `vf_clip_param`. This means that it will take more than 4479.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44793.82875523599\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 180\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.299\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1613774299621582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.955675199700636e-06\n",
      "        policy_loss: -8.403920946875587e-05\n",
      "        total_loss: 0.005177030339837074\n",
      "        vf_explained_var: 0.9535159468650818\n",
      "        vf_loss: 0.005261063575744629\n",
      "    load_time_ms: 1.263\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "    sample_time_ms: 2861.106\n",
      "    update_time_ms: 5.384\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.76666666666667\n",
      "    ram_util_percent: 53.43333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.388586587926827\n",
      "    mean_inference_ms: 0.9576625804907137\n",
      "    mean_processing_ms: 1.3925268791439098\n",
      "  time_since_restore: 492.5424406528473\n",
      "  time_this_iter_s: 2.385035514831543\n",
      "  time_total_s: 492.5424406528473\n",
      "  timestamp: 1595517947\n",
      "  timesteps_since_restore: 181000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:47,725\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4479.0x the scale of `vf_clip_param`. This means that it will take more than 4479.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 492 s, 181 iter, 181000 ts, -4.48e+04 rew\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:50,286\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4479.0x the scale of `vf_clip_param`. This means that it will take more than 4479.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44080.424879887054\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 183\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 27.263\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3659030199050903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.5877695242779737e-07\n",
      "        policy_loss: -0.0001175708748633042\n",
      "        total_loss: 4924824.5\n",
      "        vf_explained_var: -3.409385681152344e-05\n",
      "        vf_loss: 4924823.5\n",
      "    load_time_ms: 1.333\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "    sample_time_ms: 3037.424\n",
      "    update_time_ms: 5.049\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.300000000000004\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.390979539256358\n",
      "    mean_inference_ms: 0.9583775270818968\n",
      "    mean_processing_ms: 1.3926957391160244\n",
      "  time_since_restore: 498.7118694782257\n",
      "  time_this_iter_s: 3.6180381774902344\n",
      "  time_total_s: 498.7118694782257\n",
      "  timestamp: 1595517953\n",
      "  timesteps_since_restore: 183000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 498 s, 183 iter, 183000 ts, -4.41e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:53,908\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4408.0x the scale of `vf_clip_param`. This means that it will take more than 4408.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:56,986\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4408.0x the scale of `vf_clip_param`. This means that it will take more than 4408.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44080.424879887054\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 183\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.613\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3435639142990112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.47140814433078e-07\n",
      "        policy_loss: -6.418800330720842e-05\n",
      "        total_loss: 17116314.0\n",
      "        vf_explained_var: -6.0439109802246094e-05\n",
      "        vf_loss: 17116314.0\n",
      "    load_time_ms: 1.279\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "    sample_time_ms: 3000.482\n",
      "    update_time_ms: 5.247\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.266666666666666\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.390979539256358\n",
      "    mean_inference_ms: 0.9583775270818968\n",
      "    mean_processing_ms: 1.3926957391160244\n",
      "  time_since_restore: 504.0037627220154\n",
      "  time_this_iter_s: 2.2333824634552\n",
      "  time_total_s: 504.0037627220154\n",
      "  timestamp: 1595517959\n",
      "  timesteps_since_restore: 185000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:25:59,225\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4408.0x the scale of `vf_clip_param`. This means that it will take more than 4408.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 504 s, 185 iter, 185000 ts, -4.41e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:02,788\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4513.0x the scale of `vf_clip_param`. This means that it will take more than 4513.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -45109.99753807625\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 186\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 25.059\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2662330865859985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.595139451637806e-06\n",
      "        policy_loss: -8.14788363641128e-05\n",
      "        total_loss: 1984203.5\n",
      "        vf_explained_var: -0.00031113624572753906\n",
      "        vf_loss: 1984203.25\n",
      "    load_time_ms: 1.249\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "    sample_time_ms: 3067.408\n",
      "    update_time_ms: 5.39\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.875\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.393995939333244\n",
      "    mean_inference_ms: 0.9592288001016763\n",
      "    mean_processing_ms: 1.3928898181392224\n",
      "  time_since_restore: 510.1865060329437\n",
      "  time_this_iter_s: 2.629450798034668\n",
      "  time_total_s: 510.1865060329437\n",
      "  timestamp: 1595517965\n",
      "  timesteps_since_restore: 187000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 510 s, 187 iter, 187000 ts, -4.51e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:05,423\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4511.0x the scale of `vf_clip_param`. This means that it will take more than 4511.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:08,565\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4489.0x the scale of `vf_clip_param`. This means that it will take more than 4489.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:10,255\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4474.0x the scale of `vf_clip_param`. This means that it will take more than 4474.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10738.29325135081\n",
      "  episode_reward_mean: -44383.48203765948\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 20.43\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.292920708656311\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.783251718734391e-06\n",
      "        policy_loss: -0.00020689201483037323\n",
      "        total_loss: 5557459.0\n",
      "        vf_explained_var: -0.0002262592315673828\n",
      "        vf_loss: 5557458.0\n",
      "    load_time_ms: 1.094\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    sample_time_ms: 2660.716\n",
      "    update_time_ms: 4.6\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.200000000000003\n",
      "    ram_util_percent: 53.333333333333336\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.396674060890621\n",
      "    mean_inference_ms: 0.9600216492494615\n",
      "    mean_processing_ms: 1.3930701730774906\n",
      "  time_since_restore: 517.0915739536285\n",
      "  time_this_iter_s: 2.091977834701538\n",
      "  time_total_s: 517.0915739536285\n",
      "  timestamp: 1595517972\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 517 s, 190 iter, 190000 ts, -4.44e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:12,349\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4438.0x the scale of `vf_clip_param`. This means that it will take more than 4438.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:13,501\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4438.0x the scale of `vf_clip_param`. This means that it will take more than 4438.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:16,305\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4458.0x the scale of `vf_clip_param`. This means that it will take more than 4458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10743.118166463773\n",
      "  episode_reward_mean: -44578.9328041109\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 192\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.725\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.163439154624939\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.917051347139932e-07\n",
      "        policy_loss: -9.820461173148942e-07\n",
      "        total_loss: 0.005614591762423515\n",
      "        vf_explained_var: 0.9547849297523499\n",
      "        vf_loss: 0.005615577567368746\n",
      "    load_time_ms: 0.944\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "    sample_time_ms: 2332.213\n",
      "    update_time_ms: 3.914\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.55\n",
      "    ram_util_percent: 53.349999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.398231570383238\n",
      "    mean_inference_ms: 0.9605965820925985\n",
      "    mean_processing_ms: 1.3931600692248722\n",
      "  time_since_restore: 522.303236246109\n",
      "  time_this_iter_s: 1.2643065452575684\n",
      "  time_total_s: 522.303236246109\n",
      "  timestamp: 1595517977\n",
      "  timesteps_since_restore: 193000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 522 s, 193 iter, 193000 ts, -4.46e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:17,571\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4458.0x the scale of `vf_clip_param`. This means that it will take more than 4458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:18,623\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4458.0x the scale of `vf_clip_param`. This means that it will take more than 4458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:20,773\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4414.0x the scale of `vf_clip_param`. This means that it will take more than 4414.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:22,102\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4414.0x the scale of `vf_clip_param`. This means that it will take more than 4414.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10743.118166463773\n",
      "  episode_reward_mean: -44137.19960232788\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 195\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 11.526\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3367160558700562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.422898301636451e-07\n",
      "        policy_loss: -0.00011652207467705011\n",
      "        total_loss: 2750882.25\n",
      "        vf_explained_var: -0.00011587142944335938\n",
      "        vf_loss: 2750881.75\n",
      "    load_time_ms: 0.751\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "    sample_time_ms: 1792.721\n",
      "    update_time_ms: 2.645\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.15\n",
      "    ram_util_percent: 53.349999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.398379992061472\n",
      "    mean_inference_ms: 0.9608933670147232\n",
      "    mean_processing_ms: 1.393153857762674\n",
      "  time_since_restore: 528.3020992279053\n",
      "  time_this_iter_s: 1.4798786640167236\n",
      "  time_total_s: 528.3020992279053\n",
      "  timestamp: 1595517983\n",
      "  timesteps_since_restore: 197000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 528 s, 197 iter, 197000 ts, -4.41e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:23,584\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4414.0x the scale of `vf_clip_param`. This means that it will take more than 4414.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:25,769\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4377.0x the scale of `vf_clip_param`. This means that it will take more than 4377.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:26,804\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4377.0x the scale of `vf_clip_param`. This means that it will take more than 4377.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:27,886\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4377.0x the scale of `vf_clip_param`. This means that it will take more than 4377.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -10743.118166463773\n",
      "  episode_reward_mean: -43121.326364789755\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 201\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.163\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.372800350189209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0719171516248025e-06\n",
      "        policy_loss: -0.0002499742549844086\n",
      "        total_loss: 7755608.5\n",
      "        vf_explained_var: -2.7894973754882812e-05\n",
      "        vf_loss: 7755608.0\n",
      "    load_time_ms: 0.829\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "    sample_time_ms: 1653.853\n",
      "    update_time_ms: 2.044\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.399999999999995\n",
      "    ram_util_percent: 53.43333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.395490739857499\n",
      "    mean_inference_ms: 0.9608727278587942\n",
      "    mean_processing_ms: 1.3928991560036204\n",
      "  time_since_restore: 534.9838342666626\n",
      "  time_this_iter_s: 2.390728712081909\n",
      "  time_total_s: 534.9838342666626\n",
      "  timestamp: 1595517990\n",
      "  timesteps_since_restore: 201000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 534 s, 201 iter, 201000 ts, -4.31e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:30,282\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4312.0x the scale of `vf_clip_param`. This means that it will take more than 4312.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:32,420\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4312.0x the scale of `vf_clip_param`. This means that it will take more than 4312.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:34,531\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4312.0x the scale of `vf_clip_param`. This means that it will take more than 4312.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -7590.736610838424\n",
      "  episode_reward_mean: -42969.97685716731\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 204\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.741\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3663911819458008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.433014858776005e-06\n",
      "        policy_loss: -0.0001837692252593115\n",
      "        total_loss: 1796670.875\n",
      "        vf_explained_var: -5.8650970458984375e-05\n",
      "        vf_loss: 1796670.75\n",
      "    load_time_ms: 1.097\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    sample_time_ms: 2004.15\n",
      "    update_time_ms: 2.943\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.48333333333334\n",
      "    ram_util_percent: 55.43333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.393405187209828\n",
      "    mean_inference_ms: 0.960802431036589\n",
      "    mean_processing_ms: 1.3927595775794939\n",
      "  time_since_restore: 543.658798456192\n",
      "  time_this_iter_s: 4.449965953826904\n",
      "  time_total_s: 543.658798456192\n",
      "  timestamp: 1595517998\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 543 s, 204 iter, 204000 ts, -4.3e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:38,987\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4297.0x the scale of `vf_clip_param`. This means that it will take more than 4297.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:40,580\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4297.0x the scale of `vf_clip_param`. This means that it will take more than 4297.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:42,068\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4297.0x the scale of `vf_clip_param`. This means that it will take more than 4297.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -7590.736610838424\n",
      "  episode_reward_mean: -42849.45332960787\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 207\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 17.599\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3796274662017822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.526223624954582e-06\n",
      "        policy_loss: -0.0002932109928224236\n",
      "        total_loss: 9100063.0\n",
      "        vf_explained_var: -1.3947486877441406e-05\n",
      "        vf_loss: 9100063.0\n",
      "    load_time_ms: 1.131\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "    sample_time_ms: 2084.614\n",
      "    update_time_ms: 3.154\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.450000000000003\n",
      "    ram_util_percent: 55.85\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.390769571317225\n",
      "    mean_inference_ms: 0.9606568744081778\n",
      "    mean_processing_ms: 1.3925921286504142\n",
      "  time_since_restore: 549.4301071166992\n",
      "  time_this_iter_s: 2.7020063400268555\n",
      "  time_total_s: 549.4301071166992\n",
      "  timestamp: 1595518004\n",
      "  timesteps_since_restore: 207000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 549 s, 207 iter, 207000 ts, -4.28e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:44,772\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4285.0x the scale of `vf_clip_param`. This means that it will take more than 4285.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:46,047\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4285.0x the scale of `vf_clip_param`. This means that it will take more than 4285.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:47,870\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4285.0x the scale of `vf_clip_param`. This means that it will take more than 4285.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -7590.736610838424\n",
      "  episode_reward_mean: -42060.62228065729\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 210\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 19.337\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3794282674789429\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2827982775197597e-06\n",
      "        policy_loss: -0.00024311542802024633\n",
      "        total_loss: 9658958.0\n",
      "        vf_explained_var: -1.3232231140136719e-05\n",
      "        vf_loss: 9658958.0\n",
      "    load_time_ms: 1.223\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    sample_time_ms: 2279.885\n",
      "    update_time_ms: 3.382\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.6\n",
      "    ram_util_percent: 55.82000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.387465483049847\n",
      "    mean_inference_ms: 0.9603995921717021\n",
      "    mean_processing_ms: 1.3923660815161008\n",
      "  time_since_restore: 555.6982128620148\n",
      "  time_this_iter_s: 3.1796553134918213\n",
      "  time_total_s: 555.6982128620148\n",
      "  timestamp: 1595518011\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.2/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 555 s, 210 iter, 210000 ts, -4.21e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:51,055\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4206.0x the scale of `vf_clip_param`. This means that it will take more than 4206.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:52,906\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4206.0x the scale of `vf_clip_param`. This means that it will take more than 4206.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:54,938\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4206.0x the scale of `vf_clip_param`. This means that it will take more than 4206.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -7590.736610838424\n",
      "  episode_reward_mean: -41783.23949173042\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 213\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 17.654\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.378025770187378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.833293703792151e-06\n",
      "        policy_loss: -0.0003391799982637167\n",
      "        total_loss: 7678611.5\n",
      "        vf_explained_var: -1.7404556274414062e-05\n",
      "        vf_loss: 7678612.0\n",
      "    load_time_ms: 1.009\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "    sample_time_ms: 2353.47\n",
      "    update_time_ms: 2.972\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.5\n",
      "    ram_util_percent: 56.06\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.383114817956698\n",
      "    mean_inference_ms: 0.9599571865664331\n",
      "    mean_processing_ms: 1.3920679226176436\n",
      "  time_since_restore: 563.0139956474304\n",
      "  time_this_iter_s: 3.444903612136841\n",
      "  time_total_s: 563.0139956474304\n",
      "  timestamp: 1595518018\n",
      "  timesteps_since_restore: 213000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 563 s, 213 iter, 213000 ts, -4.18e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:26:58,386\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4178.0x the scale of `vf_clip_param`. This means that it will take more than 4178.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:00,188\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4178.0x the scale of `vf_clip_param`. This means that it will take more than 4178.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:02,805\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4178.0x the scale of `vf_clip_param`. This means that it will take more than 4178.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -40974.97177205525\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 216\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 18.845\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.369325041770935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2021064321743324e-06\n",
      "        policy_loss: -0.00015614079893566668\n",
      "        total_loss: 1432473.25\n",
      "        vf_explained_var: -4.744529724121094e-05\n",
      "        vf_loss: 1432473.0\n",
      "    load_time_ms: 0.9\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    sample_time_ms: 2451.482\n",
      "    update_time_ms: 3.542\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.083333333333336\n",
      "    ram_util_percent: 57.18333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.378182974320183\n",
      "    mean_inference_ms: 0.959433220350533\n",
      "    mean_processing_ms: 1.3917366758881848\n",
      "  time_since_restore: 571.53990483284\n",
      "  time_this_iter_s: 4.120362997055054\n",
      "  time_total_s: 571.53990483284\n",
      "  timestamp: 1595518026\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:06,930\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4097.0x the scale of `vf_clip_param`. This means that it will take more than 4097.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 571 s, 216 iter, 216000 ts, -4.1e+04 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:08,731\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4097.0x the scale of `vf_clip_param`. This means that it will take more than 4097.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:10,434\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4097.0x the scale of `vf_clip_param`. This means that it will take more than 4097.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -40496.17905416081\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 219\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 18.286\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.37655508518219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.116462489240803e-07\n",
      "        policy_loss: -8.974313823273405e-05\n",
      "        total_loss: 2701137.0\n",
      "        vf_explained_var: -2.6345252990722656e-05\n",
      "        vf_loss: 2701136.75\n",
      "    load_time_ms: 0.883\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "    sample_time_ms: 2477.342\n",
      "    update_time_ms: 4.272\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.375\n",
      "    ram_util_percent: 57.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.372738364856524\n",
      "    mean_inference_ms: 0.9588339904021251\n",
      "    mean_processing_ms: 1.3913605441188224\n",
      "  time_since_restore: 577.5901417732239\n",
      "  time_this_iter_s: 2.565148115158081\n",
      "  time_total_s: 577.5901417732239\n",
      "  timestamp: 1595518033\n",
      "  timesteps_since_restore: 219000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 577 s, 219 iter, 219000 ts, -4.05e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:13,002\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4050.0x the scale of `vf_clip_param`. This means that it will take more than 4050.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:14,334\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4050.0x the scale of `vf_clip_param`. This means that it will take more than 4050.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:16,217\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4050.0x the scale of `vf_clip_param`. This means that it will take more than 4050.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -40098.88281213818\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 222\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 19.695\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.382657527923584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.980828185172868e-06\n",
      "        policy_loss: -0.0002744865487329662\n",
      "        total_loss: 13111942.0\n",
      "        vf_explained_var: -1.537799835205078e-05\n",
      "        vf_loss: 13111942.0\n",
      "    load_time_ms: 0.908\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "    sample_time_ms: 2394.479\n",
      "    update_time_ms: 4.391\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.35\n",
      "    ram_util_percent: 58.0\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.366890156641577\n",
      "    mean_inference_ms: 0.9581803713221402\n",
      "    mean_processing_ms: 1.390963089096936\n",
      "  time_since_restore: 583.8352828025818\n",
      "  time_this_iter_s: 3.0466063022613525\n",
      "  time_total_s: 583.8352828025818\n",
      "  timestamp: 1595518039\n",
      "  timesteps_since_restore: 222000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 583 s, 222 iter, 222000 ts, -4.01e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:19,267\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4010.0x the scale of `vf_clip_param`. This means that it will take more than 4010.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:21,984\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4010.0x the scale of `vf_clip_param`. This means that it will take more than 4010.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -40098.882812138174\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 222\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 20.067\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3433678150177002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2441873877833132e-06\n",
      "        policy_loss: -1.6319274436682463e-05\n",
      "        total_loss: 148695.796875\n",
      "        vf_explained_var: -0.0001785755157470703\n",
      "        vf_loss: 148695.71875\n",
      "    load_time_ms: 0.956\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    sample_time_ms: 2387.787\n",
      "    update_time_ms: 4.604\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.13333333333333\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.366890156641575\n",
      "    mean_inference_ms: 0.9581803713221401\n",
      "    mean_processing_ms: 1.390963089096936\n",
      "  time_since_restore: 589.011786699295\n",
      "  time_this_iter_s: 2.4725990295410156\n",
      "  time_total_s: 589.011786699295\n",
      "  timestamp: 1595518044\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 589 s, 224 iter, 224000 ts, -4.01e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:24,462\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 4010.0x the scale of `vf_clip_param`. This means that it will take more than 4010.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:28,161\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3911.0x the scale of `vf_clip_param`. This means that it will take more than 3911.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -39114.45187905513\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 225\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 18.591\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1396889686584473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.737296588544268e-05\n",
      "        policy_loss: -0.0003764324064832181\n",
      "        total_loss: 0.005856555886566639\n",
      "        vf_explained_var: 0.962639331817627\n",
      "        vf_loss: 0.006232984364032745\n",
      "    load_time_ms: 0.994\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "    sample_time_ms: 2224.221\n",
      "    update_time_ms: 4.193\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.55\n",
      "    ram_util_percent: 59.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.361297408781357\n",
      "    mean_inference_ms: 0.9576397359252594\n",
      "    mean_processing_ms: 1.3905747665818955\n",
      "  time_since_restore: 594.0832905769348\n",
      "  time_this_iter_s: 1.3828811645507812\n",
      "  time_total_s: 594.0832905769348\n",
      "  timestamp: 1595518049\n",
      "  timesteps_since_restore: 226000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 594 s, 226 iter, 226000 ts, -3.91e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:29,549\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3911.0x the scale of `vf_clip_param`. This means that it will take more than 3911.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:31,044\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3911.0x the scale of `vf_clip_param`. This means that it will take more than 3911.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -38452.21938550449\n",
      "  episode_reward_min: -86504.65034667363\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 228\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 19.194\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.377457618713379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4186501857693656e-06\n",
      "        policy_loss: -0.00019251251069363207\n",
      "        total_loss: 2472440.5\n",
      "        vf_explained_var: -3.2901763916015625e-05\n",
      "        vf_loss: 2472440.25\n",
      "    load_time_ms: 1.029\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    sample_time_ms: 2381.594\n",
      "    update_time_ms: 3.659\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.760000000000005\n",
      "    ram_util_percent: 59.02\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.355231806634298\n",
      "    mean_inference_ms: 0.9570162578278973\n",
      "    mean_processing_ms: 1.390132907826922\n",
      "  time_since_restore: 599.1430373191833\n",
      "  time_this_iter_s: 3.572242021560669\n",
      "  time_total_s: 599.1430373191833\n",
      "  timestamp: 1595518054\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 599 s, 228 iter, 228000 ts, -3.85e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:34,622\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3845.0x the scale of `vf_clip_param`. This means that it will take more than 3845.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:35,772\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3845.0x the scale of `vf_clip_param`. This means that it will take more than 3845.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:37,484\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3845.0x the scale of `vf_clip_param`. This means that it will take more than 3845.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -37296.668110228136\n",
      "  episode_reward_min: -82056.9522993052\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 231\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 19.079\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3817026615142822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.2169818950933404e-06\n",
      "        policy_loss: -0.00036135007394477725\n",
      "        total_loss: 5281216.0\n",
      "        vf_explained_var: -2.4080276489257812e-05\n",
      "        vf_loss: 5281215.5\n",
      "    load_time_ms: 1.041\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "    sample_time_ms: 2398.967\n",
      "    update_time_ms: 3.458\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.7\n",
      "    ram_util_percent: 59.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.348683212841961\n",
      "    mean_inference_ms: 0.9563076716051573\n",
      "    mean_processing_ms: 1.3896637916891574\n",
      "  time_since_restore: 605.0771996974945\n",
      "  time_this_iter_s: 3.08061146736145\n",
      "  time_total_s: 605.0771996974945\n",
      "  timestamp: 1595518060\n",
      "  timesteps_since_restore: 231000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 605 s, 231 iter, 231000 ts, -3.73e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:40,570\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3730.0x the scale of `vf_clip_param`. This means that it will take more than 3730.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:41,757\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3730.0x the scale of `vf_clip_param`. This means that it will take more than 3730.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:43,237\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3730.0x the scale of `vf_clip_param`. This means that it will take more than 3730.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -36582.85429417938\n",
      "  episode_reward_min: -82056.9522993052\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 234\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.38334059715271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.671547794212529e-07\n",
      "        policy_loss: -0.00021406746236607432\n",
      "        total_loss: 5667453.0\n",
      "        vf_explained_var: -1.823902130126953e-05\n",
      "        vf_loss: 5667453.0\n",
      "    load_time_ms: 0.954\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "    sample_time_ms: 2093.854\n",
      "    update_time_ms: 3.152\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.025000000000002\n",
      "    ram_util_percent: 58.975\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.3413924553300545\n",
      "    mean_inference_ms: 0.955449781382536\n",
      "    mean_processing_ms: 1.3891417928730008\n",
      "  time_since_restore: 610.2064208984375\n",
      "  time_this_iter_s: 2.4722561836242676\n",
      "  time_total_s: 610.2064208984375\n",
      "  timestamp: 1595518065\n",
      "  timesteps_since_restore: 234000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 610 s, 234 iter, 234000 ts, -3.66e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:45,712\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3658.0x the scale of `vf_clip_param`. This means that it will take more than 3658.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:47,012\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3658.0x the scale of `vf_clip_param`. This means that it will take more than 3658.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:48,391\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3658.0x the scale of `vf_clip_param`. This means that it will take more than 3658.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -36538.445103481834\n",
      "  episode_reward_min: -82056.9522993052\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 237\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.533\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3773670196533203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.234326868550852e-06\n",
      "        policy_loss: -0.0002447235456202179\n",
      "        total_loss: 8079345.5\n",
      "        vf_explained_var: -3.3855438232421875e-05\n",
      "        vf_loss: 8079345.5\n",
      "    load_time_ms: 0.863\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "    sample_time_ms: 1994.323\n",
      "    update_time_ms: 2.886\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.74\n",
      "    ram_util_percent: 58.919999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.333514058607282\n",
      "    mean_inference_ms: 0.9544732907260073\n",
      "    mean_processing_ms: 1.3885514918127932\n",
      "  time_since_restore: 615.7473335266113\n",
      "  time_this_iter_s: 2.8751935958862305\n",
      "  time_total_s: 615.7473335266113\n",
      "  timestamp: 1595518071\n",
      "  timesteps_since_restore: 237000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 615 s, 237 iter, 237000 ts, -3.65e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:51,269\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3654.0x the scale of `vf_clip_param`. This means that it will take more than 3654.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:52,528\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3654.0x the scale of `vf_clip_param`. This means that it will take more than 3654.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:53,906\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3654.0x the scale of `vf_clip_param`. This means that it will take more than 3654.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -35591.82070573529\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 240\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3826327323913574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.338324288255535e-06\n",
      "        policy_loss: -0.00027991484967060387\n",
      "        total_loss: 5730153.5\n",
      "        vf_explained_var: -1.811981201171875e-05\n",
      "        vf_loss: 5730153.0\n",
      "    load_time_ms: 0.851\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    sample_time_ms: 1859.295\n",
      "    update_time_ms: 2.708\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.425\n",
      "    ram_util_percent: 58.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.325133250284469\n",
      "    mean_inference_ms: 0.9533981645549306\n",
      "    mean_processing_ms: 1.3879197993176704\n",
      "  time_since_restore: 620.8154582977295\n",
      "  time_this_iter_s: 2.4403164386749268\n",
      "  time_total_s: 620.8154582977295\n",
      "  timestamp: 1595518076\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 620 s, 240 iter, 240000 ts, -3.56e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:56,350\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3559.0x the scale of `vf_clip_param`. This means that it will take more than 3559.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:57,438\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3559.0x the scale of `vf_clip_param`. This means that it will take more than 3559.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:27:58,743\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3559.0x the scale of `vf_clip_param`. This means that it will take more than 3559.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -34841.27707352413\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 243\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.883\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3825331926345825\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.677295692090411e-06\n",
      "        policy_loss: -0.00040853690006770194\n",
      "        total_loss: 3781135.75\n",
      "        vf_explained_var: -3.337860107421875e-05\n",
      "        vf_loss: 3781136.25\n",
      "    load_time_ms: 0.86\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "    sample_time_ms: 1810.988\n",
      "    update_time_ms: 2.615\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.000000000000004\n",
      "    ram_util_percent: 58.849999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.3159277251935455\n",
      "    mean_inference_ms: 0.9521455870206488\n",
      "    mean_processing_ms: 1.3872355877761138\n",
      "  time_since_restore: 626.0940854549408\n",
      "  time_this_iter_s: 2.899193525314331\n",
      "  time_total_s: 626.0940854549408\n",
      "  timestamp: 1595518081\n",
      "  timesteps_since_restore: 243000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 626 s, 243 iter, 243000 ts, -3.48e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:01,645\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3484.0x the scale of `vf_clip_param`. This means that it will take more than 3484.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:02,821\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3484.0x the scale of `vf_clip_param`. This means that it will take more than 3484.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:04,127\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3484.0x the scale of `vf_clip_param`. This means that it will take more than 3484.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:06,628\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3431.0x the scale of `vf_clip_param`. This means that it will take more than 3431.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -34305.56628518663\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 246\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.666\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1347745656967163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1541683963732794e-05\n",
      "        policy_loss: -0.00039377022767439485\n",
      "        total_loss: 0.007048658560961485\n",
      "        vf_explained_var: 0.9533742070198059\n",
      "        vf_loss: 0.007442425470799208\n",
      "    load_time_ms: 0.81\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "    sample_time_ms: 1614.871\n",
      "    update_time_ms: 2.603\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.7\n",
      "    ram_util_percent: 58.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.306126739310998\n",
      "    mean_inference_ms: 0.9507668505843001\n",
      "    mean_processing_ms: 1.3865180663000118\n",
      "  time_since_restore: 632.1308348178864\n",
      "  time_this_iter_s: 1.0705811977386475\n",
      "  time_total_s: 632.1308348178864\n",
      "  timestamp: 1595518087\n",
      "  timesteps_since_restore: 247000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 632 s, 247 iter, 247000 ts, -3.43e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:07,701\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3431.0x the scale of `vf_clip_param`. This means that it will take more than 3431.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:08,999\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3431.0x the scale of `vf_clip_param`. This means that it will take more than 3431.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:11,801\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3347.0x the scale of `vf_clip_param`. This means that it will take more than 3347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5060.286586672388\n",
      "  episode_reward_mean: -33472.51902433574\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 249\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.159\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1384997367858887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.53611801781517e-06\n",
      "        policy_loss: -9.332466288469732e-05\n",
      "        total_loss: 0.010375404730439186\n",
      "        vf_explained_var: 0.7759227156639099\n",
      "        vf_loss: 0.010468734428286552\n",
      "    load_time_ms: 0.787\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    sample_time_ms: 1620.602\n",
      "    update_time_ms: 2.568\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.8\n",
      "    ram_util_percent: 58.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.295934523550153\n",
      "    mean_inference_ms: 0.949300078623846\n",
      "    mean_processing_ms: 1.385764262037412\n",
      "  time_since_restore: 637.2486100196838\n",
      "  time_this_iter_s: 1.0274789333343506\n",
      "  time_total_s: 637.2486100196838\n",
      "  timestamp: 1595518092\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 637 s, 250 iter, 250000 ts, -3.35e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:12,831\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3347.0x the scale of `vf_clip_param`. This means that it will take more than 3347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:14,126\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3347.0x the scale of `vf_clip_param`. This means that it will take more than 3347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:16,553\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3302.0x the scale of `vf_clip_param`. This means that it will take more than 3302.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2538.446241547162\n",
      "  episode_reward_mean: -33017.95041358371\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 252\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1291747093200684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.138286259258166e-06\n",
      "        policy_loss: -0.00013359951844904572\n",
      "        total_loss: 0.006826222874224186\n",
      "        vf_explained_var: 0.9212657809257507\n",
      "        vf_loss: 0.0069598122499883175\n",
      "    load_time_ms: 0.771\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "    sample_time_ms: 1596.967\n",
      "    update_time_ms: 2.533\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4\n",
      "    ram_util_percent: 58.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.285426676519972\n",
      "    mean_inference_ms: 0.9477698466525369\n",
      "    mean_processing_ms: 1.3849949768058065\n",
      "  time_since_restore: 642.2618982791901\n",
      "  time_this_iter_s: 1.3000037670135498\n",
      "  time_total_s: 642.2618982791901\n",
      "  timestamp: 1595518097\n",
      "  timesteps_since_restore: 253000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 642 s, 253 iter, 253000 ts, -3.3e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:17,856\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3302.0x the scale of `vf_clip_param`. This means that it will take more than 3302.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:19,208\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3302.0x the scale of `vf_clip_param`. This means that it will take more than 3302.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:22,062\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3246.0x the scale of `vf_clip_param`. This means that it will take more than 3246.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2538.446241547162\n",
      "  episode_reward_mean: -32462.670873766747\n",
      "  episode_reward_min: -71434.96612977947\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 255\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.184\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1346282958984375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.818460289854556e-06\n",
      "        policy_loss: -0.00012042999151162803\n",
      "        total_loss: 0.00791274942457676\n",
      "        vf_explained_var: 0.8996708393096924\n",
      "        vf_loss: 0.00803317315876484\n",
      "    load_time_ms: 0.784\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    sample_time_ms: 1622.6\n",
      "    update_time_ms: 2.404\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.2\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.274426685802618\n",
      "    mean_inference_ms: 0.9461338824909106\n",
      "    mean_processing_ms: 1.3841831271832012\n",
      "  time_since_restore: 647.4832167625427\n",
      "  time_this_iter_s: 1.0251338481903076\n",
      "  time_total_s: 647.4832167625427\n",
      "  timestamp: 1595518103\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 647 s, 256 iter, 256000 ts, -3.25e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:23,090\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3246.0x the scale of `vf_clip_param`. This means that it will take more than 3246.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:24,546\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3246.0x the scale of `vf_clip_param`. This means that it will take more than 3246.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:27,095\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3112.0x the scale of `vf_clip_param`. This means that it will take more than 3112.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2532.9171776324088\n",
      "  episode_reward_mean: -31123.59604226863\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 258\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1316850185394287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.753981324436609e-06\n",
      "        policy_loss: -0.00011730098776752129\n",
      "        total_loss: 0.00672492990270257\n",
      "        vf_explained_var: 0.9181762337684631\n",
      "        vf_loss: 0.006842238362878561\n",
      "    load_time_ms: 0.787\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "    sample_time_ms: 1617.152\n",
      "    update_time_ms: 2.572\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.0\n",
      "    ram_util_percent: 58.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.262723616427309\n",
      "    mean_inference_ms: 0.9443503271651966\n",
      "    mean_processing_ms: 1.3833161522418578\n",
      "  time_since_restore: 652.6002516746521\n",
      "  time_this_iter_s: 1.1216869354248047\n",
      "  time_total_s: 652.6002516746521\n",
      "  timestamp: 1595518108\n",
      "  timesteps_since_restore: 259000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:28,219\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3112.0x the scale of `vf_clip_param`. This means that it will take more than 3112.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 652 s, 259 iter, 259000 ts, -3.11e+04 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:29,559\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3112.0x the scale of `vf_clip_param`. This means that it will take more than 3112.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:32,037\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3035.0x the scale of `vf_clip_param`. This means that it will take more than 3035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2532.9171776324088\n",
      "  episode_reward_mean: -30354.74759440398\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 261\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.119\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1295199394226074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.5723148812394356e-06\n",
      "        policy_loss: -3.441524677327834e-05\n",
      "        total_loss: 0.007292564492672682\n",
      "        vf_explained_var: 0.9180157780647278\n",
      "        vf_loss: 0.007326975930482149\n",
      "    load_time_ms: 0.802\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "    sample_time_ms: 1649.127\n",
      "    update_time_ms: 2.625\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.6\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.250408495325792\n",
      "    mean_inference_ms: 0.9424384507738651\n",
      "    mean_processing_ms: 1.3824091711490878\n",
      "  time_since_restore: 657.6656248569489\n",
      "  time_this_iter_s: 1.2610273361206055\n",
      "  time_total_s: 657.6656248569489\n",
      "  timestamp: 1595518113\n",
      "  timesteps_since_restore: 262000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 657 s, 262 iter, 262000 ts, -3.04e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:33,301\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3035.0x the scale of `vf_clip_param`. This means that it will take more than 3035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:34,650\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 3035.0x the scale of `vf_clip_param`. This means that it will take more than 3035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:37,113\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2917.0x the scale of `vf_clip_param`. This means that it will take more than 2917.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:38,180\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2917.0x the scale of `vf_clip_param`. This means that it will take more than 2917.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2532.9171776324088\n",
      "  episode_reward_mean: -29173.760909649263\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 264\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.149\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3615145683288574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0274649184793816e-06\n",
      "        policy_loss: -0.00012996673467569053\n",
      "        total_loss: 2415148.0\n",
      "        vf_explained_var: -0.00010216236114501953\n",
      "        vf_loss: 2415147.75\n",
      "    load_time_ms: 0.792\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "    sample_time_ms: 1620.434\n",
      "    update_time_ms: 2.631\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.25\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.237582557940742\n",
      "    mean_inference_ms: 0.9404151869256151\n",
      "    mean_processing_ms: 1.3814483227744112\n",
      "  time_since_restore: 663.9000709056854\n",
      "  time_this_iter_s: 1.368180274963379\n",
      "  time_total_s: 663.9000709056854\n",
      "  timestamp: 1595518119\n",
      "  timesteps_since_restore: 266000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 663 s, 266 iter, 266000 ts, -2.92e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:39,551\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2917.0x the scale of `vf_clip_param`. This means that it will take more than 2917.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:42,376\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2882.0x the scale of `vf_clip_param`. This means that it will take more than 2882.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:43,411\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2882.0x the scale of `vf_clip_param`. This means that it will take more than 2882.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2532.9171776324088\n",
      "  episode_reward_mean: -28820.51528640494\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 267\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.491\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3442416191101074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.497898717341741e-07\n",
      "        policy_loss: -3.3013344364007935e-05\n",
      "        total_loss: 235076.578125\n",
      "        vf_explained_var: -0.00023496150970458984\n",
      "        vf_loss: 235076.34375\n",
      "    load_time_ms: 0.782\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "    sample_time_ms: 1627.853\n",
      "    update_time_ms: 2.436\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25\n",
      "    ram_util_percent: 58.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.224303610565493\n",
      "    mean_inference_ms: 0.9382966461861839\n",
      "    mean_processing_ms: 1.3804315578196944\n",
      "  time_since_restore: 669.0804154872894\n",
      "  time_this_iter_s: 1.3303885459899902\n",
      "  time_total_s: 669.0804154872894\n",
      "  timestamp: 1595518124\n",
      "  timesteps_since_restore: 269000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 669 s, 269 iter, 269000 ts, -2.88e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:44,744\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2882.0x the scale of `vf_clip_param`. This means that it will take more than 2882.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:47,589\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2816.0x the scale of `vf_clip_param`. This means that it will take more than 2816.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:49,237\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2816.0x the scale of `vf_clip_param`. This means that it will take more than 2816.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -2532.9171776324088\n",
      "  episode_reward_mean: -28157.562452439663\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 270\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.255\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.350609540939331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.084374415289858e-07\n",
      "        policy_loss: -7.83271825639531e-05\n",
      "        total_loss: 819978.75\n",
      "        vf_explained_var: -0.00014710426330566406\n",
      "        vf_loss: 819978.375\n",
      "    load_time_ms: 0.79\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    sample_time_ms: 1753.498\n",
      "    update_time_ms: 2.406\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.63333333333333\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.210463593880442\n",
      "    mean_inference_ms: 0.9360455930511595\n",
      "    mean_processing_ms: 1.379368269392321\n",
      "  time_since_restore: 675.4088027477264\n",
      "  time_this_iter_s: 1.8446533679962158\n",
      "  time_total_s: 675.4088027477264\n",
      "  timestamp: 1595518131\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 675 s, 272 iter, 272000 ts, -2.82e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:51,085\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2816.0x the scale of `vf_clip_param`. This means that it will take more than 2816.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:53,852\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2731.0x the scale of `vf_clip_param`. This means that it will take more than 2731.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:55,050\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2731.0x the scale of `vf_clip_param`. This means that it will take more than 2731.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -6.247634568097545\n",
      "  episode_reward_mean: -27311.872196627235\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 273\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.676\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.341943383216858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.502534807215852e-07\n",
      "        policy_loss: -2.1368026864365675e-05\n",
      "        total_loss: 247173.4375\n",
      "        vf_explained_var: -0.00016379356384277344\n",
      "        vf_loss: 247173.28125\n",
      "    load_time_ms: 0.853\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "    sample_time_ms: 1877.952\n",
      "    update_time_ms: 2.436\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.6\n",
      "    ram_util_percent: 59.23333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.196327612856492\n",
      "    mean_inference_ms: 0.933761110101728\n",
      "    mean_processing_ms: 1.3782885279932457\n",
      "  time_since_restore: 681.5387015342712\n",
      "  time_this_iter_s: 2.1775074005126953\n",
      "  time_total_s: 681.5387015342712\n",
      "  timestamp: 1595518137\n",
      "  timesteps_since_restore: 275000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:28:57,231\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2731.0x the scale of `vf_clip_param`. This means that it will take more than 2731.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 681 s, 275 iter, 275000 ts, -2.73e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:00,750\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2651.0x the scale of `vf_clip_param`. This means that it will take more than 2651.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -6.247634568097545\n",
      "  episode_reward_mean: -26511.80801123859\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 276\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.596\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.128657341003418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.956390289909905e-06\n",
      "        policy_loss: -4.891967910225503e-05\n",
      "        total_loss: 0.007362825330346823\n",
      "        vf_explained_var: 0.8939589262008667\n",
      "        vf_loss: 0.007411746308207512\n",
      "    load_time_ms: 0.853\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "    sample_time_ms: 1996.407\n",
      "    update_time_ms: 2.714\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65\n",
      "    ram_util_percent: 60.150000000000006\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.182170991530217\n",
      "    mean_inference_ms: 0.9314909305550775\n",
      "    mean_processing_ms: 1.3772023537623863\n",
      "  time_since_restore: 686.9234006404877\n",
      "  time_this_iter_s: 1.8816184997558594\n",
      "  time_total_s: 686.9234006404877\n",
      "  timestamp: 1595518142\n",
      "  timesteps_since_restore: 277000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 686 s, 277 iter, 277000 ts, -2.65e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:02,635\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2651.0x the scale of `vf_clip_param`. This means that it will take more than 2651.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:04,057\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2651.0x the scale of `vf_clip_param`. This means that it will take more than 2651.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:06,613\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2602.0x the scale of `vf_clip_param`. This means that it will take more than 2602.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -6.247634568097545\n",
      "  episode_reward_mean: -26018.913064809854\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 279\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.233\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1290488243103027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7695248970994726e-05\n",
      "        policy_loss: -0.00026242065359838307\n",
      "        total_loss: 0.007729234639555216\n",
      "        vf_explained_var: 0.8854650259017944\n",
      "        vf_loss: 0.007991660386323929\n",
      "    load_time_ms: 0.85\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    sample_time_ms: 1987.501\n",
      "    update_time_ms: 2.969\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.0\n",
      "    ram_util_percent: 60.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.167332159792162\n",
      "    mean_inference_ms: 0.9290781728987747\n",
      "    mean_processing_ms: 1.376065678490705\n",
      "  time_since_restore: 692.0459358692169\n",
      "  time_this_iter_s: 1.156987190246582\n",
      "  time_total_s: 692.0459358692169\n",
      "  timestamp: 1595518147\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 692 s, 280 iter, 280000 ts, -2.6e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:07,774\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2602.0x the scale of `vf_clip_param`. This means that it will take more than 2602.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:09,369\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2602.0x the scale of `vf_clip_param`. This means that it will take more than 2602.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -25129.04019427614\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 281\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.15\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3074965476989746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.401385922392365e-06\n",
      "        policy_loss: -0.0004412193375173956\n",
      "        total_loss: 1676907.75\n",
      "        vf_explained_var: -0.00019073486328125\n",
      "        vf_loss: 1676907.5\n",
      "    load_time_ms: 0.861\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "    sample_time_ms: 2140.155\n",
      "    update_time_ms: 3.054\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.58\n",
      "    ram_util_percent: 60.81999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.156951940109266\n",
      "    mean_inference_ms: 0.9273596102021959\n",
      "    mean_processing_ms: 1.3752686160744563\n",
      "  time_since_restore: 697.0645732879639\n",
      "  time_this_iter_s: 3.434318780899048\n",
      "  time_total_s: 697.0645732879639\n",
      "  timestamp: 1595518152\n",
      "  timesteps_since_restore: 282000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.1/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 697 s, 282 iter, 282000 ts, -2.51e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:12,806\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2513.0x the scale of `vf_clip_param`. This means that it will take more than 2513.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:15,008\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2519.0x the scale of `vf_clip_param`. This means that it will take more than 2519.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -25242.655771764923\n",
      "  episode_reward_min: -70060.45291573861\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 283\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 17.462\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2965136766433716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.860395948635414e-07\n",
      "        policy_loss: -4.5793534809490666e-05\n",
      "        total_loss: 299503.53125\n",
      "        vf_explained_var: -0.0003349781036376953\n",
      "        vf_loss: 299503.4375\n",
      "    load_time_ms: 0.844\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    sample_time_ms: 2351.612\n",
      "    update_time_ms: 3.651\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.06666666666667\n",
      "    ram_util_percent: 61.71666666666667\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.146680622751405\n",
      "    mean_inference_ms: 0.9256814091195242\n",
      "    mean_processing_ms: 1.374511866959778\n",
      "  time_since_restore: 703.1538200378418\n",
      "  time_this_iter_s: 3.89802885055542\n",
      "  time_total_s: 703.1538200378418\n",
      "  timestamp: 1595518158\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 703 s, 284 iter, 284000 ts, -2.52e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:18,910\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2524.0x the scale of `vf_clip_param`. This means that it will take more than 2524.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:22,597\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2479.0x the scale of `vf_clip_param`. This means that it will take more than 2479.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -24226.64894041367\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 285\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 17.835\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.326059103012085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.255903720855713e-06\n",
      "        policy_loss: -0.00024079513968899846\n",
      "        total_loss: 1892393.0\n",
      "        vf_explained_var: -0.00019288063049316406\n",
      "        vf_loss: 1892392.625\n",
      "    load_time_ms: 0.917\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "    sample_time_ms: 2391.732\n",
      "    update_time_ms: 3.641\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.599999999999998\n",
      "    ram_util_percent: 62.53333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.136098958936814\n",
      "    mean_inference_ms: 0.9239398063310464\n",
      "    mean_processing_ms: 1.3737370514347453\n",
      "  time_since_restore: 709.2374324798584\n",
      "  time_this_iter_s: 2.4065723419189453\n",
      "  time_total_s: 709.2374324798584\n",
      "  timestamp: 1595518165\n",
      "  timesteps_since_restore: 286000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 709 s, 286 iter, 286000 ts, -2.42e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:25,007\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2423.0x the scale of `vf_clip_param`. This means that it will take more than 2423.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:26,895\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2410.0x the scale of `vf_clip_param`. This means that it will take more than 2410.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:29,330\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2390.0x the scale of `vf_clip_param`. This means that it will take more than 2390.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -23607.130240003196\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 288\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 18.23\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2621206045150757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.548609467747156e-06\n",
      "        policy_loss: -0.0002992401132360101\n",
      "        total_loss: 700636.5625\n",
      "        vf_explained_var: -0.00030994415283203125\n",
      "        vf_loss: 700636.375\n",
      "    load_time_ms: 0.964\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "    sample_time_ms: 2417.976\n",
      "    update_time_ms: 3.649\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.333333333333332\n",
      "    ram_util_percent: 62.166666666666664\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.119953765486477\n",
      "    mean_inference_ms: 0.921274486219938\n",
      "    mean_processing_ms: 1.3725245562938424\n",
      "  time_since_restore: 715.350260257721\n",
      "  time_this_iter_s: 1.8000295162200928\n",
      "  time_total_s: 715.350260257721\n",
      "  timestamp: 1595518171\n",
      "  timesteps_since_restore: 289000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 715 s, 289 iter, 289000 ts, -2.36e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:31,133\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2361.0x the scale of `vf_clip_param`. This means that it will take more than 2361.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:32,661\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2361.0x the scale of `vf_clip_param`. This means that it will take more than 2361.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:35,798\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2313.0x the scale of `vf_clip_param`. This means that it will take more than 2313.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -23127.37341101278\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 291\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 18.384\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.118906021118164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.754789632803295e-06\n",
      "        policy_loss: -0.00012136888835811988\n",
      "        total_loss: 0.006802912801504135\n",
      "        vf_explained_var: 0.9448140859603882\n",
      "        vf_loss: 0.006924278102815151\n",
      "    load_time_ms: 0.958\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    sample_time_ms: 2399.893\n",
      "    update_time_ms: 3.611\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.65\n",
      "    ram_util_percent: 62.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.104620372048091\n",
      "    mean_inference_ms: 0.9187782680954859\n",
      "    mean_processing_ms: 1.3713588908492234\n",
      "  time_since_restore: 721.3458571434021\n",
      "  time_this_iter_s: 1.3413803577423096\n",
      "  time_total_s: 721.3458571434021\n",
      "  timestamp: 1595518177\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 721 s, 292 iter, 292000 ts, -2.31e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:37,144\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2313.0x the scale of `vf_clip_param`. This means that it will take more than 2313.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:38,591\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2313.0x the scale of `vf_clip_param`. This means that it will take more than 2313.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:41,200\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2230.0x the scale of `vf_clip_param`. This means that it will take more than 2230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -22300.292541159288\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 294\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.804\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1310182809829712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.834404429246206e-05\n",
      "        policy_loss: -0.0002418632502667606\n",
      "        total_loss: 0.007871764712035656\n",
      "        vf_explained_var: 0.8901920318603516\n",
      "        vf_loss: 0.008113624528050423\n",
      "    load_time_ms: 0.923\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "    sample_time_ms: 1971.392\n",
      "    update_time_ms: 2.846\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.0\n",
      "    ram_util_percent: 62.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.089840075728789\n",
      "    mean_inference_ms: 0.916382316482666\n",
      "    mean_processing_ms: 1.3702521152003686\n",
      "  time_since_restore: 726.8021438121796\n",
      "  time_this_iter_s: 1.4126086235046387\n",
      "  time_total_s: 726.8021438121796\n",
      "  timestamp: 1595518182\n",
      "  timesteps_since_restore: 295000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 726 s, 295 iter, 295000 ts, -2.23e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:42,617\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2230.0x the scale of `vf_clip_param`. This means that it will take more than 2230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:43,999\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2230.0x the scale of `vf_clip_param`. This means that it will take more than 2230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:46,501\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2168.0x the scale of `vf_clip_param`. This means that it will take more than 2168.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -21675.60677652643\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 297\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.311\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1244215965270996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.74812666955404e-05\n",
      "        policy_loss: -0.00018508339417167008\n",
      "        total_loss: 0.008056260645389557\n",
      "        vf_explained_var: 0.8617056012153625\n",
      "        vf_loss: 0.00824135635048151\n",
      "    load_time_ms: 0.839\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "    sample_time_ms: 1828.73\n",
      "    update_time_ms: 2.696\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.85\n",
      "    ram_util_percent: 62.349999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.0756459797975495\n",
      "    mean_inference_ms: 0.9140889956824235\n",
      "    mean_processing_ms: 1.3691905060212166\n",
      "  time_since_restore: 732.0762917995453\n",
      "  time_this_iter_s: 1.400538682937622\n",
      "  time_total_s: 732.0762917995453\n",
      "  timestamp: 1595518187\n",
      "  timesteps_since_restore: 298000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 732 s, 298 iter, 298000 ts, -2.17e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:47,904\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2168.0x the scale of `vf_clip_param`. This means that it will take more than 2168.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:49,253\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2168.0x the scale of `vf_clip_param`. This means that it will take more than 2168.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:51,711\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2063.0x the scale of `vf_clip_param`. This means that it will take more than 2063.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:52,835\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2063.0x the scale of `vf_clip_param`. This means that it will take more than 2063.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -20633.51379118096\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 300\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.211\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3508042097091675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.849268068734091e-07\n",
      "        policy_loss: -4.6501158067258075e-05\n",
      "        total_loss: 41851.72265625\n",
      "        vf_explained_var: -0.00041556358337402344\n",
      "        vf_loss: 41851.7265625\n",
      "    load_time_ms: 0.768\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "    sample_time_ms: 1675.654\n",
      "    update_time_ms: 2.469\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.15\n",
      "    ram_util_percent: 62.2\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.0620250340986965\n",
      "    mean_inference_ms: 0.9118961607076517\n",
      "    mean_processing_ms: 1.3681636210734698\n",
      "  time_since_restore: 738.3134338855743\n",
      "  time_this_iter_s: 1.3232603073120117\n",
      "  time_total_s: 738.3134338855743\n",
      "  timestamp: 1595518194\n",
      "  timesteps_since_restore: 302000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 738 s, 302 iter, 302000 ts, -2.06e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:54,161\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2063.0x the scale of `vf_clip_param`. This means that it will take more than 2063.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:56,861\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2020.0x the scale of `vf_clip_param`. This means that it will take more than 2020.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -20204.01945515842\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 303\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.851\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1167362928390503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.319237516232533e-06\n",
      "        policy_loss: -7.323455793084577e-05\n",
      "        total_loss: 0.007096948567777872\n",
      "        vf_explained_var: 0.9214895963668823\n",
      "        vf_loss: 0.007170177064836025\n",
      "    load_time_ms: 0.832\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    sample_time_ms: 1777.949\n",
      "    update_time_ms: 2.951\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.7\n",
      "    ram_util_percent: 62.56666666666666\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.048386138956182\n",
      "    mean_inference_ms: 0.9096486868962185\n",
      "    mean_processing_ms: 1.3671517184766488\n",
      "  time_since_restore: 743.4041616916656\n",
      "  time_this_iter_s: 2.3972718715667725\n",
      "  time_total_s: 743.4041616916656\n",
      "  timestamp: 1595518199\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 743 s, 304 iter, 304000 ts, -2.02e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:29:59,270\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2020.0x the scale of `vf_clip_param`. This means that it will take more than 2020.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:00,764\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 2020.0x the scale of `vf_clip_param`. This means that it will take more than 2020.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:03,870\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1940.0x the scale of `vf_clip_param`. This means that it will take more than 1940.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -19402.444405150298\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 306\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.405\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1229850053787231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.23384561954299e-06\n",
      "        policy_loss: -9.943199256667867e-05\n",
      "        total_loss: 0.009237535297870636\n",
      "        vf_explained_var: 0.824140727519989\n",
      "        vf_loss: 0.009336980991065502\n",
      "    load_time_ms: 0.868\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "    sample_time_ms: 1841.173\n",
      "    update_time_ms: 2.916\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.95\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.03514831252412\n",
      "    mean_inference_ms: 0.907481237673101\n",
      "    mean_processing_ms: 1.3661662469161209\n",
      "  time_since_restore: 749.3303084373474\n",
      "  time_this_iter_s: 1.3363945484161377\n",
      "  time_total_s: 749.3303084373474\n",
      "  timestamp: 1595518205\n",
      "  timesteps_since_restore: 307000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:05,210\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1940.0x the scale of `vf_clip_param`. This means that it will take more than 1940.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 749 s, 307 iter, 307000 ts, -1.94e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:06,655\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1940.0x the scale of `vf_clip_param`. This means that it will take more than 1940.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:09,716\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1848.0x the scale of `vf_clip_param`. This means that it will take more than 1848.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-11\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -18480.425810597724\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 309\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.451\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1245551109313965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.299840490304632e-06\n",
      "        policy_loss: -0.00010194015339948237\n",
      "        total_loss: 0.008568295277655125\n",
      "        vf_explained_var: 0.8382284641265869\n",
      "        vf_loss: 0.008670245297253132\n",
      "    load_time_ms: 0.877\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    sample_time_ms: 1920.363\n",
      "    update_time_ms: 2.976\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.022052701693879\n",
      "    mean_inference_ms: 0.9052993094420774\n",
      "    mean_processing_ms: 1.365193942616794\n",
      "  time_since_restore: 755.3200829029083\n",
      "  time_this_iter_s: 1.4942536354064941\n",
      "  time_total_s: 755.3200829029083\n",
      "  timestamp: 1595518211\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 755 s, 310 iter, 310000 ts, -1.85e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:11,214\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1848.0x the scale of `vf_clip_param`. This means that it will take more than 1848.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:12,644\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1848.0x the scale of `vf_clip_param`. This means that it will take more than 1848.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:15,296\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1780.0x the scale of `vf_clip_param`. This means that it will take more than 1780.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -17798.739240315488\n",
      "  episode_reward_min: -60620.93343373706\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 312\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.198\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1170008182525635\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.5907090932596475e-05\n",
      "        policy_loss: -0.0004763364850077778\n",
      "        total_loss: 0.007112590596079826\n",
      "        vf_explained_var: 0.8712775111198425\n",
      "        vf_loss: 0.0075889211148023605\n",
      "    load_time_ms: 0.834\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "    sample_time_ms: 1980.451\n",
      "    update_time_ms: 3.074\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.650000000000006\n",
      "    ram_util_percent: 63.2\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 4.008980577890669\n",
      "    mean_inference_ms: 0.9030743807298586\n",
      "    mean_processing_ms: 1.3642119307832505\n",
      "  time_since_restore: 761.0552294254303\n",
      "  time_this_iter_s: 1.662621259689331\n",
      "  time_total_s: 761.0552294254303\n",
      "  timestamp: 1595518216\n",
      "  timesteps_since_restore: 313000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 761 s, 313 iter, 313000 ts, -1.78e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:16,963\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1780.0x the scale of `vf_clip_param`. This means that it will take more than 1780.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:18,693\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1780.0x the scale of `vf_clip_param`. This means that it will take more than 1780.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:21,293\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1691.0x the scale of `vf_clip_param`. This means that it will take more than 1691.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -16914.84005244183\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 315\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.198\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1157639026641846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.984510840382427e-06\n",
      "        policy_loss: -0.00011734390136552975\n",
      "        total_loss: 0.007790990639477968\n",
      "        vf_explained_var: 0.8634488582611084\n",
      "        vf_loss: 0.007908344268798828\n",
      "    load_time_ms: 0.829\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    sample_time_ms: 1836.765\n",
      "    update_time_ms: 2.862\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.0\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.9957981810247616\n",
      "    mean_inference_ms: 0.9007804319913946\n",
      "    mean_processing_ms: 1.3632189742985557\n",
      "  time_since_restore: 766.6038842201233\n",
      "  time_this_iter_s: 1.2345836162567139\n",
      "  time_total_s: 766.6038842201233\n",
      "  timestamp: 1595518222\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 766 s, 316 iter, 316000 ts, -1.69e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:22,531\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1691.0x the scale of `vf_clip_param`. This means that it will take more than 1691.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:24,013\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1691.0x the scale of `vf_clip_param`. This means that it will take more than 1691.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:27,078\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1616.0x the scale of `vf_clip_param`. This means that it will take more than 1616.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -16164.740405983852\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 318\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.196\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1191108226776123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2309848898439668e-05\n",
      "        policy_loss: -0.00014357948384713382\n",
      "        total_loss: 0.00810614600777626\n",
      "        vf_explained_var: 0.8648622632026672\n",
      "        vf_loss: 0.008249721489846706\n",
      "    load_time_ms: 0.846\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "    sample_time_ms: 1820.353\n",
      "    update_time_ms: 2.84\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.5\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.982577532873246\n",
      "    mean_inference_ms: 0.8984459440254835\n",
      "    mean_processing_ms: 1.362219200545403\n",
      "  time_since_restore: 772.2716100215912\n",
      "  time_this_iter_s: 1.1295053958892822\n",
      "  time_total_s: 772.2716100215912\n",
      "  timestamp: 1595518228\n",
      "  timesteps_since_restore: 319000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 772 s, 319 iter, 319000 ts, -1.62e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:28,211\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1616.0x the scale of `vf_clip_param`. This means that it will take more than 1616.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:29,586\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1616.0x the scale of `vf_clip_param`. This means that it will take more than 1616.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:32,919\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1557.0x the scale of `vf_clip_param`. This means that it will take more than 1557.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -15570.747308643959\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 321\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.641\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1109551191329956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010716586984926835\n",
      "        policy_loss: -0.0013466825475916266\n",
      "        total_loss: 0.005907229147851467\n",
      "        vf_explained_var: 0.8622565269470215\n",
      "        vf_loss: 0.007253916934132576\n",
      "    load_time_ms: 0.853\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "    sample_time_ms: 1855.978\n",
      "    update_time_ms: 2.849\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.400000000000002\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.969451310371133\n",
      "    mean_inference_ms: 0.8961044957365202\n",
      "    mean_processing_ms: 1.361233583373253\n",
      "  time_since_restore: 778.1864650249481\n",
      "  time_this_iter_s: 1.2213494777679443\n",
      "  time_total_s: 778.1864650249481\n",
      "  timestamp: 1595518234\n",
      "  timesteps_since_restore: 322000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 778 s, 322 iter, 322000 ts, -1.56e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:34,144\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1557.0x the scale of `vf_clip_param`. This means that it will take more than 1557.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:35,550\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1557.0x the scale of `vf_clip_param`. This means that it will take more than 1557.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:38,618\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1508.0x the scale of `vf_clip_param`. This means that it will take more than 1508.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -15084.573048186396\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 324\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.467\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1187492609024048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.622022268245928e-05\n",
      "        policy_loss: -0.0003165817179251462\n",
      "        total_loss: 0.007837009616196156\n",
      "        vf_explained_var: 0.8595122694969177\n",
      "        vf_loss: 0.008153602480888367\n",
      "    load_time_ms: 0.822\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "    sample_time_ms: 1825.711\n",
      "    update_time_ms: 2.468\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.85\n",
      "    ram_util_percent: 62.95\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.9559127208115283\n",
      "    mean_inference_ms: 0.8936233278496676\n",
      "    mean_processing_ms: 1.3602215914489866\n",
      "  time_since_restore: 783.8386902809143\n",
      "  time_this_iter_s: 1.1871109008789062\n",
      "  time_total_s: 783.8386902809143\n",
      "  timestamp: 1595518239\n",
      "  timesteps_since_restore: 325000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 783 s, 325 iter, 325000 ts, -1.51e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:39,808\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1508.0x the scale of `vf_clip_param`. This means that it will take more than 1508.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:41,217\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1508.0x the scale of `vf_clip_param`. This means that it will take more than 1508.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:43,833\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1458.0x the scale of `vf_clip_param`. This means that it will take more than 1458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -14579.268268428616\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 327\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.062\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.118424654006958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.998039341648109e-05\n",
      "        policy_loss: -0.0006863250746391714\n",
      "        total_loss: 0.007426694966852665\n",
      "        vf_explained_var: 0.8448573350906372\n",
      "        vf_loss: 0.008113018237054348\n",
      "    load_time_ms: 0.771\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    sample_time_ms: 1772.54\n",
      "    update_time_ms: 2.451\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.8\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.9422898459879026\n",
      "    mean_inference_ms: 0.8910999694815638\n",
      "    mean_processing_ms: 1.3592111933125088\n",
      "  time_since_restore: 789.0737066268921\n",
      "  time_this_iter_s: 1.2182648181915283\n",
      "  time_total_s: 789.0737066268921\n",
      "  timestamp: 1595518245\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 789 s, 328 iter, 328000 ts, -1.46e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:45,054\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1458.0x the scale of `vf_clip_param`. This means that it will take more than 1458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:46,468\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1458.0x the scale of `vf_clip_param`. This means that it will take more than 1458.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:49,514\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1397.0x the scale of `vf_clip_param`. This means that it will take more than 1397.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -13972.884061955374\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 330\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.902\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1239885091781616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9705712475115433e-05\n",
      "        policy_loss: -0.00026211977819912136\n",
      "        total_loss: 0.01482505165040493\n",
      "        vf_explained_var: 0.5066635012626648\n",
      "        vf_loss: 0.015087162144482136\n",
      "    load_time_ms: 0.76\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "    sample_time_ms: 1749.412\n",
      "    update_time_ms: 2.425\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.9\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.928899743297866\n",
      "    mean_inference_ms: 0.8886184015583699\n",
      "    mean_processing_ms: 1.3582177584230906\n",
      "  time_since_restore: 794.6622669696808\n",
      "  time_this_iter_s: 1.13726806640625\n",
      "  time_total_s: 794.6622669696808\n",
      "  timestamp: 1595518250\n",
      "  timesteps_since_restore: 331000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 794 s, 331 iter, 331000 ts, -1.4e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:50,655\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1397.0x the scale of `vf_clip_param`. This means that it will take more than 1397.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:52,118\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1397.0x the scale of `vf_clip_param`. This means that it will take more than 1397.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:55,163\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1344.0x the scale of `vf_clip_param`. This means that it will take more than 1344.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -13436.358020992107\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 333\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.659\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1110056638717651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.511973194596067e-07\n",
      "        policy_loss: -2.516746462788433e-06\n",
      "        total_loss: 0.009381880983710289\n",
      "        vf_explained_var: 0.8299011588096619\n",
      "        vf_loss: 0.009384400211274624\n",
      "    load_time_ms: 0.795\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "    sample_time_ms: 1743.825\n",
      "    update_time_ms: 2.422\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.4\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.9157741332630875\n",
      "    mean_inference_ms: 0.8861876487130114\n",
      "    mean_processing_ms: 1.3572479298838755\n",
      "  time_since_restore: 800.303658246994\n",
      "  time_this_iter_s: 1.1435983180999756\n",
      "  time_total_s: 800.303658246994\n",
      "  timestamp: 1595518256\n",
      "  timesteps_since_restore: 334000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 800 s, 334 iter, 334000 ts, -1.34e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:56,310\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1344.0x the scale of `vf_clip_param`. This means that it will take more than 1344.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:30:57,761\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1344.0x the scale of `vf_clip_param`. This means that it will take more than 1344.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:00,339\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1239.0x the scale of `vf_clip_param`. This means that it will take more than 1239.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -12388.22845865373\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 336\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.877\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1082146167755127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5981257092789747e-05\n",
      "        policy_loss: -0.0001602058473508805\n",
      "        total_loss: 0.008129602298140526\n",
      "        vf_explained_var: 0.8263448476791382\n",
      "        vf_loss: 0.008289802819490433\n",
      "    load_time_ms: 0.806\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "    sample_time_ms: 1746.128\n",
      "    update_time_ms: 2.639\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.0\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.9029549456311865\n",
      "    mean_inference_ms: 0.8838195809747456\n",
      "    mean_processing_ms: 1.3563078958195833\n",
      "  time_since_restore: 805.5351865291595\n",
      "  time_this_iter_s: 1.214233160018921\n",
      "  time_total_s: 805.5351865291595\n",
      "  timestamp: 1595518261\n",
      "  timesteps_since_restore: 337000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 805 s, 337 iter, 337000 ts, -1.24e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:01,556\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1239.0x the scale of `vf_clip_param`. This means that it will take more than 1239.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:02,991\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1239.0x the scale of `vf_clip_param`. This means that it will take more than 1239.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:06,420\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1117.0x the scale of `vf_clip_param`. This means that it will take more than 1117.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -11170.197513595158\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 339\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.598\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1080600023269653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6943087050312897e-06\n",
      "        policy_loss: -4.8874855565372854e-05\n",
      "        total_loss: 0.010835420340299606\n",
      "        vf_explained_var: 0.7064602971076965\n",
      "        vf_loss: 0.010884301736950874\n",
      "    load_time_ms: 0.831\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    sample_time_ms: 1787.671\n",
      "    update_time_ms: 2.664\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.8905189360104013\n",
      "    mean_inference_ms: 0.8815474483076515\n",
      "    mean_processing_ms: 1.3553972091101985\n",
      "  time_since_restore: 811.6300678253174\n",
      "  time_this_iter_s: 1.2389342784881592\n",
      "  time_total_s: 811.6300678253174\n",
      "  timestamp: 1595518267\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:07,662\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1117.0x the scale of `vf_clip_param`. This means that it will take more than 1117.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 811 s, 340 iter, 340000 ts, -1.12e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:09,053\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1117.0x the scale of `vf_clip_param`. This means that it will take more than 1117.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:11,773\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1072.0x the scale of `vf_clip_param`. This means that it will take more than 1072.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -10715.718221615369\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 342\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.966\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1051108837127686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3401498765451834e-05\n",
      "        policy_loss: -0.0002646274515427649\n",
      "        total_loss: 0.007985573261976242\n",
      "        vf_explained_var: 0.860824704170227\n",
      "        vf_loss: 0.008250193670392036\n",
      "    load_time_ms: 0.821\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "    sample_time_ms: 1749.767\n",
      "    update_time_ms: 2.712\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.7\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.8784511188473054\n",
      "    mean_inference_ms: 0.8793557822370046\n",
      "    mean_processing_ms: 1.3545215778945419\n",
      "  time_since_restore: 816.8788621425629\n",
      "  time_this_iter_s: 1.1554572582244873\n",
      "  time_total_s: 816.8788621425629\n",
      "  timestamp: 1595518272\n",
      "  timesteps_since_restore: 343000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 816 s, 343 iter, 343000 ts, -1.07e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:12,931\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1072.0x the scale of `vf_clip_param`. This means that it will take more than 1072.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:14,434\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 1072.0x the scale of `vf_clip_param`. This means that it will take more than 1072.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:17,461\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 964.0x the scale of `vf_clip_param`. This means that it will take more than 964.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -9641.861605068503\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 345\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1099814176559448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2809097825083882e-05\n",
      "        policy_loss: -0.00012196159514132887\n",
      "        total_loss: 0.009577148593962193\n",
      "        vf_explained_var: 0.7567114233970642\n",
      "        vf_loss: 0.009699102491140366\n",
      "    load_time_ms: 0.819\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "    sample_time_ms: 1819.924\n",
      "    update_time_ms: 2.486\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.349999999999994\n",
      "    ram_util_percent: 62.849999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.8667111202990054\n",
      "    mean_inference_ms: 0.8772364808531347\n",
      "    mean_processing_ms: 1.3536673480107058\n",
      "  time_since_restore: 822.7340705394745\n",
      "  time_this_iter_s: 1.335404396057129\n",
      "  time_total_s: 822.7340705394745\n",
      "  timestamp: 1595518278\n",
      "  timesteps_since_restore: 346000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 822 s, 346 iter, 346000 ts, -9.64e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:18,799\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 964.0x the scale of `vf_clip_param`. This means that it will take more than 964.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:20,198\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 964.0x the scale of `vf_clip_param`. This means that it will take more than 964.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:22,690\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 907.0x the scale of `vf_clip_param`. This means that it will take more than 907.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:23,758\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 907.0x the scale of `vf_clip_param`. This means that it will take more than 907.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -9073.765717382776\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 348\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3493950366973877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.949867502546113e-07\n",
      "        policy_loss: -2.4746894268901087e-05\n",
      "        total_loss: 0.10322924703359604\n",
      "        vf_explained_var: -0.7730449438095093\n",
      "        vf_loss: 0.10325399786233902\n",
      "    load_time_ms: 0.826\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    sample_time_ms: 1712.3\n",
      "    update_time_ms: 2.633\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.3\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.8553029982514277\n",
      "    mean_inference_ms: 0.8751897416426435\n",
      "    mean_processing_ms: 1.3528417586795667\n",
      "  time_since_restore: 828.9711062908173\n",
      "  time_this_iter_s: 1.2916457653045654\n",
      "  time_total_s: 828.9711062908173\n",
      "  timestamp: 1595518285\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 828 s, 350 iter, 350000 ts, -9.07e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:25,053\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 907.0x the scale of `vf_clip_param`. This means that it will take more than 907.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:28,057\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 863.0x the scale of `vf_clip_param`. This means that it will take more than 863.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:29,155\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 863.0x the scale of `vf_clip_param`. This means that it will take more than 863.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -8625.098947634364\n",
      "  episode_reward_min: -54938.348922438425\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 351\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.469\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3616782426834106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.66525534648099e-06\n",
      "        policy_loss: -7.919692870927975e-05\n",
      "        total_loss: 102002.3515625\n",
      "        vf_explained_var: -0.0001895427703857422\n",
      "        vf_loss: 102002.2734375\n",
      "    load_time_ms: 0.812\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "    sample_time_ms: 1735.597\n",
      "    update_time_ms: 2.588\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.85\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.844179030788418\n",
      "    mean_inference_ms: 0.8732010196145713\n",
      "    mean_processing_ms: 1.3520340318672714\n",
      "  time_since_restore: 834.450765132904\n",
      "  time_this_iter_s: 1.3926987648010254\n",
      "  time_total_s: 834.450765132904\n",
      "  timestamp: 1595518290\n",
      "  timesteps_since_restore: 353000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 834 s, 353 iter, 353000 ts, -8.63e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:30,550\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 863.0x the scale of `vf_clip_param`. This means that it will take more than 863.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:34,415\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 763.0x the scale of `vf_clip_param`. This means that it will take more than 763.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -7634.020514838762\n",
      "  episode_reward_min: -34098.93249779816\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 354\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.646\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1137224435806274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2531339962151833e-05\n",
      "        policy_loss: -0.00015796518709976226\n",
      "        total_loss: 0.009966821409761906\n",
      "        vf_explained_var: 0.7373019456863403\n",
      "        vf_loss: 0.01012479979544878\n",
      "    load_time_ms: 0.798\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "    sample_time_ms: 1792.125\n",
      "    update_time_ms: 2.61\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.05\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.833400639521339\n",
      "    mean_inference_ms: 0.8712888312898126\n",
      "    mean_processing_ms: 1.3512823290783558\n",
      "  time_since_restore: 839.5373935699463\n",
      "  time_this_iter_s: 1.228658676147461\n",
      "  time_total_s: 839.5373935699463\n",
      "  timestamp: 1595518295\n",
      "  timesteps_since_restore: 355000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 839 s, 355 iter, 355000 ts, -7.63e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:35,647\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 763.0x the scale of `vf_clip_param`. This means that it will take more than 763.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:37,179\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 763.0x the scale of `vf_clip_param`. This means that it will take more than 763.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:40,013\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 748.0x the scale of `vf_clip_param`. This means that it will take more than 748.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-41\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -7475.8665187159495\n",
      "  episode_reward_min: -34098.93249779816\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 357\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.68\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1069116592407227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.911217951710569e-06\n",
      "        policy_loss: -0.00012234115274623036\n",
      "        total_loss: 0.010142032988369465\n",
      "        vf_explained_var: 0.7580845355987549\n",
      "        vf_loss: 0.01026435848325491\n",
      "    load_time_ms: 0.762\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "    sample_time_ms: 1822.09\n",
      "    update_time_ms: 2.6\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.9\n",
      "    ram_util_percent: 63.2\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.8229411827851676\n",
      "    mean_inference_ms: 0.8694521983613085\n",
      "    mean_processing_ms: 1.3505641716823993\n",
      "  time_since_restore: 845.04026055336\n",
      "  time_this_iter_s: 1.1461577415466309\n",
      "  time_total_s: 845.04026055336\n",
      "  timestamp: 1595518301\n",
      "  timesteps_since_restore: 358000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 845 s, 358 iter, 358000 ts, -7.48e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:41,162\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 748.0x the scale of `vf_clip_param`. This means that it will take more than 748.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:42,804\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 748.0x the scale of `vf_clip_param`. This means that it will take more than 748.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:45,357\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 723.0x the scale of `vf_clip_param`. This means that it will take more than 723.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -7229.225666071969\n",
      "  episode_reward_min: -34098.93249779816\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 360\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.282\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1066327095031738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.56967541645281e-06\n",
      "        policy_loss: -2.9577255190815777e-05\n",
      "        total_loss: 0.009450826793909073\n",
      "        vf_explained_var: 0.7937144041061401\n",
      "        vf_loss: 0.00948039349168539\n",
      "    load_time_ms: 0.791\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "    sample_time_ms: 1825.735\n",
      "    update_time_ms: 2.443\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.15\n",
      "    ram_util_percent: 63.05\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.812762504931621\n",
      "    mean_inference_ms: 0.8676777242932531\n",
      "    mean_processing_ms: 1.3498703471514435\n",
      "  time_since_restore: 850.4603455066681\n",
      "  time_this_iter_s: 1.2350125312805176\n",
      "  time_total_s: 850.4603455066681\n",
      "  timestamp: 1595518306\n",
      "  timesteps_since_restore: 361000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 850 s, 361 iter, 361000 ts, -7.23e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:46,598\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 723.0x the scale of `vf_clip_param`. This means that it will take more than 723.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:48,301\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 723.0x the scale of `vf_clip_param`. This means that it will take more than 723.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:50,975\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 688.0x the scale of `vf_clip_param`. This means that it will take more than 688.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -6882.0505223711925\n",
      "  episode_reward_min: -34098.93249779816\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 363\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.324\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1079462766647339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0334234907059e-05\n",
      "        policy_loss: -0.0003015327383764088\n",
      "        total_loss: 0.009365313686430454\n",
      "        vf_explained_var: 0.770489513874054\n",
      "        vf_loss: 0.00966684054583311\n",
      "    load_time_ms: 0.784\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    sample_time_ms: 1748.256\n",
      "    update_time_ms: 3.233\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.95\n",
      "    ram_util_percent: 62.95\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.802895031422219\n",
      "    mean_inference_ms: 0.8659791976930089\n",
      "    mean_processing_ms: 1.3491987334325137\n",
      "  time_since_restore: 856.0331695079803\n",
      "  time_this_iter_s: 1.2124276161193848\n",
      "  time_total_s: 856.0331695079803\n",
      "  timestamp: 1595518312\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 856 s, 364 iter, 364000 ts, -6.88e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:52,191\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 688.0x the scale of `vf_clip_param`. This means that it will take more than 688.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:53,632\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 688.0x the scale of `vf_clip_param`. This means that it will take more than 688.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:56,771\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 648.0x the scale of `vf_clip_param`. This means that it will take more than 648.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -6477.525331797149\n",
      "  episode_reward_min: -34098.93249779816\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 366\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.544\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1085600852966309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.486905395926442e-05\n",
      "        policy_loss: -0.00019452476408332586\n",
      "        total_loss: 0.009792651981115341\n",
      "        vf_explained_var: 0.7465780973434448\n",
      "        vf_loss: 0.009987177327275276\n",
      "    load_time_ms: 0.795\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "    sample_time_ms: 1759.56\n",
      "    update_time_ms: 3.167\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.2\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.7932854108928318\n",
      "    mean_inference_ms: 0.864339914982637\n",
      "    mean_processing_ms: 1.348556131156942\n",
      "  time_since_restore: 861.7346081733704\n",
      "  time_this_iter_s: 1.1300108432769775\n",
      "  time_total_s: 861.7346081733704\n",
      "  timestamp: 1595518317\n",
      "  timesteps_since_restore: 367000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 861 s, 367 iter, 367000 ts, -6.48e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:57,904\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 648.0x the scale of `vf_clip_param`. This means that it will take more than 648.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:31:59,313\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 648.0x the scale of `vf_clip_param`. This means that it will take more than 648.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:01,880\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 608.0x the scale of `vf_clip_param`. This means that it will take more than 608.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -6079.704608901448\n",
      "  episode_reward_min: -31605.680819544545\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 369\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.438\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1133662462234497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.677758650155738e-05\n",
      "        policy_loss: -0.0002925176522694528\n",
      "        total_loss: 0.009395342320203781\n",
      "        vf_explained_var: 0.7838338613510132\n",
      "        vf_loss: 0.00968785211443901\n",
      "    load_time_ms: 0.788\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    sample_time_ms: 1761.885\n",
      "    update_time_ms: 3.166\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.9\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.7839437434173635\n",
      "    mean_inference_ms: 0.8627592878265196\n",
      "    mean_processing_ms: 1.3479352338795711\n",
      "  time_since_restore: 867.0873055458069\n",
      "  time_this_iter_s: 1.385477066040039\n",
      "  time_total_s: 867.0873055458069\n",
      "  timestamp: 1595518323\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 867 s, 370 iter, 370000 ts, -6.08e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:03,268\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 608.0x the scale of `vf_clip_param`. This means that it will take more than 608.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:04,776\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 608.0x the scale of `vf_clip_param`. This means that it will take more than 608.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:07,396\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 574.0x the scale of `vf_clip_param`. This means that it will take more than 574.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -5737.89214844654\n",
      "  episode_reward_min: -31605.680819544545\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 372\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.892\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1149005889892578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1171102414664347e-05\n",
      "        policy_loss: -0.00011075782822445035\n",
      "        total_loss: 0.01008030865341425\n",
      "        vf_explained_var: 0.7526981830596924\n",
      "        vf_loss: 0.010191070847213268\n",
      "    load_time_ms: 0.784\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "    sample_time_ms: 1742.222\n",
      "    update_time_ms: 2.354\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.650000000000006\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.7747319277497597\n",
      "    mean_inference_ms: 0.8611939063581605\n",
      "    mean_processing_ms: 1.3473227570592479\n",
      "  time_since_restore: 872.4480748176575\n",
      "  time_this_iter_s: 1.2419166564941406\n",
      "  time_total_s: 872.4480748176575\n",
      "  timestamp: 1595518328\n",
      "  timesteps_since_restore: 373000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 872 s, 373 iter, 373000 ts, -5.74e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:08,641\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 574.0x the scale of `vf_clip_param`. This means that it will take more than 574.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:10,081\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 574.0x the scale of `vf_clip_param`. This means that it will take more than 574.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:13,237\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 540.0x the scale of `vf_clip_param`. This means that it will take more than 540.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-14\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -5403.186843273583\n",
      "  episode_reward_min: -31605.680819544545\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 375\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.445\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1126627922058105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1852791178389452e-05\n",
      "        policy_loss: -0.000297122955089435\n",
      "        total_loss: 0.011039611883461475\n",
      "        vf_explained_var: 0.6525733470916748\n",
      "        vf_loss: 0.011336728930473328\n",
      "    load_time_ms: 0.813\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    sample_time_ms: 1742.849\n",
      "    update_time_ms: 2.497\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.299999999999997\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.765410448013039\n",
      "    mean_inference_ms: 0.8595765499108934\n",
      "    mean_processing_ms: 1.3467032175894824\n",
      "  time_since_restore: 878.2458872795105\n",
      "  time_this_iter_s: 1.2179062366485596\n",
      "  time_total_s: 878.2458872795105\n",
      "  timestamp: 1595518334\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 878 s, 376 iter, 376000 ts, -5.4e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:14,458\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 540.0x the scale of `vf_clip_param`. This means that it will take more than 540.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:15,833\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 540.0x the scale of `vf_clip_param`. This means that it will take more than 540.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:18,791\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 499.0x the scale of `vf_clip_param`. This means that it will take more than 499.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-20\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.098577456046133\n",
      "  episode_reward_mean: -4992.741678684891\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 378\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.229\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.111849308013916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.637249266030267e-06\n",
      "        policy_loss: -0.00020009803120046854\n",
      "        total_loss: 0.013562293723225594\n",
      "        vf_explained_var: 0.5022243857383728\n",
      "        vf_loss: 0.013762391172349453\n",
      "    load_time_ms: 0.808\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "    sample_time_ms: 1789.869\n",
      "    update_time_ms: 2.813\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.25\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.756154075994026\n",
      "    mean_inference_ms: 0.8579583043511414\n",
      "    mean_processing_ms: 1.3460942571496661\n",
      "  time_since_restore: 883.8262436389923\n",
      "  time_this_iter_s: 1.2575597763061523\n",
      "  time_total_s: 883.8262436389923\n",
      "  timestamp: 1595518340\n",
      "  timesteps_since_restore: 379000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 883 s, 379 iter, 379000 ts, -4.99e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:20,053\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 499.0x the scale of `vf_clip_param`. This means that it will take more than 499.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:21,572\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 499.0x the scale of `vf_clip_param`. This means that it will take more than 499.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:24,176\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 514.0x the scale of `vf_clip_param`. This means that it will take more than 514.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -5144.272759331029\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 381\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.027\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1131107807159424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.500473424850497e-06\n",
      "        policy_loss: -0.00013016605225857347\n",
      "        total_loss: 0.010844037868082523\n",
      "        vf_explained_var: 0.7178533673286438\n",
      "        vf_loss: 0.01097419299185276\n",
      "    load_time_ms: 0.775\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "    sample_time_ms: 1772.385\n",
      "    update_time_ms: 2.88\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.95\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.746927392490726\n",
      "    mean_inference_ms: 0.8563369728511447\n",
      "    mean_processing_ms: 1.3454988336612734\n",
      "  time_since_restore: 889.1674308776855\n",
      "  time_this_iter_s: 1.23203706741333\n",
      "  time_total_s: 889.1674308776855\n",
      "  timestamp: 1595518345\n",
      "  timesteps_since_restore: 382000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 889 s, 382 iter, 382000 ts, -5.14e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:25,411\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 514.0x the scale of `vf_clip_param`. This means that it will take more than 514.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:26,810\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 514.0x the scale of `vf_clip_param`. This means that it will take more than 514.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:29,918\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 456.0x the scale of `vf_clip_param`. This means that it will take more than 456.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -4557.3826085204755\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 384\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.706\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.115702748298645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.5357573728542775e-05\n",
      "        policy_loss: -0.0005157794803380966\n",
      "        total_loss: 0.009942718781530857\n",
      "        vf_explained_var: 0.6619869470596313\n",
      "        vf_loss: 0.010458516888320446\n",
      "    load_time_ms: 0.766\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "    sample_time_ms: 1762.383\n",
      "    update_time_ms: 3.017\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.05\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.7374582162659866\n",
      "    mean_inference_ms: 0.8546248549750408\n",
      "    mean_processing_ms: 1.344866545555157\n",
      "  time_since_restore: 894.886846780777\n",
      "  time_this_iter_s: 1.2231266498565674\n",
      "  time_total_s: 894.886846780777\n",
      "  timestamp: 1595518351\n",
      "  timesteps_since_restore: 385000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 894 s, 385 iter, 385000 ts, -4.56e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:31,146\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 456.0x the scale of `vf_clip_param`. This means that it will take more than 456.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:32,811\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 456.0x the scale of `vf_clip_param`. This means that it will take more than 456.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:35,557\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 412.0x the scale of `vf_clip_param`. This means that it will take more than 412.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -4121.67908778904\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 387\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.228\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1019904613494873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.881826993776485e-05\n",
      "        policy_loss: -0.0007405348005704582\n",
      "        total_loss: 0.008324672468006611\n",
      "        vf_explained_var: 0.7449268698692322\n",
      "        vf_loss: 0.009065193124115467\n",
      "    load_time_ms: 0.787\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    sample_time_ms: 1772.727\n",
      "    update_time_ms: 2.918\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.35\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.728021269186676\n",
      "    mean_inference_ms: 0.852907210558008\n",
      "    mean_processing_ms: 1.3442446785552167\n",
      "  time_since_restore: 900.5231115818024\n",
      "  time_this_iter_s: 1.2350115776062012\n",
      "  time_total_s: 900.5231115818024\n",
      "  timestamp: 1595518356\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 900 s, 388 iter, 388000 ts, -4.12e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:36,795\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 412.0x the scale of `vf_clip_param`. This means that it will take more than 412.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:38,279\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 412.0x the scale of `vf_clip_param`. This means that it will take more than 412.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:40,929\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 405.0x the scale of `vf_clip_param`. This means that it will take more than 405.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -4052.087403748014\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 390\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1071747541427612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2811413145973347e-05\n",
      "        policy_loss: -0.0001540899247629568\n",
      "        total_loss: 0.011263283900916576\n",
      "        vf_explained_var: 0.6481062769889832\n",
      "        vf_loss: 0.011417364701628685\n",
      "    load_time_ms: 0.8\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "    sample_time_ms: 1771.591\n",
      "    update_time_ms: 2.807\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.718896232350302\n",
      "    mean_inference_ms: 0.8512515257971106\n",
      "    mean_processing_ms: 1.3436339606602126\n",
      "  time_since_restore: 905.8799185752869\n",
      "  time_this_iter_s: 1.2321786880493164\n",
      "  time_total_s: 905.8799185752869\n",
      "  timestamp: 1595518362\n",
      "  timesteps_since_restore: 391000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 905 s, 391 iter, 391000 ts, -4.05e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:42,165\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 405.0x the scale of `vf_clip_param`. This means that it will take more than 405.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:43,633\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 405.0x the scale of `vf_clip_param`. This means that it will take more than 405.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:46,738\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 388.0x the scale of `vf_clip_param`. This means that it will take more than 388.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -3875.164827031801\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 393\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.256\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1104025840759277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.659862341010012e-05\n",
      "        policy_loss: -0.00037163353408686817\n",
      "        total_loss: 0.009058808907866478\n",
      "        vf_explained_var: 0.7685719728469849\n",
      "        vf_loss: 0.009430437348783016\n",
      "    load_time_ms: 0.804\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "    sample_time_ms: 1795.539\n",
      "    update_time_ms: 2.749\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.599999999999994\n",
      "    ram_util_percent: 62.75\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.7097835788242834\n",
      "    mean_inference_ms: 0.8495956710663078\n",
      "    mean_processing_ms: 1.3430298830275276\n",
      "  time_since_restore: 911.8328969478607\n",
      "  time_this_iter_s: 1.3894233703613281\n",
      "  time_total_s: 911.8328969478607\n",
      "  timestamp: 1595518368\n",
      "  timesteps_since_restore: 394000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:48,130\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 388.0x the scale of `vf_clip_param`. This means that it will take more than 388.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 911 s, 394 iter, 394000 ts, -3.88e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:49,582\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 388.0x the scale of `vf_clip_param`. This means that it will take more than 388.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:52,687\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -3578.589786212287\n",
      "  episode_reward_min: -22757.221193421992\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 396\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.204\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1192795038223267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.350687964280951e-06\n",
      "        policy_loss: -6.677961209788918e-05\n",
      "        total_loss: 0.014676127582788467\n",
      "        vf_explained_var: 0.4796716570854187\n",
      "        vf_loss: 0.014742904342710972\n",
      "    load_time_ms: 0.811\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "    sample_time_ms: 1809.099\n",
      "    update_time_ms: 2.625\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.65\n",
      "    ram_util_percent: 62.75\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.700831240716027\n",
      "    mean_inference_ms: 0.8479713879029819\n",
      "    mean_processing_ms: 1.3424403599177137\n",
      "  time_since_restore: 917.5913813114166\n",
      "  time_this_iter_s: 1.211467981338501\n",
      "  time_total_s: 917.5913813114166\n",
      "  timestamp: 1595518373\n",
      "  timesteps_since_restore: 397000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 917 s, 397 iter, 397000 ts, -3.58e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:53,902\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:55,303\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:57,905\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-32-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.181812414910842\n",
      "  episode_reward_mean: -3578.876365141916\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.503\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1145228147506714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.847406282555312e-05\n",
      "        policy_loss: -0.0004801559553015977\n",
      "        total_loss: 0.011213067919015884\n",
      "        vf_explained_var: 0.6204102039337158\n",
      "        vf_loss: 0.011693224310874939\n",
      "    load_time_ms: 0.802\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    sample_time_ms: 1800.321\n",
      "    update_time_ms: 2.59\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.700000000000003\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6920255744999566\n",
      "    mean_inference_ms: 0.8463814503402086\n",
      "    mean_processing_ms: 1.3418633846292733\n",
      "  time_since_restore: 922.8533928394318\n",
      "  time_this_iter_s: 1.2677843570709229\n",
      "  time_total_s: 922.8533928394318\n",
      "  timestamp: 1595518379\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 922 s, 400 iter, 400000 ts, -3.58e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:32:59,176\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:00,677\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 358.0x the scale of `vf_clip_param`. This means that it will take more than 358.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:03,562\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 343.0x the scale of `vf_clip_param`. This means that it will take more than 343.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3433.8064397848125\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 402\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.347\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1177051067352295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.7172427002806216e-05\n",
      "        policy_loss: -0.0003254549519624561\n",
      "        total_loss: 0.00986569281667471\n",
      "        vf_explained_var: 0.6975922584533691\n",
      "        vf_loss: 0.010191157460212708\n",
      "    load_time_ms: 0.813\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "    sample_time_ms: 1785.351\n",
      "    update_time_ms: 2.61\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.3\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.683467161609854\n",
      "    mean_inference_ms: 0.8448463565383925\n",
      "    mean_processing_ms: 1.3412851267123624\n",
      "  time_since_restore: 928.5123245716095\n",
      "  time_this_iter_s: 1.2849724292755127\n",
      "  time_total_s: 928.5123245716095\n",
      "  timestamp: 1595518384\n",
      "  timesteps_since_restore: 403000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 928 s, 403 iter, 403000 ts, -3.43e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:04,851\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 343.0x the scale of `vf_clip_param`. This means that it will take more than 343.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:06,323\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 343.0x the scale of `vf_clip_param`. This means that it will take more than 343.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:09,050\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 331.0x the scale of `vf_clip_param`. This means that it will take more than 331.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3307.4924389555326\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 405\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.781\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1246875524520874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.230047150282189e-05\n",
      "        policy_loss: -0.0002502422430552542\n",
      "        total_loss: 0.019449470564723015\n",
      "        vf_explained_var: 0.04853945970535278\n",
      "        vf_loss: 0.01969972252845764\n",
      "    load_time_ms: 0.829\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "    sample_time_ms: 1731.928\n",
      "    update_time_ms: 2.783\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5\n",
      "    ram_util_percent: 62.650000000000006\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.674913608307597\n",
      "    mean_inference_ms: 0.84328441906031\n",
      "    mean_processing_ms: 1.340697427849178\n",
      "  time_since_restore: 933.9212789535522\n",
      "  time_this_iter_s: 1.218745231628418\n",
      "  time_total_s: 933.9212789535522\n",
      "  timestamp: 1595518390\n",
      "  timesteps_since_restore: 406000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 933 s, 406 iter, 406000 ts, -3.31e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:10,275\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 331.0x the scale of `vf_clip_param`. This means that it will take more than 331.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:11,937\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 331.0x the scale of `vf_clip_param`. This means that it will take more than 331.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:14,531\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 332.0x the scale of `vf_clip_param`. This means that it will take more than 332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3319.8860098250316\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 408\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.042\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1277251243591309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2896060979983304e-05\n",
      "        policy_loss: -0.000268553732894361\n",
      "        total_loss: 0.12215007096529007\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.122418612241745\n",
      "    load_time_ms: 0.844\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "    sample_time_ms: 1750.704\n",
      "    update_time_ms: 2.74\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.5\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.666413325057744\n",
      "    mean_inference_ms: 0.8417200114129868\n",
      "    mean_processing_ms: 1.3401145812639463\n",
      "  time_since_restore: 939.3186604976654\n",
      "  time_this_iter_s: 1.1500282287597656\n",
      "  time_total_s: 939.3186604976654\n",
      "  timestamp: 1595518395\n",
      "  timesteps_since_restore: 409000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 939 s, 409 iter, 409000 ts, -3.32e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:15,685\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 332.0x the scale of `vf_clip_param`. This means that it will take more than 332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:17,601\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 332.0x the scale of `vf_clip_param`. This means that it will take more than 332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:20,162\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 345.0x the scale of `vf_clip_param`. This means that it will take more than 345.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3446.0617522139546\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 411\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.2\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1180241107940674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7438530903746141e-06\n",
      "        policy_loss: -7.317733980016783e-05\n",
      "        total_loss: 0.011360974051058292\n",
      "        vf_explained_var: 0.6528990268707275\n",
      "        vf_loss: 0.011434145271778107\n",
      "    load_time_ms: 0.822\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    sample_time_ms: 1754.62\n",
      "    update_time_ms: 2.698\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.3\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.658048279949384\n",
      "    mean_inference_ms: 0.8401831415245963\n",
      "    mean_processing_ms: 1.3395444336433582\n",
      "  time_since_restore: 944.9860870838165\n",
      "  time_this_iter_s: 1.2020893096923828\n",
      "  time_total_s: 944.9860870838165\n",
      "  timestamp: 1595518401\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 944 s, 412 iter, 412000 ts, -3.45e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:21,368\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 345.0x the scale of `vf_clip_param`. This means that it will take more than 345.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:22,799\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 345.0x the scale of `vf_clip_param`. This means that it will take more than 345.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:25,453\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3464.9930154229114\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 414\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.331\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1230430603027344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.259306974243373e-05\n",
      "        policy_loss: -0.00046474457485601306\n",
      "        total_loss: 0.011012323200702667\n",
      "        vf_explained_var: 0.6465420722961426\n",
      "        vf_loss: 0.01147706713527441\n",
      "    load_time_ms: 0.779\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "    sample_time_ms: 1731.696\n",
      "    update_time_ms: 2.744\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.6\n",
      "    ram_util_percent: 62.849999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6497380719695434\n",
      "    mean_inference_ms: 0.8386418601543935\n",
      "    mean_processing_ms: 1.3389779299747842\n",
      "  time_since_restore: 950.2343578338623\n",
      "  time_this_iter_s: 1.1722044944763184\n",
      "  time_total_s: 950.2343578338623\n",
      "  timestamp: 1595518406\n",
      "  timesteps_since_restore: 415000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 950 s, 415 iter, 415000 ts, -3.46e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:26,628\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:28,229\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:30,857\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3458.697791498001\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 417\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1166540384292603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.4224261728231795e-06\n",
      "        policy_loss: -5.914688154007308e-05\n",
      "        total_loss: 0.010596930049359798\n",
      "        vf_explained_var: 0.7146599888801575\n",
      "        vf_loss: 0.010656080208718777\n",
      "    load_time_ms: 0.824\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "    sample_time_ms: 1741.931\n",
      "    update_time_ms: 2.854\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.0\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6415559640766317\n",
      "    mean_inference_ms: 0.8371249055444118\n",
      "    mean_processing_ms: 1.3384230120760996\n",
      "  time_since_restore: 955.8268740177155\n",
      "  time_this_iter_s: 1.3753483295440674\n",
      "  time_total_s: 955.8268740177155\n",
      "  timestamp: 1595518412\n",
      "  timesteps_since_restore: 418000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 955 s, 418 iter, 418000 ts, -3.46e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:32,237\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:33,772\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 346.0x the scale of `vf_clip_param`. This means that it will take more than 346.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:36,384\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 347.0x the scale of `vf_clip_param`. This means that it will take more than 347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3469.4906003346837\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 420\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.001\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1124626398086548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.7873505789320916e-05\n",
      "        policy_loss: -0.0005459528183564544\n",
      "        total_loss: 0.009834576398134232\n",
      "        vf_explained_var: 0.7757465243339539\n",
      "        vf_loss: 0.010380515828728676\n",
      "    load_time_ms: 0.834\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "    sample_time_ms: 1711.611\n",
      "    update_time_ms: 3.09\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.0\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.633572617494596\n",
      "    mean_inference_ms: 0.8356516685501463\n",
      "    mean_processing_ms: 1.3378758170523344\n",
      "  time_since_restore: 961.1501908302307\n",
      "  time_this_iter_s: 1.1902742385864258\n",
      "  time_total_s: 961.1501908302307\n",
      "  timestamp: 1595518417\n",
      "  timesteps_since_restore: 421000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 961 s, 421 iter, 421000 ts, -3.47e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:37,581\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 347.0x the scale of `vf_clip_param`. This means that it will take more than 347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:39,067\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 347.0x the scale of `vf_clip_param`. This means that it will take more than 347.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:41,651\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 335.0x the scale of `vf_clip_param`. This means that it will take more than 335.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3349.544606302948\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 423\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.799\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1041958332061768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2692213431364507e-06\n",
      "        policy_loss: -1.864242585725151e-05\n",
      "        total_loss: 0.011089345440268517\n",
      "        vf_explained_var: 0.599829912185669\n",
      "        vf_loss: 0.011107990518212318\n",
      "    load_time_ms: 0.848\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    sample_time_ms: 1726.602\n",
      "    update_time_ms: 2.946\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2\n",
      "    ram_util_percent: 62.650000000000006\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.625737349100784\n",
      "    mean_inference_ms: 0.8342092325540814\n",
      "    mean_processing_ms: 1.3373383860013153\n",
      "  time_since_restore: 966.575186252594\n",
      "  time_this_iter_s: 1.364891529083252\n",
      "  time_total_s: 966.575186252594\n",
      "  timestamp: 1595518423\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 966 s, 424 iter, 424000 ts, -3.35e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:43,018\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 335.0x the scale of `vf_clip_param`. This means that it will take more than 335.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:44,514\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 335.0x the scale of `vf_clip_param`. This means that it will take more than 335.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:47,137\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 324.0x the scale of `vf_clip_param`. This means that it will take more than 324.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3242.241235402843\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 426\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.249\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.114675521850586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.102012659743195e-06\n",
      "        policy_loss: -8.394813630729914e-05\n",
      "        total_loss: 0.011971572414040565\n",
      "        vf_explained_var: 0.5919598340988159\n",
      "        vf_loss: 0.012055523693561554\n",
      "    load_time_ms: 0.806\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "    sample_time_ms: 1741.827\n",
      "    update_time_ms: 2.806\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.3\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6180658134231556\n",
      "    mean_inference_ms: 0.8328006192344316\n",
      "    mean_processing_ms: 1.3368146574000783\n",
      "  time_since_restore: 972.0969378948212\n",
      "  time_this_iter_s: 1.413318395614624\n",
      "  time_total_s: 972.0969378948212\n",
      "  timestamp: 1595518428\n",
      "  timesteps_since_restore: 427000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 972 s, 427 iter, 427000 ts, -3.24e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:48,553\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 324.0x the scale of `vf_clip_param`. This means that it will take more than 324.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:50,044\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 324.0x the scale of `vf_clip_param`. This means that it will take more than 324.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:52,618\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 339.0x the scale of `vf_clip_param`. This means that it will take more than 339.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3393.480384775766\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 429\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.561\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1189113855361938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.176751776365563e-05\n",
      "        policy_loss: -0.00048730181879363954\n",
      "        total_loss: 0.045307550579309464\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04579484462738037\n",
      "    load_time_ms: 0.768\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    sample_time_ms: 1717.98\n",
      "    update_time_ms: 2.356\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.5\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6105042458770322\n",
      "    mean_inference_ms: 0.831398725974163\n",
      "    mean_processing_ms: 1.3363010048723094\n",
      "  time_since_restore: 977.3404667377472\n",
      "  time_this_iter_s: 1.187941074371338\n",
      "  time_total_s: 977.3404667377472\n",
      "  timestamp: 1595518433\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 977 s, 430 iter, 430000 ts, -3.39e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:53,809\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 339.0x the scale of `vf_clip_param`. This means that it will take more than 339.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:55,248\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 339.0x the scale of `vf_clip_param`. This means that it will take more than 339.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:57,849\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 319.0x the scale of `vf_clip_param`. This means that it will take more than 319.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-33-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3191.5526255907052\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 432\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.03\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.106961727142334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.131065563546144e-07\n",
      "        policy_loss: 2.837943975464441e-05\n",
      "        total_loss: 0.01056781504303217\n",
      "        vf_explained_var: 0.6859862208366394\n",
      "        vf_loss: 0.01053942833095789\n",
      "    load_time_ms: 0.76\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "    sample_time_ms: 1716.356\n",
      "    update_time_ms: 2.384\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.45\n",
      "    ram_util_percent: 62.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.6030508185413663\n",
      "    mean_inference_ms: 0.8300208878996416\n",
      "    mean_processing_ms: 1.3358000054002517\n",
      "  time_since_restore: 982.5800778865814\n",
      "  time_this_iter_s: 1.2083890438079834\n",
      "  time_total_s: 982.5800778865814\n",
      "  timestamp: 1595518439\n",
      "  timesteps_since_restore: 433000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 982 s, 433 iter, 433000 ts, -3.19e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:33:59,061\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 319.0x the scale of `vf_clip_param`. This means that it will take more than 319.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:00,505\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 319.0x the scale of `vf_clip_param`. This means that it will take more than 319.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:03,353\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 308.0x the scale of `vf_clip_param`. This means that it will take more than 308.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3084.0712233165523\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 435\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.699\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1013455390930176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.194391683791764e-05\n",
      "        policy_loss: -0.0004010088450741023\n",
      "        total_loss: 0.009840896353125572\n",
      "        vf_explained_var: 0.7624527215957642\n",
      "        vf_loss: 0.010241905227303505\n",
      "    load_time_ms: 0.755\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    sample_time_ms: 1720.873\n",
      "    update_time_ms: 2.475\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.4\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.595773074195309\n",
      "    mean_inference_ms: 0.8286795566572249\n",
      "    mean_processing_ms: 1.3353114666762869\n",
      "  time_since_restore: 988.1077411174774\n",
      "  time_this_iter_s: 1.245408058166504\n",
      "  time_total_s: 988.1077411174774\n",
      "  timestamp: 1595518444\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 988 s, 436 iter, 436000 ts, -3.08e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:04,602\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 308.0x the scale of `vf_clip_param`. This means that it will take more than 308.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:06,084\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 308.0x the scale of `vf_clip_param`. This means that it will take more than 308.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:08,691\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 296.0x the scale of `vf_clip_param`. This means that it will take more than 296.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2957.7267777158936\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 438\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.903\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1010618209838867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.648651030147448e-05\n",
      "        policy_loss: -0.0009193210862576962\n",
      "        total_loss: 0.009959203191101551\n",
      "        vf_explained_var: 0.7055465579032898\n",
      "        vf_loss: 0.010878528468310833\n",
      "    load_time_ms: 0.755\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "    sample_time_ms: 1700.305\n",
      "    update_time_ms: 2.585\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.099999999999994\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5885951447497524\n",
      "    mean_inference_ms: 0.82734810346899\n",
      "    mean_processing_ms: 1.3348287157170793\n",
      "  time_since_restore: 993.3752360343933\n",
      "  time_this_iter_s: 1.1882429122924805\n",
      "  time_total_s: 993.3752360343933\n",
      "  timestamp: 1595518449\n",
      "  timesteps_since_restore: 439000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 993 s, 439 iter, 439000 ts, -2.96e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:09,883\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 296.0x the scale of `vf_clip_param`. This means that it will take more than 296.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:11,332\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 296.0x the scale of `vf_clip_param`. This means that it will take more than 296.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:13,922\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 284.0x the scale of `vf_clip_param`. This means that it will take more than 284.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2844.1284749634465\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 441\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.544\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0958266258239746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011413192987674847\n",
      "        policy_loss: -0.0008181185694411397\n",
      "        total_loss: 0.010904019698500633\n",
      "        vf_explained_var: 0.6118245124816895\n",
      "        vf_loss: 0.011722140945494175\n",
      "    load_time_ms: 0.774\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "    sample_time_ms: 1701.822\n",
      "    update_time_ms: 2.599\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.05\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5815036987732265\n",
      "    mean_inference_ms: 0.8260289872446385\n",
      "    mean_processing_ms: 1.3343536018433113\n",
      "  time_since_restore: 998.605954170227\n",
      "  time_this_iter_s: 1.2053196430206299\n",
      "  time_total_s: 998.605954170227\n",
      "  timestamp: 1595518455\n",
      "  timesteps_since_restore: 442000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 998 s, 442 iter, 442000 ts, -2.84e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:15,130\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 284.0x the scale of `vf_clip_param`. This means that it will take more than 284.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:16,656\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 284.0x the scale of `vf_clip_param`. This means that it will take more than 284.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:19,383\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 301.0x the scale of `vf_clip_param`. This means that it will take more than 301.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3008.1815587294736\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 444\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.807\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.089462399482727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002612575190141797\n",
      "        policy_loss: -0.0015292338794097304\n",
      "        total_loss: 0.009452280588448048\n",
      "        vf_explained_var: 0.6821622848510742\n",
      "        vf_loss: 0.010981502942740917\n",
      "    load_time_ms: 0.814\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "    sample_time_ms: 1699.64\n",
      "    update_time_ms: 3.859\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5\n",
      "    ram_util_percent: 62.9\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5745488114533335\n",
      "    mean_inference_ms: 0.824735018152293\n",
      "    mean_processing_ms: 1.3338905966205805\n",
      "  time_since_restore: 1004.1308469772339\n",
      "  time_this_iter_s: 1.2869775295257568\n",
      "  time_total_s: 1004.1308469772339\n",
      "  timestamp: 1595518460\n",
      "  timesteps_since_restore: 445000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1004 s, 445 iter, 445000 ts, -3.01e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:20,673\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 301.0x the scale of `vf_clip_param`. This means that it will take more than 301.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:22,076\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 301.0x the scale of `vf_clip_param`. This means that it will take more than 301.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:24,587\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 305.0x the scale of `vf_clip_param`. This means that it will take more than 305.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:25,628\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 305.0x the scale of `vf_clip_param`. This means that it will take more than 305.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -3052.270442743962\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 447\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.47\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3579829931259155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.997452939889627e-07\n",
      "        policy_loss: -6.499957817140967e-05\n",
      "        total_loss: 336237.75\n",
      "        vf_explained_var: -0.00011670589447021484\n",
      "        vf_loss: 336237.5625\n",
      "    load_time_ms: 0.827\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "    sample_time_ms: 1675.914\n",
      "    update_time_ms: 3.888\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.65\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.567716690248295\n",
      "    mean_inference_ms: 0.8234673785689526\n",
      "    mean_processing_ms: 1.3334360109421328\n",
      "  time_since_restore: 1010.4021334648132\n",
      "  time_this_iter_s: 1.3311688899993896\n",
      "  time_total_s: 1010.4021334648132\n",
      "  timestamp: 1595518466\n",
      "  timesteps_since_restore: 449000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1010 s, 449 iter, 449000 ts, -3.05e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:26,962\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 305.0x the scale of `vf_clip_param`. This means that it will take more than 305.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:29,507\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 295.0x the scale of `vf_clip_param`. This means that it will take more than 295.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:30,789\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 295.0x the scale of `vf_clip_param`. This means that it will take more than 295.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2951.12912827087\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 450\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.442\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3510639667510986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.991195161594078e-06\n",
      "        policy_loss: -4.993057154933922e-05\n",
      "        total_loss: 28101.095703125\n",
      "        vf_explained_var: -0.0005018711090087891\n",
      "        vf_loss: 28101.091796875\n",
      "    load_time_ms: 0.832\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    sample_time_ms: 1720.336\n",
      "    update_time_ms: 3.874\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.56666666666666\n",
      "    ram_util_percent: 62.666666666666664\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5609903969275343\n",
      "    mean_inference_ms: 0.8222205174588847\n",
      "    mean_processing_ms: 1.3329925358625159\n",
      "  time_since_restore: 1016.0763373374939\n",
      "  time_this_iter_s: 1.862955093383789\n",
      "  time_total_s: 1016.0763373374939\n",
      "  timestamp: 1595518472\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1016 s, 452 iter, 452000 ts, -2.95e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:32,655\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 295.0x the scale of `vf_clip_param`. This means that it will take more than 295.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:35,250\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 299.0x the scale of `vf_clip_param`. This means that it will take more than 299.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:36,563\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 299.0x the scale of `vf_clip_param`. This means that it will take more than 299.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2988.5186408214463\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 453\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.616\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.350780725479126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.712675144626701e-07\n",
      "        policy_loss: -1.3904571460443549e-05\n",
      "        total_loss: 0.06322459876537323\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.06323850154876709\n",
      "    load_time_ms: 0.864\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "    sample_time_ms: 1730.347\n",
      "    update_time_ms: 2.569\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.6\n",
      "    ram_util_percent: 62.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5544549751166215\n",
      "    mean_inference_ms: 0.821023278671511\n",
      "    mean_processing_ms: 1.3325385728274861\n",
      "  time_since_restore: 1021.6605548858643\n",
      "  time_this_iter_s: 1.6859228610992432\n",
      "  time_total_s: 1021.6605548858643\n",
      "  timestamp: 1595518478\n",
      "  timesteps_since_restore: 455000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:38,252\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 299.0x the scale of `vf_clip_param`. This means that it will take more than 299.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1021 s, 455 iter, 455000 ts, -2.99e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:41,501\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 290.0x the scale of `vf_clip_param`. This means that it will take more than 290.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:42,755\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 290.0x the scale of `vf_clip_param`. This means that it will take more than 290.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2900.208286889105\n",
      "  episode_reward_min: -21470.76033194405\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 456\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.665\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.357641339302063\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.757544295827756e-08\n",
      "        policy_loss: -6.430339999496937e-05\n",
      "        total_loss: 1455.8773193359375\n",
      "        vf_explained_var: -0.0005344152450561523\n",
      "        vf_loss: 1455.876953125\n",
      "    load_time_ms: 0.864\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "    sample_time_ms: 1853.945\n",
      "    update_time_ms: 2.475\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.03333333333333\n",
      "    ram_util_percent: 63.03333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5479964930594985\n",
      "    mean_inference_ms: 0.8198394245427673\n",
      "    mean_processing_ms: 1.3320852738801483\n",
      "  time_since_restore: 1027.8360817432404\n",
      "  time_this_iter_s: 1.687058925628662\n",
      "  time_total_s: 1027.8360817432404\n",
      "  timestamp: 1595518484\n",
      "  timesteps_since_restore: 458000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.4/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1027 s, 458 iter, 458000 ts, -2.9e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:44,445\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 290.0x the scale of `vf_clip_param`. This means that it will take more than 290.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:47,043\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 265.0x the scale of `vf_clip_param`. This means that it will take more than 265.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:48,765\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 265.0x the scale of `vf_clip_param`. This means that it will take more than 265.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2654.037365586509\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 459\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3549143075942993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8026104246237082e-06\n",
      "        policy_loss: -0.00018866729806177318\n",
      "        total_loss: 54823.0390625\n",
      "        vf_explained_var: -0.00023937225341796875\n",
      "        vf_loss: 54822.96484375\n",
      "    load_time_ms: 0.877\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "    sample_time_ms: 1938.132\n",
      "    update_time_ms: 2.693\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6\n",
      "    ram_util_percent: 63.599999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.541659869455623\n",
      "    mean_inference_ms: 0.8186794751926568\n",
      "    mean_processing_ms: 1.3316406072082527\n",
      "  time_since_restore: 1033.8361811637878\n",
      "  time_this_iter_s: 1.6910874843597412\n",
      "  time_total_s: 1033.8361811637878\n",
      "  timestamp: 1595518490\n",
      "  timesteps_since_restore: 461000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1033 s, 461 iter, 461000 ts, -2.65e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:50,464\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 265.0x the scale of `vf_clip_param`. This means that it will take more than 265.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:53,150\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 257.0x the scale of `vf_clip_param`. This means that it will take more than 257.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:54,593\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 257.0x the scale of `vf_clip_param`. This means that it will take more than 257.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-34-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2571.5945031599376\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 462\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.752\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3368364572525024\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.496833859477192e-06\n",
      "        policy_loss: -0.0001708121271803975\n",
      "        total_loss: 0.024790290743112564\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.024961089715361595\n",
      "    load_time_ms: 0.95\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    sample_time_ms: 1924.407\n",
      "    update_time_ms: 2.803\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7\n",
      "    ram_util_percent: 63.849999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5354791469477442\n",
      "    mean_inference_ms: 0.8175521504155264\n",
      "    mean_processing_ms: 1.3312100292651337\n",
      "  time_since_restore: 1039.4781012535095\n",
      "  time_this_iter_s: 1.5236241817474365\n",
      "  time_total_s: 1039.4781012535095\n",
      "  timestamp: 1595518496\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:56,119\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 257.0x the scale of `vf_clip_param`. This means that it will take more than 257.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1039 s, 464 iter, 464000 ts, -2.57e+03 rew\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:34:58,872\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:00,203\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2546.3469140304296\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 465\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.707\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3523112535476685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.712198228342459e-06\n",
      "        policy_loss: -0.0001728887582430616\n",
      "        total_loss: 0.05943603441119194\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.059608932584524155\n",
      "    load_time_ms: 0.973\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "    sample_time_ms: 1912.679\n",
      "    update_time_ms: 2.805\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.0\n",
      "    ram_util_percent: 63.73333333333333\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5294333265870357\n",
      "    mean_inference_ms: 0.8164545248399374\n",
      "    mean_processing_ms: 1.3307935802664534\n",
      "  time_since_restore: 1045.527738571167\n",
      "  time_this_iter_s: 1.9747803211212158\n",
      "  time_total_s: 1045.527738571167\n",
      "  timestamp: 1595518502\n",
      "  timesteps_since_restore: 467000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1045 s, 467 iter, 467000 ts, -2.55e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:02,182\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:04,745\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:06,180\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.247490168390454\n",
      "  episode_reward_mean: -2546.32818486062\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 468\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.095\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3599790334701538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3905167861594236e-06\n",
      "        policy_loss: 2.931499511760194e-05\n",
      "        total_loss: 157825.53125\n",
      "        vf_explained_var: -0.0001760721206665039\n",
      "        vf_loss: 157825.421875\n",
      "    load_time_ms: 0.932\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    sample_time_ms: 1857.921\n",
      "    update_time_ms: 2.706\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.75\n",
      "    ram_util_percent: 63.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5235557739473977\n",
      "    mean_inference_ms: 0.8153988946124566\n",
      "    mean_processing_ms: 1.3303870097072863\n",
      "  time_since_restore: 1050.9540612697601\n",
      "  time_this_iter_s: 1.438967227935791\n",
      "  time_total_s: 1050.9540612697601\n",
      "  timestamp: 1595518507\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1050 s, 470 iter, 470000 ts, -2.55e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:07,621\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 255.0x the scale of `vf_clip_param`. This means that it will take more than 255.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:10,157\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 261.0x the scale of `vf_clip_param`. This means that it will take more than 261.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:11,338\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 261.0x the scale of `vf_clip_param`. This means that it will take more than 261.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.423347598780352\n",
      "  episode_reward_mean: -2609.565385021742\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 471\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.117\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3500689268112183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.153749379540386e-07\n",
      "        policy_loss: -2.680206307559274e-05\n",
      "        total_loss: 0.0695660412311554\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.06959284842014313\n",
      "    load_time_ms: 0.837\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "    sample_time_ms: 1796.769\n",
      "    update_time_ms: 2.456\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.25\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5177784214997665\n",
      "    mean_inference_ms: 0.8143679690677825\n",
      "    mean_processing_ms: 1.3299878721194878\n",
      "  time_since_restore: 1056.1350073814392\n",
      "  time_this_iter_s: 1.4734573364257812\n",
      "  time_total_s: 1056.1350073814392\n",
      "  timestamp: 1595518512\n",
      "  timesteps_since_restore: 473000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1056 s, 473 iter, 473000 ts, -2.61e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:12,815\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 261.0x the scale of `vf_clip_param`. This means that it will take more than 261.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:15,890\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 264.0x the scale of `vf_clip_param`. This means that it will take more than 264.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:17,648\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 264.0x the scale of `vf_clip_param`. This means that it will take more than 264.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.423347598780352\n",
      "  episode_reward_mean: -2641.1722192658185\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 474\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.814\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3567112684249878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.0023924157139845e-06\n",
      "        policy_loss: -0.0001471242867410183\n",
      "        total_loss: 0.05875174701213837\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.05889887362718582\n",
      "    load_time_ms: 0.874\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    sample_time_ms: 1856.128\n",
      "    update_time_ms: 2.609\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.65\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.5120642733531056\n",
      "    mean_inference_ms: 0.8133478947919976\n",
      "    mean_processing_ms: 1.3295933057279439\n",
      "  time_since_restore: 1062.3336589336395\n",
      "  time_this_iter_s: 1.3793127536773682\n",
      "  time_total_s: 1062.3336589336395\n",
      "  timestamp: 1595518519\n",
      "  timesteps_since_restore: 476000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:19,031\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 264.0x the scale of `vf_clip_param`. This means that it will take more than 264.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1062 s, 476 iter, 476000 ts, -2.64e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:21,690\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:22,997\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.423347598780352\n",
      "  episode_reward_mean: -2533.818454719399\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 477\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.888\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3661081790924072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.52955656935228e-07\n",
      "        policy_loss: -7.613277557538822e-05\n",
      "        total_loss: 150744.3125\n",
      "        vf_explained_var: -0.00010609626770019531\n",
      "        vf_loss: 150744.171875\n",
      "    load_time_ms: 0.94\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "    sample_time_ms: 1785.216\n",
      "    update_time_ms: 2.656\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.506459850181514\n",
      "    mean_inference_ms: 0.8123565810596332\n",
      "    mean_processing_ms: 1.329199752970981\n",
      "  time_since_restore: 1067.6006395816803\n",
      "  time_this_iter_s: 1.3115112781524658\n",
      "  time_total_s: 1067.6006395816803\n",
      "  timestamp: 1595518524\n",
      "  timesteps_since_restore: 479000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1067 s, 479 iter, 479000 ts, -2.53e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:24,312\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:26,772\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 241.0x the scale of `vf_clip_param`. This means that it will take more than 241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:27,858\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 241.0x the scale of `vf_clip_param`. This means that it will take more than 241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.423347598780352\n",
      "  episode_reward_mean: -2407.335744325422\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 480\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.923\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3638216257095337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.79839934289339e-06\n",
      "        policy_loss: -0.00029584692674688995\n",
      "        total_loss: 140837.703125\n",
      "        vf_explained_var: -0.00012743473052978516\n",
      "        vf_loss: 140837.53125\n",
      "    load_time_ms: 0.944\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "    sample_time_ms: 1773.051\n",
      "    update_time_ms: 2.708\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.9\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.50089617036745\n",
      "    mean_inference_ms: 0.8113645665766183\n",
      "    mean_processing_ms: 1.3288050842042995\n",
      "  time_since_restore: 1072.6273257732391\n",
      "  time_this_iter_s: 1.4936997890472412\n",
      "  time_total_s: 1072.6273257732391\n",
      "  timestamp: 1595518529\n",
      "  timesteps_since_restore: 482000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1072 s, 482 iter, 482000 ts, -2.41e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:29,355\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 241.0x the scale of `vf_clip_param`. This means that it will take more than 241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:31,990\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:33,396\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.423347598780352\n",
      "  episode_reward_mean: -2356.89901651875\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 483\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.831\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3694047927856445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.988180419080891e-06\n",
      "        policy_loss: -0.00023073006013873965\n",
      "        total_loss: 149410.359375\n",
      "        vf_explained_var: -0.0001556873321533203\n",
      "        vf_loss: 149410.203125\n",
      "    load_time_ms: 0.95\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "    sample_time_ms: 1682.732\n",
      "    update_time_ms: 3.308\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.15\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4954246905190343\n",
      "    mean_inference_ms: 0.8103869125461186\n",
      "    mean_processing_ms: 1.3284159754095968\n",
      "  time_since_restore: 1078.0488622188568\n",
      "  time_this_iter_s: 1.3924064636230469\n",
      "  time_total_s: 1078.0488622188568\n",
      "  timestamp: 1595518534\n",
      "  timesteps_since_restore: 485000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1078 s, 485 iter, 485000 ts, -2.36e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:34,793\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:37,743\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:38,840\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2476.9104914359746\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 486\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.56\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3570491075515747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0255574807160883e-06\n",
      "        policy_loss: 1.245737075805664e-05\n",
      "        total_loss: 0.04591645672917366\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04590398445725441\n",
      "    load_time_ms: 0.887\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    sample_time_ms: 1686.484\n",
      "    update_time_ms: 3.156\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.900000000000006\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4900010167323456\n",
      "    mean_inference_ms: 0.8094081965938632\n",
      "    mean_processing_ms: 1.3280185714448174\n",
      "  time_since_restore: 1083.4046602249146\n",
      "  time_this_iter_s: 1.3186166286468506\n",
      "  time_total_s: 1083.4046602249146\n",
      "  timestamp: 1595518540\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1083 s, 488 iter, 488000 ts, -2.48e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:40,168\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:42,616\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:43,723\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:45,046\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 253.0x the scale of `vf_clip_param`. This means that it will take more than 253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2363.3695894920525\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 492\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.625\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3700737953186035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.160331511433469e-06\n",
      "        policy_loss: -0.0003172159194946289\n",
      "        total_loss: 23925.0078125\n",
      "        vf_explained_var: -0.0002180337905883789\n",
      "        vf_loss: 23925.005859375\n",
      "    load_time_ms: 0.918\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    sample_time_ms: 1848.114\n",
      "    update_time_ms: 3.202\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.520000000000003\n",
      "    ram_util_percent: 63.379999999999995\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4792317512122684\n",
      "    mean_inference_ms: 0.8074372156351459\n",
      "    mean_processing_ms: 1.3272213638098822\n",
      "  time_since_restore: 1091.3607256412506\n",
      "  time_this_iter_s: 3.090508222579956\n",
      "  time_total_s: 1091.3607256412506\n",
      "  timestamp: 1595518548\n",
      "  timesteps_since_restore: 492000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1091 s, 492 iter, 492000 ts, -2.36e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:48,140\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:49,207\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:50,657\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:53,103\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 233.0x the scale of `vf_clip_param`. This means that it will take more than 233.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2331.8265302613377\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 495\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.712\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0873161554336548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.7246604500087415e-07\n",
      "        policy_loss: 2.0571231289068237e-05\n",
      "        total_loss: 0.014087546616792679\n",
      "        vf_explained_var: 0.46156448125839233\n",
      "        vf_loss: 0.0140669671818614\n",
      "    load_time_ms: 0.848\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    sample_time_ms: 1623.914\n",
      "    update_time_ms: 2.451\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.9\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4739026936102255\n",
      "    mean_inference_ms: 0.8064513047240264\n",
      "    mean_processing_ms: 1.3268231907123331\n",
      "  time_since_restore: 1097.437387228012\n",
      "  time_this_iter_s: 1.1260764598846436\n",
      "  time_total_s: 1097.437387228012\n",
      "  timestamp: 1595518554\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1097 s, 496 iter, 496000 ts, -2.33e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:54,232\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 233.0x the scale of `vf_clip_param`. This means that it will take more than 233.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:55,594\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 233.0x the scale of `vf_clip_param`. This means that it will take more than 233.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:58,072\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-35-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2287.660622056407\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 498\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.506\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0982849597930908\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.266832497203723e-05\n",
      "        policy_loss: -0.0004814796557184309\n",
      "        total_loss: 0.02452016808092594\n",
      "        vf_explained_var: -0.2506129741668701\n",
      "        vf_loss: 0.025001635774970055\n",
      "    load_time_ms: 0.853\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "    sample_time_ms: 1638.842\n",
      "    update_time_ms: 2.435\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.5\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.468613243422343\n",
      "    mean_inference_ms: 0.8054672326485798\n",
      "    mean_processing_ms: 1.3264268999131672\n",
      "  time_since_restore: 1102.4375476837158\n",
      "  time_this_iter_s: 1.1699538230895996\n",
      "  time_total_s: 1102.4375476837158\n",
      "  timestamp: 1595518559\n",
      "  timesteps_since_restore: 499000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1102 s, 499 iter, 499000 ts, -2.29e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:35:59,244\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:00,593\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:03,416\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 214.0x the scale of `vf_clip_param`. This means that it will take more than 214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2135.544332498005\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 501\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.333\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0931864976882935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3279974155011587e-05\n",
      "        policy_loss: -0.00033115004771389067\n",
      "        total_loss: 0.014265361241996288\n",
      "        vf_explained_var: 0.3584636449813843\n",
      "        vf_loss: 0.014596506021916866\n",
      "    load_time_ms: 0.82\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "    sample_time_ms: 1626.364\n",
      "    update_time_ms: 2.442\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.2\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4633699708463697\n",
      "    mean_inference_ms: 0.804486956874602\n",
      "    mean_processing_ms: 1.3260294423168535\n",
      "  time_since_restore: 1107.8245503902435\n",
      "  time_this_iter_s: 1.2316749095916748\n",
      "  time_total_s: 1107.8245503902435\n",
      "  timestamp: 1595518564\n",
      "  timesteps_since_restore: 502000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1107 s, 502 iter, 502000 ts, -2.14e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:04,650\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 214.0x the scale of `vf_clip_param`. This means that it will take more than 214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:05,998\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 214.0x the scale of `vf_clip_param`. This means that it will take more than 214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:08,448\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 207.0x the scale of `vf_clip_param`. This means that it will take more than 207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:09,554\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 207.0x the scale of `vf_clip_param`. This means that it will take more than 207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -2066.0497864372824\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 504\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.137\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3610502481460571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.519816234780592e-06\n",
      "        policy_loss: -5.8594705478753895e-05\n",
      "        total_loss: 0.0759894847869873\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.0760480985045433\n",
      "    load_time_ms: 0.777\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "    sample_time_ms: 1657.334\n",
      "    update_time_ms: 2.342\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.099999999999994\n",
      "    ram_util_percent: 63.349999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4581706610080567\n",
      "    mean_inference_ms: 0.8035091351513566\n",
      "    mean_processing_ms: 1.3256290712762333\n",
      "  time_since_restore: 1114.206479549408\n",
      "  time_this_iter_s: 1.4900269508361816\n",
      "  time_total_s: 1114.206479549408\n",
      "  timestamp: 1595518571\n",
      "  timesteps_since_restore: 506000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1114 s, 506 iter, 506000 ts, -2.07e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17036)\u001b[0m 2020-07-23 18:36:11,047\tWARNING ppo.py:121 -- The magnitude of your environment rewards are more than 207.0x the scale of `vf_clip_param`. This means that it will take more than 207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1908.2317866253416\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 507\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.162\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3674006462097168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.121793270111084e-06\n",
      "        policy_loss: -1.291942589887185e-05\n",
      "        total_loss: 0.0564182884991169\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.056431204080581665\n",
      "    load_time_ms: 0.79\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "    sample_time_ms: 1695.29\n",
      "    update_time_ms: 2.417\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.4\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.452987493717792\n",
      "    mean_inference_ms: 0.8025273651314323\n",
      "    mean_processing_ms: 1.325229421818517\n",
      "  time_since_restore: 1119.5871188640594\n",
      "  time_this_iter_s: 1.289046049118042\n",
      "  time_total_s: 1119.5871188640594\n",
      "  timestamp: 1595518576\n",
      "  timesteps_since_restore: 509000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1119 s, 509 iter, 509000 ts, -1.91e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1674.6881668263043\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 510\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.725\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3418432474136353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.975531618991226e-07\n",
      "        policy_loss: 5.116701140650548e-05\n",
      "        total_loss: 0.03451605886220932\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03446488082408905\n",
      "    load_time_ms: 0.798\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    sample_time_ms: 1694.963\n",
      "    update_time_ms: 2.365\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.0\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4478753798184467\n",
      "    mean_inference_ms: 0.801554202708511\n",
      "    mean_processing_ms: 1.3248285198405714\n",
      "  time_since_restore: 1124.9752006530762\n",
      "  time_this_iter_s: 1.338172197341919\n",
      "  time_total_s: 1124.9752006530762\n",
      "  timestamp: 1595518581\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1124 s, 512 iter, 512000 ts, -1.67e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1668.4013650410955\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 513\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.828\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3674428462982178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.8590383029441e-07\n",
      "        policy_loss: -1.6250610315182712e-06\n",
      "        total_loss: 317220.03125\n",
      "        vf_explained_var: -0.00012791156768798828\n",
      "        vf_loss: 317219.875\n",
      "    load_time_ms: 0.811\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "    sample_time_ms: 1717.222\n",
      "    update_time_ms: 2.402\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.05\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.442760078491824\n",
      "    mean_inference_ms: 0.8005696687104701\n",
      "    mean_processing_ms: 1.324427585720041\n",
      "  time_since_restore: 1130.090098619461\n",
      "  time_this_iter_s: 1.3135673999786377\n",
      "  time_total_s: 1130.090098619461\n",
      "  timestamp: 1595518586\n",
      "  timesteps_since_restore: 515000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1130 s, 515 iter, 515000 ts, -1.67e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1687.324749850638\n",
      "  episode_reward_min: -20201.948432758036\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 516\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.956\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3484450578689575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.79967696062522e-06\n",
      "        policy_loss: -2.6950836399919353e-05\n",
      "        total_loss: 0.03320968523621559\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03323665261268616\n",
      "    load_time_ms: 0.779\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "    sample_time_ms: 1705.23\n",
      "    update_time_ms: 2.318\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.3\n",
      "    ram_util_percent: 63.349999999999994\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.43769113327648\n",
      "    mean_inference_ms: 0.7995906340429694\n",
      "    mean_processing_ms: 1.3240302833163538\n",
      "  time_since_restore: 1135.5525906085968\n",
      "  time_this_iter_s: 1.6152973175048828\n",
      "  time_total_s: 1135.5525906085968\n",
      "  timestamp: 1595518592\n",
      "  timesteps_since_restore: 518000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1135 s, 518 iter, 518000 ts, -1.69e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1541.9944365266063\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 522\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.315\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.388189435005188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.064998044370441e-06\n",
      "        policy_loss: -0.00013926505926065147\n",
      "        total_loss: 210003.828125\n",
      "        vf_explained_var: -0.0001245737075805664\n",
      "        vf_loss: 210003.6875\n",
      "    load_time_ms: 0.796\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "    sample_time_ms: 1772.118\n",
      "    update_time_ms: 2.386\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.066666666666666\n",
      "    ram_util_percent: 63.29999999999999\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4276457883619207\n",
      "    mean_inference_ms: 0.797625162313112\n",
      "    mean_processing_ms: 1.3232383022937895\n",
      "  time_since_restore: 1142.8940567970276\n",
      "  time_this_iter_s: 2.4759833812713623\n",
      "  time_total_s: 1142.8940567970276\n",
      "  timestamp: 1595518599\n",
      "  timesteps_since_restore: 522000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1142 s, 522 iter, 522000 ts, -1.54e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1541.9742872095592\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 525\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.236\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0921952724456787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4864087461319286e-05\n",
      "        policy_loss: -7.778740109642968e-05\n",
      "        total_loss: 0.01501114759594202\n",
      "        vf_explained_var: 0.3650267720222473\n",
      "        vf_loss: 0.01508892048150301\n",
      "    load_time_ms: 0.79\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "    sample_time_ms: 1576.218\n",
      "    update_time_ms: 2.38\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.9\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.422632173869731\n",
      "    mean_inference_ms: 0.7966313368270224\n",
      "    mean_processing_ms: 1.3228424145376736\n",
      "  time_since_restore: 1148.8297052383423\n",
      "  time_this_iter_s: 1.0946323871612549\n",
      "  time_total_s: 1148.8297052383423\n",
      "  timestamp: 1595518605\n",
      "  timesteps_since_restore: 526000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1148 s, 526 iter, 526000 ts, -1.54e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1276.8454777110883\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 528\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.349\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0991965532302856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.525465672282735e-06\n",
      "        policy_loss: -4.432678179000504e-05\n",
      "        total_loss: 0.018165558576583862\n",
      "        vf_explained_var: 0.18164962530136108\n",
      "        vf_loss: 0.01820988580584526\n",
      "    load_time_ms: 0.783\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "    sample_time_ms: 1572.145\n",
      "    update_time_ms: 2.359\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.4\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4176414659741443\n",
      "    mean_inference_ms: 0.7956400749083776\n",
      "    mean_processing_ms: 1.322446055668485\n",
      "  time_since_restore: 1153.9324505329132\n",
      "  time_this_iter_s: 1.0717127323150635\n",
      "  time_total_s: 1153.9324505329132\n",
      "  timestamp: 1595518610\n",
      "  timesteps_since_restore: 529000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1153 s, 529 iter, 529000 ts, -1.28e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1264.2412268754988\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 531\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.729\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0928339958190918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.863984238705598e-05\n",
      "        policy_loss: -0.0006072950200177729\n",
      "        total_loss: 0.01416054181754589\n",
      "        vf_explained_var: 0.41328394412994385\n",
      "        vf_loss: 0.014767841435968876\n",
      "    load_time_ms: 0.826\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    sample_time_ms: 1589.115\n",
      "    update_time_ms: 2.418\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.412676101780911\n",
      "    mean_inference_ms: 0.7946454400796646\n",
      "    mean_processing_ms: 1.322049438019974\n",
      "  time_since_restore: 1159.0001301765442\n",
      "  time_this_iter_s: 1.2168216705322266\n",
      "  time_total_s: 1159.0001301765442\n",
      "  timestamp: 1595518615\n",
      "  timesteps_since_restore: 532000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1159 s, 532 iter, 532000 ts, -1.26e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1264.2491652842025\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 534\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.11\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.094644546508789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012829666957259178\n",
      "        policy_loss: -0.0005743007641285658\n",
      "        total_loss: 0.01570754125714302\n",
      "        vf_explained_var: 0.34720104932785034\n",
      "        vf_loss: 0.01628183387219906\n",
      "    load_time_ms: 0.844\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "    sample_time_ms: 1652.996\n",
      "    update_time_ms: 2.37\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5\n",
      "    ram_util_percent: 63.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.407741955991935\n",
      "    mean_inference_ms: 0.7936540949269103\n",
      "    mean_processing_ms: 1.3216537535084247\n",
      "  time_since_restore: 1164.483805656433\n",
      "  time_this_iter_s: 1.0550034046173096\n",
      "  time_total_s: 1164.483805656433\n",
      "  timestamp: 1595518621\n",
      "  timesteps_since_restore: 535000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1164 s, 535 iter, 535000 ts, -1.26e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1352.603674163488\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 537\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0926815271377563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.696528074215166e-05\n",
      "        policy_loss: -5.913162385695614e-05\n",
      "        total_loss: 0.015348073095083237\n",
      "        vf_explained_var: 0.36863481998443604\n",
      "        vf_loss: 0.015407205559313297\n",
      "    load_time_ms: 0.843\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "    sample_time_ms: 1722.279\n",
      "    update_time_ms: 2.42\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.25\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.4028797784054343\n",
      "    mean_inference_ms: 0.792683290805018\n",
      "    mean_processing_ms: 1.3212639865737918\n",
      "  time_since_restore: 1170.3114981651306\n",
      "  time_this_iter_s: 1.3941476345062256\n",
      "  time_total_s: 1170.3114981651306\n",
      "  timestamp: 1595518627\n",
      "  timesteps_since_restore: 538000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1170 s, 538 iter, 538000 ts, -1.35e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1396.890591310493\n",
      "  episode_reward_min: -17043.693723663153\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 540\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.142\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.094404935836792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6770840375102125e-05\n",
      "        policy_loss: -0.00021512413513846695\n",
      "        total_loss: 0.01760854572057724\n",
      "        vf_explained_var: 0.17662590742111206\n",
      "        vf_loss: 0.017823677510023117\n",
      "    load_time_ms: 0.818\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "    sample_time_ms: 1817.057\n",
      "    update_time_ms: 2.704\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.45\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3981092815754277\n",
      "    mean_inference_ms: 0.7917347976652862\n",
      "    mean_processing_ms: 1.3208785365257691\n",
      "  time_since_restore: 1176.1755239963531\n",
      "  time_this_iter_s: 1.0706596374511719\n",
      "  time_total_s: 1176.1755239963531\n",
      "  timestamp: 1595518633\n",
      "  timesteps_since_restore: 541000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1176 s, 541 iter, 541000 ts, -1.4e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1245.4603630296035\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 543\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.104726791381836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.693157335277647e-05\n",
      "        policy_loss: -0.0005059127579443157\n",
      "        total_loss: 0.030954986810684204\n",
      "        vf_explained_var: -0.9478957653045654\n",
      "        vf_loss: 0.03146090731024742\n",
      "    load_time_ms: 0.887\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    sample_time_ms: 1756.387\n",
      "    update_time_ms: 2.935\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.1\n",
      "    ram_util_percent: 63.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3933461075861158\n",
      "    mean_inference_ms: 0.7907837614005225\n",
      "    mean_processing_ms: 1.3204959943290229\n",
      "  time_since_restore: 1181.2469930648804\n",
      "  time_this_iter_s: 1.2072298526763916\n",
      "  time_total_s: 1181.2469930648804\n",
      "  timestamp: 1595518638\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1181 s, 544 iter, 544000 ts, -1.25e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1232.7913108484\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 546\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.573\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1011229753494263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.137545693083666e-05\n",
      "        policy_loss: -0.0004009742697235197\n",
      "        total_loss: 0.01443046797066927\n",
      "        vf_explained_var: 0.37564677000045776\n",
      "        vf_loss: 0.014831453561782837\n",
      "    load_time_ms: 0.903\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "    sample_time_ms: 1728.038\n",
      "    update_time_ms: 2.943\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.900000000000002\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3886394248429794\n",
      "    mean_inference_ms: 0.7898401163816371\n",
      "    mean_processing_ms: 1.3201186200503372\n",
      "  time_since_restore: 1186.455581665039\n",
      "  time_this_iter_s: 1.194307804107666\n",
      "  time_total_s: 1186.455581665039\n",
      "  timestamp: 1595518643\n",
      "  timesteps_since_restore: 547000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1186 s, 547 iter, 547000 ts, -1.23e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1226.4801460138085\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 549\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.485\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1053967475891113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.05716387933353e-06\n",
      "        policy_loss: -0.0002472724881954491\n",
      "        total_loss: 0.01576760783791542\n",
      "        vf_explained_var: 0.2823023200035095\n",
      "        vf_loss: 0.016014890745282173\n",
      "    load_time_ms: 0.894\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    sample_time_ms: 1689.491\n",
      "    update_time_ms: 2.86\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.45\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.384014858263454\n",
      "    mean_inference_ms: 0.788917284920824\n",
      "    mean_processing_ms: 1.3197464383068727\n",
      "  time_since_restore: 1192.2427546977997\n",
      "  time_this_iter_s: 1.2702815532684326\n",
      "  time_total_s: 1192.2427546977997\n",
      "  timestamp: 1595518649\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1192 s, 550 iter, 550000 ts, -1.23e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.620004704282471\n",
      "  episode_reward_mean: -1156.9777282070725\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 552\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.659\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1028281450271606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.312025677994825e-06\n",
      "        policy_loss: -6.726074207108468e-05\n",
      "        total_loss: 0.015047548338770866\n",
      "        vf_explained_var: 0.2589588165283203\n",
      "        vf_loss: 0.015114809386432171\n",
      "    load_time_ms: 0.824\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "    sample_time_ms: 1774.984\n",
      "    update_time_ms: 2.798\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.4\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3794753176208343\n",
      "    mean_inference_ms: 0.7880121747765176\n",
      "    mean_processing_ms: 1.3193870152468534\n",
      "  time_since_restore: 1198.0068607330322\n",
      "  time_this_iter_s: 1.1829650402069092\n",
      "  time_total_s: 1198.0068607330322\n",
      "  timestamp: 1595518655\n",
      "  timesteps_since_restore: 553000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1198 s, 553 iter, 553000 ts, -1.16e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.598978149918924\n",
      "  episode_reward_mean: -1100.138931010151\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 555\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.466\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1048322916030884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.1203715479932725e-05\n",
      "        policy_loss: -0.00015526532661169767\n",
      "        total_loss: 0.018279749900102615\n",
      "        vf_explained_var: 0.09026700258255005\n",
      "        vf_loss: 0.01843501254916191\n",
      "    load_time_ms: 0.822\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    sample_time_ms: 1760.152\n",
      "    update_time_ms: 2.429\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.4\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3749231864436418\n",
      "    mean_inference_ms: 0.7870941294729485\n",
      "    mean_processing_ms: 1.3190282130869269\n",
      "  time_since_restore: 1203.0741136074066\n",
      "  time_this_iter_s: 1.0740060806274414\n",
      "  time_total_s: 1203.0741136074066\n",
      "  timestamp: 1595518660\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.5/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1203 s, 556 iter, 556000 ts, -1.1e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -1074.8493003952904\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 558\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.727\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1045562028884888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.9478729629772715e-05\n",
      "        policy_loss: -0.00016900920309126377\n",
      "        total_loss: 0.015888899564743042\n",
      "        vf_explained_var: 0.2585025429725647\n",
      "        vf_loss: 0.01605791226029396\n",
      "    load_time_ms: 0.809\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "    sample_time_ms: 1829.357\n",
      "    update_time_ms: 2.369\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.666666666666664\n",
      "    ram_util_percent: 64.1\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3703843572338066\n",
      "    mean_inference_ms: 0.7861750361028218\n",
      "    mean_processing_ms: 1.3186679396710617\n",
      "  time_since_restore: 1209.4813406467438\n",
      "  time_this_iter_s: 1.6949059963226318\n",
      "  time_total_s: 1209.4813406467438\n",
      "  timestamp: 1595518666\n",
      "  timesteps_since_restore: 559000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1209 s, 559 iter, 559000 ts, -1.07e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -1049.5663125516203\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 561\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.24\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3614070415496826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.27123006072361e-06\n",
      "        policy_loss: 8.435249583271798e-06\n",
      "        total_loss: 0.22047656774520874\n",
      "        vf_explained_var: 0.0204160213470459\n",
      "        vf_loss: 0.22046810388565063\n",
      "    load_time_ms: 0.837\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "    sample_time_ms: 2080.379\n",
      "    update_time_ms: 2.864\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.05\n",
      "    ram_util_percent: 64.175\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.365998135569024\n",
      "    mean_inference_ms: 0.7852939087516019\n",
      "    mean_processing_ms: 1.318313562634724\n",
      "  time_since_restore: 1214.7081863880157\n",
      "  time_this_iter_s: 2.83109712600708\n",
      "  time_total_s: 1214.7081863880157\n",
      "  timestamp: 1595518671\n",
      "  timesteps_since_restore: 561000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1214 s, 561 iter, 561000 ts, -1.05e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -1043.2377544560543\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 564\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.721\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3661491870880127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2836327389086364e-06\n",
      "        policy_loss: -9.269666770705953e-05\n",
      "        total_loss: 0.30694180727005005\n",
      "        vf_explained_var: -0.08007669448852539\n",
      "        vf_loss: 0.3070344924926758\n",
      "    load_time_ms: 0.849\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    sample_time_ms: 2010.872\n",
      "    update_time_ms: 3.054\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.766666666666666\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3615817807110835\n",
      "    mean_inference_ms: 0.7843977578627422\n",
      "    mean_processing_ms: 1.3179534806764672\n",
      "  time_since_restore: 1219.7484798431396\n",
      "  time_this_iter_s: 2.5023081302642822\n",
      "  time_total_s: 1219.7484798431396\n",
      "  timestamp: 1595518676\n",
      "  timesteps_since_restore: 564000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1219 s, 564 iter, 564000 ts, -1.04e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -1036.9184541336244\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 567\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.493\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.355854868888855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.194000219584268e-06\n",
      "        policy_loss: 4.6236993512138724e-05\n",
      "        total_loss: 0.27123841643333435\n",
      "        vf_explained_var: -0.10952103137969971\n",
      "        vf_loss: 0.27119219303131104\n",
      "    load_time_ms: 0.929\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "    sample_time_ms: 2019.253\n",
      "    update_time_ms: 3.131\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.5\n",
      "    ram_util_percent: 66.375\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3571539013947675\n",
      "    mean_inference_ms: 0.7834901768385951\n",
      "    mean_processing_ms: 1.317594445317396\n",
      "  time_since_restore: 1224.857723236084\n",
      "  time_this_iter_s: 2.493495225906372\n",
      "  time_total_s: 1224.857723236084\n",
      "  timestamp: 1595518681\n",
      "  timesteps_since_restore: 567000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1224 s, 567 iter, 567000 ts, -1.04e+03 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -1005.35055020673\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 570\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3784687519073486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.7940017111614e-07\n",
      "        policy_loss: -8.87765854713507e-05\n",
      "        total_loss: 48409.2890625\n",
      "        vf_explained_var: -0.00013458728790283203\n",
      "        vf_loss: 48409.234375\n",
      "    load_time_ms: 0.872\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    sample_time_ms: 1860.593\n",
      "    update_time_ms: 3.219\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.725\n",
      "    ram_util_percent: 66.475\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3527479039703674\n",
      "    mean_inference_ms: 0.7825768398404347\n",
      "    mean_processing_ms: 1.3172383370893357\n",
      "  time_since_restore: 1230.727970123291\n",
      "  time_this_iter_s: 3.1096136569976807\n",
      "  time_total_s: 1230.727970123291\n",
      "  timestamp: 1595518687\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1230 s, 570 iter, 570000 ts, -1.01e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -935.813882538562\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 573\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.652\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0979931354522705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.0410260630305856e-05\n",
      "        policy_loss: -0.00037460707244463265\n",
      "        total_loss: 0.01740448735654354\n",
      "        vf_explained_var: 0.18863368034362793\n",
      "        vf_loss: 0.01777910627424717\n",
      "    load_time_ms: 0.868\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "    sample_time_ms: 1667.819\n",
      "    update_time_ms: 2.87\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.6\n",
      "    ram_util_percent: 66.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3483516144819276\n",
      "    mean_inference_ms: 0.7816609322679304\n",
      "    mean_processing_ms: 1.3168819642178873\n",
      "  time_since_restore: 1236.6578040122986\n",
      "  time_this_iter_s: 1.1339645385742188\n",
      "  time_total_s: 1236.6578040122986\n",
      "  timestamp: 1595518693\n",
      "  timesteps_since_restore: 574000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1236 s, 574 iter, 574000 ts, -936 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -916.8857276927828\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 576\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.943\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0956735610961914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010244494478683919\n",
      "        policy_loss: -0.000560410029720515\n",
      "        total_loss: 0.020765477791428566\n",
      "        vf_explained_var: -0.11622273921966553\n",
      "        vf_loss: 0.021325882524251938\n",
      "    load_time_ms: 0.793\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "    sample_time_ms: 1670.624\n",
      "    update_time_ms: 2.792\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.3\n",
      "    ram_util_percent: 66.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.343936505083316\n",
      "    mean_inference_ms: 0.7807319512722074\n",
      "    mean_processing_ms: 1.3165267691875397\n",
      "  time_since_restore: 1241.771791934967\n",
      "  time_this_iter_s: 1.2825100421905518\n",
      "  time_total_s: 1241.771791934967\n",
      "  timestamp: 1595518698\n",
      "  timesteps_since_restore: 577000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1241 s, 577 iter, 577000 ts, -917 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -878.9862667507476\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 579\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.141\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3486919403076172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.042420864105225e-06\n",
      "        policy_loss: -5.465126014314592e-05\n",
      "        total_loss: 0.04586336016654968\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04591801390051842\n",
      "    load_time_ms: 0.839\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "    sample_time_ms: 1627.529\n",
      "    update_time_ms: 2.5\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05\n",
      "    ram_util_percent: 66.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3395603091693835\n",
      "    mean_inference_ms: 0.7798068734665119\n",
      "    mean_processing_ms: 1.3161705685680358\n",
      "  time_since_restore: 1248.2617011070251\n",
      "  time_this_iter_s: 1.5548417568206787\n",
      "  time_total_s: 1248.2617011070251\n",
      "  timestamp: 1595518705\n",
      "  timesteps_since_restore: 581000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1248 s, 581 iter, 581000 ts, -879 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -834.6529934596801\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 582\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.958\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3460842370986938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.726218205381883e-06\n",
      "        policy_loss: -4.372787589090876e-05\n",
      "        total_loss: 0.18715408444404602\n",
      "        vf_explained_var: -0.15927767753601074\n",
      "        vf_loss: 0.18719780445098877\n",
      "    load_time_ms: 0.855\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    sample_time_ms: 1681.215\n",
      "    update_time_ms: 2.979\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.650000000000006\n",
      "    ram_util_percent: 65.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.335245451005419\n",
      "    mean_inference_ms: 0.7788962025367852\n",
      "    mean_processing_ms: 1.3158215307733416\n",
      "  time_since_restore: 1253.6937720775604\n",
      "  time_this_iter_s: 1.4835326671600342\n",
      "  time_total_s: 1253.6937720775604\n",
      "  timestamp: 1595518710\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1253 s, 584 iter, 584000 ts, -835 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -752.5441389610221\n",
      "  episode_reward_min: -10736.477853410228\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 585\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.343\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3657649755477905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.928967534098774e-06\n",
      "        policy_loss: -0.0004890708951279521\n",
      "        total_loss: 489184.78125\n",
      "        vf_explained_var: -0.00015556812286376953\n",
      "        vf_loss: 489184.53125\n",
      "    load_time_ms: 0.85\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "    sample_time_ms: 1743.326\n",
      "    update_time_ms: 3.375\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.05\n",
      "    ram_util_percent: 66.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.330980571768455\n",
      "    mean_inference_ms: 0.7779963222196283\n",
      "    mean_processing_ms: 1.315486412252823\n",
      "  time_since_restore: 1259.4373486042023\n",
      "  time_this_iter_s: 1.2830078601837158\n",
      "  time_total_s: 1259.4373486042023\n",
      "  timestamp: 1595518716\n",
      "  timesteps_since_restore: 587000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1259 s, 587 iter, 587000 ts, -753 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -720.9761633608307\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 591\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.888\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3608999252319336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6149878092619474e-06\n",
      "        policy_loss: 3.031539927178528e-05\n",
      "        total_loss: 0.2567211091518402\n",
      "        vf_explained_var: -0.04266703128814697\n",
      "        vf_loss: 0.2566908001899719\n",
      "    load_time_ms: 0.821\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "    sample_time_ms: 1834.644\n",
      "    update_time_ms: 3.19\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.45\n",
      "    ram_util_percent: 66.45\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3226022480473163\n",
      "    mean_inference_ms: 0.7762375408262626\n",
      "    mean_processing_ms: 1.3148367280247553\n",
      "  time_since_restore: 1266.8320372104645\n",
      "  time_this_iter_s: 2.4750113487243652\n",
      "  time_total_s: 1266.8320372104645\n",
      "  timestamp: 1595518724\n",
      "  timesteps_since_restore: 591000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1266 s, 591 iter, 591000 ts, -721 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -714.5974966806289\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 594\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.609\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.354797601699829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.8444994743258576e-08\n",
      "        policy_loss: -1.0759353244793601e-05\n",
      "        total_loss: 0.24657422304153442\n",
      "        vf_explained_var: -0.0343170166015625\n",
      "        vf_loss: 0.24658499658107758\n",
      "    load_time_ms: 0.848\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "    sample_time_ms: 1890.069\n",
      "    update_time_ms: 3.904\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.125\n",
      "    ram_util_percent: 64.625\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.318555024376583\n",
      "    mean_inference_ms: 0.7753887884267594\n",
      "    mean_processing_ms: 1.3145186276362906\n",
      "  time_since_restore: 1272.859874010086\n",
      "  time_this_iter_s: 3.2637667655944824\n",
      "  time_total_s: 1272.859874010086\n",
      "  timestamp: 1595518730\n",
      "  timesteps_since_restore: 594000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1272 s, 594 iter, 594000 ts, -715 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -657.778743437409\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 597\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.174\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.357905387878418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6318499547196552e-05\n",
      "        policy_loss: -0.00010953235323540866\n",
      "        total_loss: 0.2414640486240387\n",
      "        vf_explained_var: -0.02718353271484375\n",
      "        vf_loss: 0.24157358705997467\n",
      "    load_time_ms: 0.847\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "    sample_time_ms: 1848.81\n",
      "    update_time_ms: 3.602\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.474999999999998\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.314548027288759\n",
      "    mean_inference_ms: 0.7745473467324174\n",
      "    mean_processing_ms: 1.3142042058402164\n",
      "  time_since_restore: 1278.183424949646\n",
      "  time_this_iter_s: 2.894972562789917\n",
      "  time_total_s: 1278.183424949646\n",
      "  timestamp: 1595518735\n",
      "  timesteps_since_restore: 597000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1278 s, 597 iter, 597000 ts, -658 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -695.6430291625907\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 600\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 16.061\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1049648523330688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.7671190006658435e-05\n",
      "        policy_loss: -0.0002825927804224193\n",
      "        total_loss: 0.01964387111365795\n",
      "        vf_explained_var: 0.08755266666412354\n",
      "        vf_loss: 0.019926462322473526\n",
      "    load_time_ms: 0.826\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "    sample_time_ms: 1705.671\n",
      "    update_time_ms: 3.602\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.4\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3105761359738652\n",
      "    mean_inference_ms: 0.7737103522376372\n",
      "    mean_processing_ms: 1.313893947259844\n",
      "  time_since_restore: 1284.1444544792175\n",
      "  time_this_iter_s: 1.0319926738739014\n",
      "  time_total_s: 1284.1444544792175\n",
      "  timestamp: 1595518741\n",
      "  timesteps_since_restore: 601000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1284 s, 601 iter, 601000 ts, -696 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -695.6870473136399\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 603\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.759\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1084567308425903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00020774149743374437\n",
      "        policy_loss: -0.0012345104478299618\n",
      "        total_loss: 0.019225414842367172\n",
      "        vf_explained_var: 0.04246652126312256\n",
      "        vf_loss: 0.020459935069084167\n",
      "    load_time_ms: 0.773\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    sample_time_ms: 1613.078\n",
      "    update_time_ms: 2.505\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.8\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.306631587795703\n",
      "    mean_inference_ms: 0.7728801353610383\n",
      "    mean_processing_ms: 1.3135889016456244\n",
      "  time_since_restore: 1289.194441318512\n",
      "  time_this_iter_s: 1.0800974369049072\n",
      "  time_total_s: 1289.194441318512\n",
      "  timestamp: 1595518746\n",
      "  timesteps_since_restore: 604000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1289 s, 604 iter, 604000 ts, -696 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -683.1026866930947\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 606\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.982\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.349504828453064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.8522480849678686e-07\n",
      "        policy_loss: -5.4588319471804425e-05\n",
      "        total_loss: 0.05019554868340492\n",
      "        vf_explained_var: -0.9997494220733643\n",
      "        vf_loss: 0.05025015026330948\n",
      "    load_time_ms: 0.799\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    sample_time_ms: 1585.14\n",
      "    update_time_ms: 2.374\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.3027250195149778\n",
      "    mean_inference_ms: 0.7720563352612817\n",
      "    mean_processing_ms: 1.3132882040275413\n",
      "  time_since_restore: 1295.3868434429169\n",
      "  time_this_iter_s: 1.3336725234985352\n",
      "  time_total_s: 1295.3868434429169\n",
      "  timestamp: 1595518752\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1295 s, 608 iter, 608000 ts, -683 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -689.423804072987\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 609\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.459\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3571722507476807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.180044950568117e-05\n",
      "        policy_loss: -0.000663942308165133\n",
      "        total_loss: 0.06145014613866806\n",
      "        vf_explained_var: -0.7161256074905396\n",
      "        vf_loss: 0.06211409345269203\n",
      "    load_time_ms: 0.825\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "    sample_time_ms: 1625.788\n",
      "    update_time_ms: 2.5\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.23333333333333\n",
      "    ram_util_percent: 64.43333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.298816605355898\n",
      "    mean_inference_ms: 0.7712280540182823\n",
      "    mean_processing_ms: 1.3129915438289546\n",
      "  time_since_restore: 1300.6141772270203\n",
      "  time_this_iter_s: 1.6922695636749268\n",
      "  time_total_s: 1300.6141772270203\n",
      "  timestamp: 1595518757\n",
      "  timesteps_since_restore: 611000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1300 s, 611 iter, 611000 ts, -689 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -670.5074985986197\n",
      "  episode_reward_min: -10738.435098189335\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 615\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.838\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3582209348678589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0383903827460017e-05\n",
      "        policy_loss: -0.00021845435549039394\n",
      "        total_loss: 0.25863945484161377\n",
      "        vf_explained_var: -0.003970026969909668\n",
      "        vf_loss: 0.2588579058647156\n",
      "    load_time_ms: 0.831\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "    sample_time_ms: 1722.637\n",
      "    update_time_ms: 2.483\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.400000000000006\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.291157213411755\n",
      "    mean_inference_ms: 0.7696234264203835\n",
      "    mean_processing_ms: 1.3124070467211693\n",
      "  time_since_restore: 1307.9930930137634\n",
      "  time_this_iter_s: 2.476475238800049\n",
      "  time_total_s: 1307.9930930137634\n",
      "  timestamp: 1595518765\n",
      "  timesteps_since_restore: 615000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1307 s, 615 iter, 615000 ts, -671 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-30\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -790.5044279841195\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 618\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.375276803970337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.2866339501633774e-06\n",
      "        policy_loss: -1.3864517313777469e-05\n",
      "        total_loss: 0.28358617424964905\n",
      "        vf_explained_var: -0.05641746520996094\n",
      "        vf_loss: 0.2836000323295593\n",
      "    load_time_ms: 0.809\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "    sample_time_ms: 1755.588\n",
      "    update_time_ms: 2.566\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.0\n",
      "    ram_util_percent: 64.85\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.287399174071295\n",
      "    mean_inference_ms: 0.768835406834639\n",
      "    mean_processing_ms: 1.3121208807976665\n",
      "  time_since_restore: 1313.162986755371\n",
      "  time_this_iter_s: 2.5805671215057373\n",
      "  time_total_s: 1313.162986755371\n",
      "  timestamp: 1595518770\n",
      "  timesteps_since_restore: 618000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1313 s, 618 iter, 618000 ts, -791 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -708.4813938780344\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 621\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.775\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3573721647262573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3465285064739874e-06\n",
      "        policy_loss: 1.4328003089758568e-05\n",
      "        total_loss: 0.2327449768781662\n",
      "        vf_explained_var: 0.030694305896759033\n",
      "        vf_loss: 0.2327306717634201\n",
      "    load_time_ms: 0.789\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "    sample_time_ms: 1803.228\n",
      "    update_time_ms: 2.436\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.125\n",
      "    ram_util_percent: 64.42500000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.283715526165447\n",
      "    mean_inference_ms: 0.7680664232216391\n",
      "    mean_processing_ms: 1.311837647949392\n",
      "  time_since_restore: 1318.862586259842\n",
      "  time_this_iter_s: 2.8941047191619873\n",
      "  time_total_s: 1318.862586259842\n",
      "  timestamp: 1595518776\n",
      "  timesteps_since_restore: 621000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1318 s, 621 iter, 621000 ts, -708 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -777.9263726311099\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 624\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0918762683868408\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.97229217318818e-05\n",
      "        policy_loss: -0.0003856935363728553\n",
      "        total_loss: 0.019872959703207016\n",
      "        vf_explained_var: 0.07757574319839478\n",
      "        vf_loss: 0.02025865577161312\n",
      "    load_time_ms: 0.782\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "    sample_time_ms: 1673.519\n",
      "    update_time_ms: 2.437\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.6\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2800893090771517\n",
      "    mean_inference_ms: 0.7673115015008556\n",
      "    mean_processing_ms: 1.3115579367907575\n",
      "  time_since_restore: 1324.938961982727\n",
      "  time_this_iter_s: 1.1346521377563477\n",
      "  time_total_s: 1324.938961982727\n",
      "  timestamp: 1595518782\n",
      "  timesteps_since_restore: 625000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1324 s, 625 iter, 625000 ts, -778 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -784.3115732329154\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 627\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.06\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.089716911315918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.9286155293229967e-05\n",
      "        policy_loss: -0.00020081328693777323\n",
      "        total_loss: 0.021674614399671555\n",
      "        vf_explained_var: -0.19371426105499268\n",
      "        vf_loss: 0.02187541499733925\n",
      "    load_time_ms: 0.813\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    sample_time_ms: 1706.009\n",
      "    update_time_ms: 2.567\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.85\n",
      "    ram_util_percent: 64.4\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.276485771694421\n",
      "    mean_inference_ms: 0.7665590875406632\n",
      "    mean_processing_ms: 1.3112819790680459\n",
      "  time_since_restore: 1330.454844713211\n",
      "  time_this_iter_s: 1.1659574508666992\n",
      "  time_total_s: 1330.454844713211\n",
      "  timestamp: 1595518787\n",
      "  timesteps_since_restore: 628000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1330 s, 628 iter, 628000 ts, -784 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -784.3028526663076\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 630\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.146\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0967892408370972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.1980357461143285e-06\n",
      "        policy_loss: -2.155351648980286e-05\n",
      "        total_loss: 0.028078660368919373\n",
      "        vf_explained_var: -0.6978253126144409\n",
      "        vf_loss: 0.02810022234916687\n",
      "    load_time_ms: 0.809\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "    sample_time_ms: 1685.793\n",
      "    update_time_ms: 2.752\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.9\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.272932115790652\n",
      "    mean_inference_ms: 0.7658196172019014\n",
      "    mean_processing_ms: 1.311008622845649\n",
      "  time_since_restore: 1335.9525558948517\n",
      "  time_this_iter_s: 1.164961814880371\n",
      "  time_total_s: 1335.9525558948517\n",
      "  timestamp: 1595518793\n",
      "  timesteps_since_restore: 631000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1335 s, 631 iter, 631000 ts, -784 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-39-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -746.4207914241949\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 633\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.247\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.095684289932251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.481489541940391e-06\n",
      "        policy_loss: -0.00019000243628397584\n",
      "        total_loss: 0.03882965072989464\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.0390196330845356\n",
      "    load_time_ms: 0.82\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "    sample_time_ms: 1761.175\n",
      "    update_time_ms: 2.789\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.549999999999997\n",
      "    ram_util_percent: 66.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.269468926648122\n",
      "    mean_inference_ms: 0.7651054553366932\n",
      "    mean_processing_ms: 1.3107427404463443\n",
      "  time_since_restore: 1341.6496217250824\n",
      "  time_this_iter_s: 1.1912431716918945\n",
      "  time_total_s: 1341.6496217250824\n",
      "  timestamp: 1595518799\n",
      "  timesteps_since_restore: 634000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1341 s, 634 iter, 634000 ts, -746 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -664.4732938724404\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 636\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.108\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0884462594985962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010425681102788076\n",
      "        policy_loss: -0.0005573148955591023\n",
      "        total_loss: 0.016574453562498093\n",
      "        vf_explained_var: 0.2917054295539856\n",
      "        vf_loss: 0.01713176816701889\n",
      "    load_time_ms: 0.789\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "    sample_time_ms: 1737.813\n",
      "    update_time_ms: 2.75\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.3\n",
      "    ram_util_percent: 66.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.266027120473135\n",
      "    mean_inference_ms: 0.7643897287252399\n",
      "    mean_processing_ms: 1.3104783872145\n",
      "  time_since_restore: 1346.8774654865265\n",
      "  time_this_iter_s: 1.1105952262878418\n",
      "  time_total_s: 1346.8774654865265\n",
      "  timestamp: 1595518804\n",
      "  timesteps_since_restore: 637000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1346 s, 637 iter, 637000 ts, -664 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -607.5628118478327\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 639\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.907\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3509881496429443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7644634315038275e-07\n",
      "        policy_loss: 1.1315345545881428e-05\n",
      "        total_loss: 0.07973859459161758\n",
      "        vf_explained_var: -0.7701979875564575\n",
      "        vf_loss: 0.07972727715969086\n",
      "    load_time_ms: 0.787\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "    sample_time_ms: 1705.946\n",
      "    update_time_ms: 2.403\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.5\n",
      "    ram_util_percent: 66.3\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.262562123045308\n",
      "    mean_inference_ms: 0.7636599677172528\n",
      "    mean_processing_ms: 1.310212714256835\n",
      "  time_since_restore: 1353.2165644168854\n",
      "  time_this_iter_s: 1.3806571960449219\n",
      "  time_total_s: 1353.2165644168854\n",
      "  timestamp: 1595518810\n",
      "  timesteps_since_restore: 641000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.0/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1353 s, 641 iter, 641000 ts, -608 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -601.2656190079668\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 642\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.702\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.372087836265564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0429024541735998e-06\n",
      "        policy_loss: -5.329513442120515e-05\n",
      "        total_loss: 178568.109375\n",
      "        vf_explained_var: -9.512901306152344e-05\n",
      "        vf_loss: 178567.9375\n",
      "    load_time_ms: 0.787\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    sample_time_ms: 1649.548\n",
      "    update_time_ms: 2.378\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.4\n",
      "    ram_util_percent: 64.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2591300661411866\n",
      "    mean_inference_ms: 0.7629362981769241\n",
      "    mean_processing_ms: 1.309947713194127\n",
      "  time_since_restore: 1358.3467574119568\n",
      "  time_this_iter_s: 1.3346083164215088\n",
      "  time_total_s: 1358.3467574119568\n",
      "  timestamp: 1595518815\n",
      "  timesteps_since_restore: 644000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 644\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1358 s, 644 iter, 644000 ts, -601 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -639.1704145233433\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 645\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.607\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3524090051651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.966425078440807e-07\n",
      "        policy_loss: 1.0074615602206904e-05\n",
      "        total_loss: 0.04130380600690842\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.041293736547231674\n",
      "    load_time_ms: 0.783\n",
      "    num_steps_sampled: 647000\n",
      "    num_steps_trained: 647000\n",
      "    sample_time_ms: 1700.88\n",
      "    update_time_ms: 2.319\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.900000000000006\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2557227298238764\n",
      "    mean_inference_ms: 0.7622193492144856\n",
      "    mean_processing_ms: 1.3096827972070306\n",
      "  time_since_restore: 1364.086139202118\n",
      "  time_this_iter_s: 1.3816125392913818\n",
      "  time_total_s: 1364.086139202118\n",
      "  timestamp: 1595518821\n",
      "  timesteps_since_restore: 647000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 647000\n",
      "  training_iteration: 647\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1364 s, 647 iter, 647000 ts, -639 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -626.5223257975624\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 648\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.4\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3368239402770996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.4434199278621236e-06\n",
      "        policy_loss: -6.737518560839817e-05\n",
      "        total_loss: 0.03559686988592148\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03566426411271095\n",
      "    load_time_ms: 0.775\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    sample_time_ms: 1736.977\n",
      "    update_time_ms: 2.345\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.75\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2523578648994875\n",
      "    mean_inference_ms: 0.7615135091301616\n",
      "    mean_processing_ms: 1.3094204762837007\n",
      "  time_since_restore: 1369.4032287597656\n",
      "  time_this_iter_s: 1.4263441562652588\n",
      "  time_total_s: 1369.4032287597656\n",
      "  timestamp: 1595518826\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 650\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1369 s, 650 iter, 650000 ts, -627 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -626.5181867641603\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 651\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.362951397895813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1693834949255688e-06\n",
      "        policy_loss: -3.139972614008002e-05\n",
      "        total_loss: 0.11759679019451141\n",
      "        vf_explained_var: -0.6913546323776245\n",
      "        vf_loss: 0.11762818694114685\n",
      "    load_time_ms: 0.791\n",
      "    num_steps_sampled: 653000\n",
      "    num_steps_trained: 653000\n",
      "    sample_time_ms: 1748.972\n",
      "    update_time_ms: 2.457\n",
      "  iterations_since_restore: 653\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.599999999999994\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.248984789590927\n",
      "    mean_inference_ms: 0.7608023296925776\n",
      "    mean_processing_ms: 1.3091580140909875\n",
      "  time_since_restore: 1374.7036819458008\n",
      "  time_this_iter_s: 1.4528961181640625\n",
      "  time_total_s: 1374.7036819458008\n",
      "  timestamp: 1595518832\n",
      "  timesteps_since_restore: 653000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 653000\n",
      "  training_iteration: 653\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1374 s, 653 iter, 653000 ts, -627 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -626.5911250657921\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 654\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.366\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.345523715019226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.753968020551838e-06\n",
      "        policy_loss: 1.2135982615291141e-05\n",
      "        total_loss: 0.04675505682826042\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04674292728304863\n",
      "    load_time_ms: 0.813\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    sample_time_ms: 1731.92\n",
      "    update_time_ms: 2.592\n",
      "  iterations_since_restore: 656\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.6\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.245666714614543\n",
      "    mean_inference_ms: 0.7601089611981936\n",
      "    mean_processing_ms: 1.3088914878920161\n",
      "  time_since_restore: 1380.23437833786\n",
      "  time_this_iter_s: 1.455613613128662\n",
      "  time_total_s: 1380.23437833786\n",
      "  timestamp: 1595518837\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 656\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1380 s, 656 iter, 656000 ts, -627 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.468003427410517\n",
      "  episode_reward_mean: -626.5450122116002\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 657\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.253\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3518571853637695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.596378294583701e-07\n",
      "        policy_loss: -4.809856545762159e-05\n",
      "        total_loss: 0.07457096874713898\n",
      "        vf_explained_var: -0.9675568342208862\n",
      "        vf_loss: 0.07461903244256973\n",
      "    load_time_ms: 0.836\n",
      "    num_steps_sampled: 659000\n",
      "    num_steps_trained: 659000\n",
      "    sample_time_ms: 1723.429\n",
      "    update_time_ms: 2.844\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.05\n",
      "    ram_util_percent: 64.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2423871096478125\n",
      "    mean_inference_ms: 0.7594261873913818\n",
      "    mean_processing_ms: 1.30862890709973\n",
      "  time_since_restore: 1385.4358479976654\n",
      "  time_this_iter_s: 1.434929609298706\n",
      "  time_total_s: 1385.4358479976654\n",
      "  timestamp: 1595518842\n",
      "  timesteps_since_restore: 659000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 659000\n",
      "  training_iteration: 659\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1385 s, 659 iter, 659000 ts, -627 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -639.3195837564804\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 660\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.795\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3436102867126465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2485385241234326e-06\n",
      "        policy_loss: -2.7129173759021796e-05\n",
      "        total_loss: 0.042136773467063904\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04216389358043671\n",
      "    load_time_ms: 0.853\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "    sample_time_ms: 1756.53\n",
      "    update_time_ms: 2.92\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.099999999999994\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2390329947189618\n",
      "    mean_inference_ms: 0.758720413797922\n",
      "    mean_processing_ms: 1.3083656829217125\n",
      "  time_since_restore: 1391.0486192703247\n",
      "  time_this_iter_s: 1.588139295578003\n",
      "  time_total_s: 1391.0486192703247\n",
      "  timestamp: 1595518848\n",
      "  timesteps_since_restore: 662000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 662\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1391 s, 662 iter, 662000 ts, -639 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -639.3319648990032\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 663\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.613\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3421895503997803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0023396902834065e-05\n",
      "        policy_loss: -0.00042773628956638277\n",
      "        total_loss: 0.042330194264650345\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04275793582201004\n",
      "    load_time_ms: 0.82\n",
      "    num_steps_sampled: 665000\n",
      "    num_steps_trained: 665000\n",
      "    sample_time_ms: 1716.751\n",
      "    update_time_ms: 2.724\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.45\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.235674050067307\n",
      "    mean_inference_ms: 0.7580135789709389\n",
      "    mean_processing_ms: 1.3080999392061843\n",
      "  time_since_restore: 1396.1733751296997\n",
      "  time_this_iter_s: 1.4262566566467285\n",
      "  time_total_s: 1396.1733751296997\n",
      "  timestamp: 1595518853\n",
      "  timesteps_since_restore: 665000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 665000\n",
      "  training_iteration: 665\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1396 s, 665 iter, 665000 ts, -639 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -639.3181009315027\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 666\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.855\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3450692892074585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.406273319546017e-07\n",
      "        policy_loss: -2.358055098738987e-05\n",
      "        total_loss: 0.06729662418365479\n",
      "        vf_explained_var: -0.7378778457641602\n",
      "        vf_loss: 0.06732018291950226\n",
      "    load_time_ms: 0.795\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    sample_time_ms: 1720.9\n",
      "    update_time_ms: 2.741\n",
      "  iterations_since_restore: 668\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.900000000000006\n",
      "    ram_util_percent: 64.7\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2323565872299582\n",
      "    mean_inference_ms: 0.7573184686012634\n",
      "    mean_processing_ms: 1.307839154675018\n",
      "  time_since_restore: 1401.4278750419617\n",
      "  time_this_iter_s: 1.404463768005371\n",
      "  time_total_s: 1401.4278750419617\n",
      "  timestamp: 1595518859\n",
      "  timesteps_since_restore: 668000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 668\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1401 s, 668 iter, 668000 ts, -639 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -620.4073806677935\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 669\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.69\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3476190567016602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1110305422334932e-06\n",
      "        policy_loss: 2.663326267793309e-05\n",
      "        total_loss: 0.07933735102415085\n",
      "        vf_explained_var: -0.5786008834838867\n",
      "        vf_loss: 0.07931071519851685\n",
      "    load_time_ms: 0.868\n",
      "    num_steps_sampled: 671000\n",
      "    num_steps_trained: 671000\n",
      "    sample_time_ms: 1717.155\n",
      "    update_time_ms: 2.864\n",
      "  iterations_since_restore: 671\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.8\n",
      "    ram_util_percent: 64.6\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2290714485014167\n",
      "    mean_inference_ms: 0.7566329096985547\n",
      "    mean_processing_ms: 1.3075875939093504\n",
      "  time_since_restore: 1406.864238023758\n",
      "  time_this_iter_s: 1.3652048110961914\n",
      "  time_total_s: 1406.864238023758\n",
      "  timestamp: 1595518864\n",
      "  timesteps_since_restore: 671000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 671000\n",
      "  training_iteration: 671\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1406 s, 671 iter, 671000 ts, -620 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -620.4151579840045\n",
      "  episode_reward_min: -14525.788691568561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 672\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.623\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.347991943359375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.407797172869323e-07\n",
      "        policy_loss: -0.00012956809950992465\n",
      "        total_loss: 77955.0546875\n",
      "        vf_explained_var: -0.00012135505676269531\n",
      "        vf_loss: 77954.9765625\n",
      "    load_time_ms: 0.876\n",
      "    num_steps_sampled: 674000\n",
      "    num_steps_trained: 674000\n",
      "    sample_time_ms: 1783.477\n",
      "    update_time_ms: 2.891\n",
      "  iterations_since_restore: 674\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.05\n",
      "    ram_util_percent: 64.75\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.225832561209899\n",
      "    mean_inference_ms: 0.7559575347075527\n",
      "    mean_processing_ms: 1.3073378369411193\n",
      "  time_since_restore: 1412.8140230178833\n",
      "  time_this_iter_s: 1.470306158065796\n",
      "  time_total_s: 1412.8140230178833\n",
      "  timestamp: 1595518870\n",
      "  timesteps_since_restore: 674000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 674000\n",
      "  training_iteration: 674\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1412 s, 674 iter, 674000 ts, -620 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.604987030729615\n",
      "  episode_reward_mean: -816.2363555732586\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 675\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 15.731\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.350881576538086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.599856223170718e-08\n",
      "        policy_loss: 8.45336944621522e-06\n",
      "        total_loss: 0.045990876853466034\n",
      "        vf_explained_var: -0.8860254287719727\n",
      "        vf_loss: 0.04598243534564972\n",
      "    load_time_ms: 0.881\n",
      "    num_steps_sampled: 677000\n",
      "    num_steps_trained: 677000\n",
      "    sample_time_ms: 1826.631\n",
      "    update_time_ms: 2.987\n",
      "  iterations_since_restore: 677\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.833333333333336\n",
      "    ram_util_percent: 64.83333333333334\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2226989459775557\n",
      "    mean_inference_ms: 0.7553120619996023\n",
      "    mean_processing_ms: 1.3070987894833923\n",
      "  time_since_restore: 1418.5366740226746\n",
      "  time_this_iter_s: 1.6095499992370605\n",
      "  time_total_s: 1418.5366740226746\n",
      "  timestamp: 1595518876\n",
      "  timesteps_since_restore: 677000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 677000\n",
      "  training_iteration: 677\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1418 s, 677 iter, 677000 ts, -816 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.8537744576954625\n",
      "  episode_reward_mean: -809.9248572825906\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 678\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.03\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3529069423675537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.150338999257656e-07\n",
      "        policy_loss: 1.8234253730042838e-05\n",
      "        total_loss: 0.08405322581529617\n",
      "        vf_explained_var: -0.9497082233428955\n",
      "        vf_loss: 0.0840350016951561\n",
      "    load_time_ms: 0.779\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    sample_time_ms: 1892.898\n",
      "    update_time_ms: 2.766\n",
      "  iterations_since_restore: 680\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5\n",
      "    ram_util_percent: 65.05000000000001\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.219641720628009\n",
      "    mean_inference_ms: 0.7546901350832182\n",
      "    mean_processing_ms: 1.3068670406202387\n",
      "  time_since_restore: 1424.650173664093\n",
      "  time_this_iter_s: 1.7207286357879639\n",
      "  time_total_s: 1424.650173664093\n",
      "  timestamp: 1595518882\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 680\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1424 s, 680 iter, 680000 ts, -810 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.8537744576954625\n",
      "  episode_reward_mean: -809.9872450539912\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 681\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.713\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3571876287460327\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1414289247113629e-07\n",
      "        policy_loss: -2.373981442360673e-05\n",
      "        total_loss: 0.11586938798427582\n",
      "        vf_explained_var: -0.6195802688598633\n",
      "        vf_loss: 0.11589311063289642\n",
      "    load_time_ms: 0.778\n",
      "    num_steps_sampled: 683000\n",
      "    num_steps_trained: 683000\n",
      "    sample_time_ms: 1830.412\n",
      "    update_time_ms: 2.724\n",
      "  iterations_since_restore: 683\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.5\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.216617241154172\n",
      "    mean_inference_ms: 0.7540766522481581\n",
      "    mean_processing_ms: 1.3066389765653965\n",
      "  time_since_restore: 1429.8668620586395\n",
      "  time_this_iter_s: 1.40175199508667\n",
      "  time_total_s: 1429.8668620586395\n",
      "  timestamp: 1595518887\n",
      "  timesteps_since_restore: 683000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 683000\n",
      "  training_iteration: 683\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1429 s, 683 iter, 683000 ts, -810 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.8537744576954625\n",
      "  episode_reward_mean: -809.9961643359644\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 684\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.613\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3410392999649048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1978447219007649e-05\n",
      "        policy_loss: -5.021476681577042e-05\n",
      "        total_loss: 0.04246090352535248\n",
      "        vf_explained_var: -0.9884902238845825\n",
      "        vf_loss: 0.04251111298799515\n",
      "    load_time_ms: 0.755\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "    sample_time_ms: 1801.404\n",
      "    update_time_ms: 2.689\n",
      "  iterations_since_restore: 686\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.9\n",
      "    ram_util_percent: 64.85\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2136113881645105\n",
      "    mean_inference_ms: 0.7534657871600775\n",
      "    mean_processing_ms: 1.3064072434403002\n",
      "  time_since_restore: 1435.1454939842224\n",
      "  time_this_iter_s: 1.7241716384887695\n",
      "  time_total_s: 1435.1454939842224\n",
      "  timestamp: 1595518892\n",
      "  timesteps_since_restore: 686000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 686\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1435 s, 686 iter, 686000 ts, -810 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.94247897100012\n",
      "  episode_reward_mean: -784.7040059759457\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 687\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 12.983\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3487855195999146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.465912065323209e-06\n",
      "        policy_loss: -0.00022122669906821102\n",
      "        total_loss: 0.06728748232126236\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.06750869005918503\n",
      "    load_time_ms: 0.766\n",
      "    num_steps_sampled: 689000\n",
      "    num_steps_trained: 689000\n",
      "    sample_time_ms: 1731.286\n",
      "    update_time_ms: 2.467\n",
      "  iterations_since_restore: 689\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.2\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2106560135460716\n",
      "    mean_inference_ms: 0.7528666768200141\n",
      "    mean_processing_ms: 1.306170932433095\n",
      "  time_since_restore: 1440.4486155509949\n",
      "  time_this_iter_s: 1.5143120288848877\n",
      "  time_total_s: 1440.4486155509949\n",
      "  timestamp: 1595518898\n",
      "  timesteps_since_restore: 689000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 689000\n",
      "  training_iteration: 689\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1440 s, 689 iter, 689000 ts, -785 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.94247897100012\n",
      "  episode_reward_mean: -677.395395257573\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 690\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.005\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.357966661453247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.854678991250694e-06\n",
      "        policy_loss: -3.691482561407611e-05\n",
      "        total_loss: 0.08516182005405426\n",
      "        vf_explained_var: -0.7306976318359375\n",
      "        vf_loss: 0.08519873023033142\n",
      "    load_time_ms: 0.766\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    sample_time_ms: 1695.346\n",
      "    update_time_ms: 2.464\n",
      "  iterations_since_restore: 692\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.45\n",
      "    ram_util_percent: 65.35\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2077607008726003\n",
      "    mean_inference_ms: 0.7522825026575081\n",
      "    mean_processing_ms: 1.3059379751972038\n",
      "  time_since_restore: 1445.624959230423\n",
      "  time_this_iter_s: 1.361241340637207\n",
      "  time_total_s: 1445.624959230423\n",
      "  timestamp: 1595518903\n",
      "  timesteps_since_restore: 692000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 692\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1445 s, 692 iter, 692000 ts, -677 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.94247897100012\n",
      "  episode_reward_mean: -677.4652114606433\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 693\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 14.026\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3590160608291626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6499459889018908e-05\n",
      "        policy_loss: -0.0002707595704123378\n",
      "        total_loss: 326900.34375\n",
      "        vf_explained_var: -8.082389831542969e-05\n",
      "        vf_loss: 326900.1875\n",
      "    load_time_ms: 0.796\n",
      "    num_steps_sampled: 695000\n",
      "    num_steps_trained: 695000\n",
      "    sample_time_ms: 1798.71\n",
      "    update_time_ms: 2.521\n",
      "  iterations_since_restore: 695\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.0\n",
      "    ram_util_percent: 65.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2048690288863333\n",
      "    mean_inference_ms: 0.7516992845337078\n",
      "    mean_processing_ms: 1.3057088688623317\n",
      "  time_since_restore: 1451.6273324489594\n",
      "  time_this_iter_s: 1.4636576175689697\n",
      "  time_total_s: 1451.6273324489594\n",
      "  timestamp: 1595518909\n",
      "  timesteps_since_restore: 695000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 695000\n",
      "  training_iteration: 695\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1451 s, 695 iter, 695000 ts, -677 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -5.94247897100012\n",
      "  episode_reward_mean: -740.619411376991\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 696\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.709\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3499743938446045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.642512299644295e-06\n",
      "        policy_loss: -8.154487295541912e-05\n",
      "        total_loss: 0.053326018154621124\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.05340754985809326\n",
      "    load_time_ms: 0.793\n",
      "    num_steps_sampled: 698000\n",
      "    num_steps_trained: 698000\n",
      "    sample_time_ms: 1774.354\n",
      "    update_time_ms: 2.486\n",
      "  iterations_since_restore: 698\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.349999999999994\n",
      "    ram_util_percent: 65.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.2020426800268917\n",
      "    mean_inference_ms: 0.7511373683683776\n",
      "    mean_processing_ms: 1.305483474022389\n",
      "  time_since_restore: 1456.8934240341187\n",
      "  time_this_iter_s: 1.3963005542755127\n",
      "  time_total_s: 1456.8934240341187\n",
      "  timestamp: 1595518914\n",
      "  timesteps_since_restore: 698000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 698000\n",
      "  training_iteration: 698\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=17036], 1456 s, 698 iter, 698000 ts, -741 rew\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=17035)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=17033)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=17034)\u001b[0m -----------------------\n",
      "Result for PPO_EnergyOptPOEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-23_18-41-58\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -6.010638117001379\n",
      "  episode_reward_mean: -702.7642679412602\n",
      "  episode_reward_min: -15803.393034681561\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 699\n",
      "  experiment_id: 7eca8b5e43da4c948c7b8f10416b7db0\n",
      "  hostname: solom-XPS-13-9380\n",
      "  info:\n",
      "    grad_time_ms: 13.586\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0918323993682861\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.411716170376167e-05\n",
      "        policy_loss: -0.0003285584389232099\n",
      "        total_loss: 6558.8740234375\n",
      "        vf_explained_var: -0.0025751590728759766\n",
      "        vf_loss: 6558.87353515625\n",
      "    load_time_ms: 0.784\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    sample_time_ms: 1766.747\n",
      "    update_time_ms: 2.503\n",
      "  iterations_since_restore: 700\n",
      "  node_ip: 192.168.100.38\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.5\n",
      "    ram_util_percent: 65.5\n",
      "  pid: 17036\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 3.199258854968805\n",
      "    mean_inference_ms: 0.7505859196884179\n",
      "    mean_processing_ms: 1.3052625062197558\n",
      "  time_since_restore: 1460.9262781143188\n",
      "  time_this_iter_s: 1.1341962814331055\n",
      "  time_total_s: 1460.9262781143188\n",
      "  timestamp: 1595518918\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 700\n",
      "  trial_id: 9c518072\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.8/16.5 GB\n",
      "Result logdir: /home/solom/ray_results/training_example15\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - PPO_EnergyOptPOEnv-v0_0:\tTERMINATED, [4 CPUs, 0 GPUs], [pid=17036], 1460 s, 700 iter, 700000 ts, -703 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 20,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 700,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizing the results\n",
    "\n",
    "The simulation results are saved within the `ray_results/training_example` directory (we defined `training_example` at the start of this tutorial). The `ray_results` folder is by default located at your root `~/ray_results`. \n",
    "\n",
    "You can run `tensorboard --logdir=~/ray_results/training_example` (install it with `pip install tensorboard`) to visualize the different data outputted by your simulation.\n",
    "\n",
    "For more instructions about visualizing, please see `tutorial05_visualize.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Restart from a checkpoint / Transfer learning\n",
    "\n",
    "If you wish to do transfer learning, or to resume a previous training, you will need to start the simulation from a previous checkpoint. To do that, you can add a `restore` parameter in the `run_experiments` argument, as follows:\n",
    "\n",
    "```python\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"restore\": \"/ray_results/experiment/dir/checkpoint_50/checkpoint-50\"\n",
    "        \"checkpoint_freq\": 1,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 1,\n",
    "        },\n",
    "    },\n",
    "})\n",
    "```\n",
    "\n",
    "The `\"restore\"` path should be such that the `[restore]/.tune_metadata` file exists.\n",
    "\n",
    "There is also a `\"resume\"` parameter that you can set to `True` if you just wish to continue the training from a previously saved checkpoint, in case you are still training on the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = run_experiments({\n",
    "#     flow_params[\"exp_tag\"]: {\n",
    "#         \"run\": alg_run,\n",
    "#         \"env\": gym_name,\n",
    "#         \"config\": {\n",
    "#             **config\n",
    "#         },\n",
    "#         \"restore\": \"/ray_results/training_example13/PPO_EnergyOptPOEnv-v0_0_2020-07-23_13-30-07yze28sum/checkpoint_400/checkpoint-400\", \n",
    "#         \"checkpoint_freq\": 20,\n",
    "#         \"checkpoint_at_end\": True,\n",
    "#         \"max_failures\": 999,\n",
    "#         \"stop\": {\n",
    "#             \"training_iteration\": 700,\n",
    "#         },\n",
    "#     },\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
